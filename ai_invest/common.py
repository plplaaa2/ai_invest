
import json
import os
import re
import requests
import time
import math
import io
import pandas as pd
import feedparser
from datetime import datetime, timedelta, date, timezone
from bs4 import BeautifulSoup

try:
    import yfinance as yf
except ImportError:
    yf = None

from prompts import REPORT_PROMPTS

KST = timezone(timedelta(hours=9))

def get_now_kst():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

# --- [0. ì‹œìŠ¤í…œ ê³µí†µ ê²½ë¡œ ì„¤ì •] ---
OPTIONS_PATH = "/data/options.json"
BASE_PATH = "/share/ai_analyst"
CONFIG_PATH = os.path.join(BASE_PATH, "rss_config.json")
PENDING_PATH = os.path.join(BASE_PATH, "pending")
REPORT_DIR = os.path.join(BASE_PATH, "reports")

def load_addon_config():
    if os.path.exists(OPTIONS_PATH):
        try:
            with open(OPTIONS_PATH, "r", encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {}

config = load_addon_config() 


# í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§
openai_key = config.get("openai_api_key", "")
gemini_key = config.get("gemini_api_key", "")
headers = {"Content-Type": "application/json"}

# ğŸ¯ 2. Cloud LLM ëª¨ë“œ íŒì • ë¡œì§ ë³´ì™„
if openai_key or gemini_key:
    if openai_key:
        headers["Authorization"] = f"Bearer {openai_key}"
    print(f"ğŸš€ Cloud LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤. (OpenAI: {'OK' if openai_key else 'NO'}, Gemini: {'OK' if gemini_key else 'NO'})")
else:
    print("ğŸ  Local LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤ (API í‚¤ ì—†ìŒ).")
        

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def safe_float(v):
    if v is None or v == "" or v == "-": return 0.0
    try:
        clean_v = re.sub(r'[^\d.-]', '', str(v))
        return float(clean_v) if clean_v else 0.0
    except: return 0.0

def check_keyword_filter(text, exc_list):
    """
    í†µí•© í•„í„°ë§ ë¡œì§: ì œì™¸ì–´(Exclude) í¬í•¨ ì‹œ íƒˆë½
    scraper.pyì™€ app.pyì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©
    """
    if not text: return False
    text = text.lower()
    
    exc_list = exc_list or []

    # 1. ì œì™¸ì–´(Exclude) ì²´í¬
    if any(x in text for x in exc_list if x):
        return False
            
    return True

def check_news_filter(title, g_exc):
    """ì „ì—­ ì œì™¸ í•„í„°ë§Œ ì²˜ë¦¬"""
    if not title: return False
    title = title.lower()
    
    # 1. ì œì™¸ í•„í„°ë§ (Global)
    exc_list = [k.strip().lower() for k in g_exc.split(",") if k.strip()]
    if any(x in title for x in exc_list): return False
    
    return True

def save_to_influx(symbol, data, current_time):
    point = Point("financial_metrics").tag("symbol", symbol)
    for f, v in data.items(): point.field(f, float(v))
    point.time(current_time)
    if write_api:
        try:
            write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
            return True
        except Exception as e:
            print(f"âš ï¸ InfluxDB ì“°ê¸° ì—ëŸ¬ ({symbol}): {e}")
    return False
    
def save_report_to_file(content, section_name):
    # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ì„¸ë¶„í™”
    base_dir = REPORT_DIR
    dir_map = {
        'daily': '01_daily', 'weekly': '02_weekly', 
        'monthly': '03_monthly', 'yearly': '04_yearly'
    }
    # section_nameì´ ë§µì— ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë” ì‚¬ìš©
    subdir = dir_map.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(base_dir, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    # 2. íŒŒì¼ëª… ìƒì„± ë° ì €ì¥ (ê¸°ë¡ìš©)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"{timestamp}_{section_name.replace(' ', '_')}.txt"
    filepath = os.path.join(report_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    # 3. ğŸ¯ AI ì°¸ì¡°ìš© Latest íŒŒì¼ ê°±ì‹  (ê³ ì • ê²½ë¡œ)
    latest_path = os.path.join(report_dir, "latest.txt")
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(content)

    # 4. ğŸ§¹ ê³„ì¸µí˜• ìë™ ì •ì œ (Purge) ë¡œì§
    # ê·œì¹™: Daily(7ì¼), Weekly(30ì¼), Monthly(365ì¼) ë³´ê´€
    purge_rules = {'01_daily': 9, '02_weekly': 35, '03_monthly': 370}
    if subdir in purge_rules:
        limit_days = purge_rules[subdir]
        threshold = time.time() - (limit_days * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue # ìµœì‹  ë§¥ë½ì€ ë³´í˜¸
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
                
    return filepath
    
def load_historical_contexts():
    """íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì‘ë™í•˜ë©°, AIì—ê²Œ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•©ë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    
    for label, rel_path in dir_map.items():
        full_path = os.path.join(base_dir, rel_path)
        
        # ğŸ›¡ï¸ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë¡ì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: í•´ë‹¹ ì£¼ê¸°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        else:
            # ğŸ’¡ íŒŒì¼ì´ ì—†ì„ ë•Œ AIì—ê²Œ ì¤„ ë©”ì‹œì§€
            # AIê°€ "ê³¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë‹ˆ ì˜¤ëŠ˜ ìˆ˜ì¹˜ì— ë” ì§‘ì¤‘í•´ì„œ ë¶„ì„í•´ë¼"ë¼ê³  íŒë‹¨í•˜ê²Œ ìœ ë„í•©ë‹ˆë‹¤.
            context_text += f"\n<{label}>: ì‹œìŠ¤í…œ ë„ì… ì´ˆê¸° ë‹¨ê³„ë¡œ, ì•„ì§ {label} ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ê°€ìš©í•œ ìµœì‹  ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
            
    return context_text
    
def get_krx_summary_raw(ignore_cache=False):
    """KOSPI/KOSDAQ ì§€ìˆ˜ ë° KOSPI 3ëŒ€ ì£¼ì²´(ê°œì¸/ì™¸ì¸/ê¸°ê´€) ì¢…í•© ë¶„ì„"""
    results = {}
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, "krx_summary_v2.json")
    
    # ğŸ¯ ìºì‹œ ì²˜ë¦¬ (10ë¶„)
    if not ignore_cache and os.path.exists(cache_path):
        try:
            if time.time() - os.path.getmtime(cache_path) < 600:
                with open(cache_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except: pass

    try:
        from pykrx import stock
        from pykrx import bond
        now = get_now_kst()
        
        # ğŸ¯ í•œêµ­ ì¥ ì‹œê°„(09:00) ì „ì´ë©´ ì–´ì œ ë‚ ì§œë¥¼ ê¸°ì¤€ì¼ë¡œ ì„¤ì •
        if now.hour < 9:
            target_date = (now - timedelta(days=1)).strftime("%Y%m%d")
        else:
            target_date = now.strftime("%Y%m%d")
            
        # íœ´ì¼ ë“±ì„ ê³ ë ¤í•˜ì—¬ ë„‰ë„‰í•˜ê²Œ 14ì¼ ì „ë¶€í„° ì¡°íšŒ
        start_dt = (now - timedelta(days=14)).strftime("%Y%m%d")
        
        # 1. ì§€ìˆ˜ ë°ì´í„° (KOSPI/KOSDAQ)
        for code, name in [("1001", "KOSPI"), ("2001", "KOSDAQ")]:
            df = stock.get_index_ohlcv(start_dt, target_date, code)
            if not df.empty:
                last = df.iloc[-1]
                price = float(last['ì¢…ê°€'])
                pct = float(last['ë“±ë½ë¥ ']) if 'ë“±ë½ë¥ ' in df.columns else 0.0
                
                # ë“±ë½í­ ê³„ì‚° (ì¢…ê°€ì™€ ë“±ë½ë¥  ì—­ì‚°)
                prev = price / (1 + (pct / 100))
                diff = price - prev
                
                results[name] = {
                    "price": price, "pct": pct,
                    "amount": float(last['ê±°ë˜ëŒ€ê¸ˆ']) / 100_000_000,
                    "date": last.name.strftime("%m-%d"),
                    # ëŒ€ì‹œë³´ë“œ í˜¸í™˜ìš© í‚¤ ì¶”ê°€
                    "value": price,
                    "val_str": f"{price:,.2f}",
                    "delta_str": f"{diff:+.2f} ({pct:+.2f}%)"
                }

        # 2. KOSPI/KOSDAQ ì£¼ì²´ë³„ ê±°ë˜ëŒ€ê¸ˆ ë° Top 10 ì¢…ëª©
        if "KOSPI" in results:
            actual_date = df.index[-1].strftime("%Y%m%d") # ì‹¤ì œ ë°ì´í„° ë‚ ì§œ
            
            for mkt in ["KOSPI", "KOSDAQ"]:
                try:
                    # (A) ê±°ë˜ëŒ€ê¸ˆ í•©ê³„
                    df_inv = stock.get_market_trading_value_by_date(actual_date, actual_date, mkt)
                    if not df_inv.empty:
                        row = df_inv.iloc[-1]
                        for kor, eng in [('ê°œì¸', 'Individual'), ('ì™¸êµ­ì¸í•©ê³„', 'Foreigner'), ('ê¸°ê´€í•©ê³„', 'Institution')]:
                            val_bill = float(row[kor]) / 100_000_000
                            results[f"{mkt}_{eng}"] = {
                                "value": val_bill,
                                "val_str": f"{val_bill/10000:,.2f}ì¡°" if abs(val_bill) >= 10000 else f"{val_bill:,.0f}ì–µ"
                            }

                    # (B) ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ì¢…ëª©
                    for kor, eng in [("ê°œì¸", "Top_Individual"), ("ì™¸êµ­ì¸", "Top_Foreigner"), ("ê¸°ê´€í•©ê³„", "Top_Institution")]:
                        df_top = stock.get_market_net_purchases_of_equities(actual_date, actual_date, mkt, kor)
                        if not df_top.empty:
                            items = [f"{r['ì¢…ëª©ëª…']}({float(r['ì¢…ëª©ë³„ìˆœë§¤ìˆ˜ê¸ˆì•¡'])/100_000_000:,.0f}ì–µ)" for _, r in df_top.head(10).iterrows()]
                            results[f"{mkt}_{eng}"] = ", ".join(items)

                    # (C) ê³µë§¤ë„ ê±°ë˜ëŸ‰
                    df_short = stock.get_shorting_investor_volume_by_date(actual_date, actual_date, mkt)
                    if not df_short.empty:
                        s_row = df_short.iloc[-1]
                        results[f'{mkt}_Short'] = {"total": f"{s_row['í•©ê³„']:,.0f}ì£¼", "for": f"{s_row['ì™¸êµ­ì¸']:,.0f}ì£¼"}
                except: pass

            # (D) ì±„ê¶Œ ê¸ˆë¦¬
            try:
                df_bond = bond.get_otc_treasury_yields(actual_date)
                if not df_bond.empty:
                    for label, key in [("KR_3Y", "êµ­ê³ ì±„ 3ë…„"), ("KR_10Y", "êµ­ê³ ì±„ 10ë…„")]:
                        if key in df_bond.index:
                            val = float(df_bond.loc[key, "ìˆ˜ìµë¥ "])
                            diff = float(df_bond.loc[key, "ëŒ€ë¹„"])
                            results[label] = {
                                "value": val, "diff": diff,
                                "val_str": f"{val:.2f}%", "delta_str": f"{diff:+.2f}"
                            }
            except: pass

        # ìºì‹œ ì €ì¥ í›„ ë°˜í™˜
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False)
        return results

    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return results
    
def get_krx_market_data(r_type="daily"):
    """(í†µí•©) ì§€ìˆ˜, ìˆ˜ê¸‰, ê¸ˆë¦¬ ìš”ì•½ ë³´ê³ ì„œ (ê¸°ê°„ë³„ ë§ì¶¤)"""
    # ğŸ¯ ë³´ê³ ì„œ ìœ í˜•ë³„ ê¸°ê°„ ì„¤ì •
    if r_type == 'daily':
        fetch_days = 7      # ë°ì´í„° í™•ë³´: 1ì£¼ì¼
        comp_idx = -2       # ë³€í™” ê¸°ì¤€: ì „ì¼ ëŒ€ë¹„ (Daily Change)
        period_name = "ì¼ê°„(1D)"
    elif r_type == 'weekly':
        fetch_days = 14     # ë°ì´í„° í™•ë³´: 2ì£¼ì¼
        comp_idx = -6       # ë³€í™” ê¸°ì¤€: 1ì£¼ ì „ ëŒ€ë¹„ (Weekly Change, approx 5 trading days)
        period_name = "ì£¼ê°„(1W)"
    else: # monthly
        fetch_days = 60     # ë°ì´í„° í™•ë³´: 2ë‹¬
        comp_idx = -21      # ë³€í™” ê¸°ì¤€: 1ë‹¬ ì „ ëŒ€ë¹„ (Monthly Change, approx 20 trading days)
        period_name = "ì›”ê°„(1M)"

    data = get_krx_summary_raw() # ìµœì‹  ìˆ˜ê¸‰/ê¸ˆë¦¬ìš© (Snapshot)
    summary = f"### [ KRX ì‹œì¥ ì§€í‘œ ({period_name} ë³€ë™) ]\n"

    try:
        from pykrx import stock
        now = get_now_kst()
        target_date = now.strftime("%Y%m%d")
        if now.hour < 9: target_date = (now - timedelta(days=1)).strftime("%Y%m%d")
        start_dt = (now - timedelta(days=fetch_days)).strftime("%Y%m%d")

        for code, name in [("1001", "KOSPI"), ("2001", "KOSDAQ")]:
            df = stock.get_index_ohlcv(start_dt, target_date, code)
            if not df.empty and len(df) >= 2:
                curr = float(df.iloc[-1]['ì¢…ê°€'])
                prev_idx = comp_idx if len(df) >= abs(comp_idx) else 0
                prev = float(df.iloc[prev_idx]['ì¢…ê°€'])
                
                diff = curr - prev
                pct = (diff / prev) * 100
                
                # ì‹œê³„ì—´ ìˆ˜ì¹˜ ì¶”ì¶œ (ìµœëŒ€ fetch_days ê°œ)
                ts_values = [f"{float(val):,.2f}" for val in df['ì¢…ê°€'].tolist()]
                ts_str = " -> ".join(ts_values)
                
                summary += f"- {name}: {curr:,.2f} ({pct:+.2f}% / {period_name} ë³€ë™)\n"
                summary += f"  â”” ì‹œê³„ì—´(ê³¼ê±°->í˜„ì¬): {ts_str}\n"
    except Exception as e:
        summary += f"âš ï¸ ì§€ìˆ˜ ë°ì´í„° ì‹œê³„ì—´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}\n"

    summary += f"- KOSPI ìˆ˜ê¸‰(ìˆœë§¤ìˆ˜): ê°œì¸ {data.get('KOSPI_Individual',{}).get('val_str','0ì–µ')}, ì™¸êµ­ì¸ {data.get('KOSPI_Foreigner',{}).get('val_str','0ì–µ')}, ê¸°ê´€ {data.get('KOSPI_Institution',{}).get('val_str','0ì–µ')}\n"
    summary += f"- KOSDAQ ìˆ˜ê¸‰(ìˆœë§¤ìˆ˜): ê°œì¸ {data.get('KOSDAQ_Individual',{}).get('val_str','0ì–µ')}, ì™¸êµ­ì¸ {data.get('KOSDAQ_Foreigner',{}).get('val_str','0ì–µ')}, ê¸°ê´€ {data.get('KOSDAQ_Institution',{}).get('val_str','0ì–µ')}\n"
    k3 = data.get('KR_3Y', {}).get('val_str', 'N/A')
    k10 = data.get('KR_10Y', {}).get('val_str', 'N/A')
    summary += f"- êµ­ê³ ì±„ ê¸ˆë¦¬: 3ë…„ë¬¼ {k3} | 10ë…„ë¬¼ {k10}\n"
    return summary

def get_krx_top_investors():
    """(í†µí•©) 3ëŒ€ ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ ìƒìœ„ ë° ê³µë§¤ë„ ë³´ê³ ì„œ"""
    data = get_krx_summary_raw()
    if not data: return ""
    
    report = "### [ KOSPI ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ]\n"
    report += f"- ğŸ‘¤ ê°œì¸: {data.get('KOSPI_Top_Individual', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸŒ ì™¸ì¸: {data.get('KOSPI_Top_Foreigner', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸ¢ ê¸°ê´€: {data.get('KOSPI_Top_Institution', 'ë°ì´í„° ì—†ìŒ')}\n"
    
    s_total = data.get('KOSPI_Short', {}).get('total', 'N/A')
    s_for = data.get('KOSPI_Short', {}).get('for', 'N/A')
    report += f"ğŸ“Š ê³µë§¤ë„: ì´ {s_total} (ì™¸ì¸ {s_for})\n"

    report += "\n### [ KOSDAQ ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ]\n"
    report += f"- ğŸ‘¤ ê°œì¸: {data.get('KOSDAQ_Top_Individual', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸŒ ì™¸ì¸: {data.get('KOSDAQ_Top_Foreigner', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸ¢ ê¸°ê´€: {data.get('KOSDAQ_Top_Institution', 'ë°ì´í„° ì—†ìŒ')}\n"
    
    s_total_kq = data.get('KOSDAQ_Short', {}).get('total', 'N/A')
    s_for_kq = data.get('KOSDAQ_Short', {}).get('for', 'N/A')
    report += f"ğŸ“Š ê³µë§¤ë„: ì´ {s_total_kq} (ì™¸ì¸ {s_for_kq})\n"
    return report

def load_data():
    """ì„œë¹„ìŠ¤ ì„¤ì •(RSS, AI ëª¨ë¸ ë“±)ì„ ë¡œë“œí•˜ê³  ë¯¸ì¡´ì¬ ì‹œ ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    default_structure = {
        "feeds": [], 
        "update_interval": 10, 
        "view_range": "ì‹¤ì‹œê°„", 
        "retention_days": 7,
        "report_news_count": 100, 
        "report_auto_gen": True, 
        "report_gen_time": "08:00", 
        "report_days": 3,
        
        # ğŸ¯ ë‰´ìŠ¤ íŒë… ëª¨ë¸ ì„¤ì • (Filter)
        "filter_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.1,  # ğŸ’¡ íŒë…ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
            "prompt": "íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ê°€ ê±°ì‹œê²½ì œë‚˜ ìœ ë™ì„±ì— ì¤‘ìš”í•œì§€ íŒë…í•˜ì—¬ 0~5ì ì„ ë§¤ê¸°ì„¸ìš”."
        },
        
        # ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ëª¨ë¸ ì„¤ì • (Analyst)
        "analyst_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.3,  # ğŸ’¡ ë³´ê³ ì„œëŠ” ì•½ê°„ì˜ í†µì°°ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ 0.3~0.5 ê¶Œì¥
            "prompt": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ì „ëµì„ ì œì‹œí•˜ì„¸ìš”."
        }
    }
    
    # 1. íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ë³µêµ¬)
    if not os.path.exists(CONFIG_PATH):
        try:
            os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump(default_structure, f, indent=4, ensure_ascii=False)
            print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: {CONFIG_PATH}")
            return default_structure
        except:
            return default_structure

    # 2. íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ ë° ëˆ„ë½ëœ í‚¤ ë³´ì •
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            loaded = json.load(f)
            # ìƒˆë¡œìš´ ê¸°ëŠ¥(ì˜¨ë„ ë“±)ì´ ì¶”ê°€ë˜ì–´ í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ ë³‘í•©
            for key, val in default_structure.items():
                if key not in loaded: 
                    loaded[key] = val
                elif isinstance(val, dict): # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬(ëª¨ë¸ ì„¤ì •) ë‚´ë¶€ í‚¤ ë³´ì •
                    for sub_key, sub_val in val.items():
                        if sub_key not in loaded[key]:
                            loaded[key][sub_key] = sub_val
            return loaded
    except:
        return default_structure

# ê³µí†µ ë°ì´í„° ê°ì²´ (ëª¨ë“  ëª¨ë“ˆì—ì„œ ê³µìœ )
data = load_data()


def get_global_market_data(r_type="daily"):
    """yfinanceë¥¼ í†µí•´ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not yf: return "âš ï¸ yfinance ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    end_dt = get_now_kst()
    
    # ğŸ¯ ë³´ê³ ì„œ ìœ í˜•ë³„ ê¸°ê°„ ë° ë¹„êµ ì‹œì  ì„¤ì •
    if r_type == 'daily': 
        days = 7
        comp_idx = -2 # ì „ì¼ ëŒ€ë¹„
    elif r_type == 'weekly': 
        days = 14
        comp_idx = -6 # 1ì£¼ ì „ ëŒ€ë¹„ (ì•½ 5ê±°ë˜ì¼)
    else: 
        days = 60
        comp_idx = -21 # 1ë‹¬ ì „ ëŒ€ë¹„ (ì•½ 20ê±°ë˜ì¼)
    
    start_dt = end_dt - timedelta(days=days + 5) # ì—¬ìœ  ìˆê²Œ ì¡°íšŒ
    
    tickers = {
        "ğŸ‡ºğŸ‡¸ ë¯¸êµ­ 3ëŒ€ ì§€ìˆ˜ & VIX": {
            "^GSPC": "S&P500", "^DJI": "Dow Jones", "^IXIC": "Nasdaq", 
            "^SOX": "SOX(ë°˜ë„ì²´)", "^VIX": "VIX"
        },
        "ğŸŒ ê¸€ë¡œë²Œ ì§€ìˆ˜": {
            "^N225": "Nikkei 225", "^GDAXI": "DAX", "^HSI": "Hang Seng"
        },
        "ğŸ’µ ê¸ˆë¦¬ & í™˜ìœ¨": {
            "^TNX": "ë¯¸êµ­ì±„ 10ë…„", "^TYX": "ë¯¸êµ­ì±„ 30ë…„", "^FVX": "ë¯¸êµ­ì±„ 5ë…„", 
            "KRW=X": "USD/KRW", "DX-Y.NYB": "ë‹¬ëŸ¬ ì¸ë±ìŠ¤", "JPY=X": "USD/JPY"
        },
        "ğŸ›¢ï¸ ì›ìì¬ & ì½”ì¸": {
            "CL=F": "WTI ì›ìœ ", "GC=F": "ê¸ˆ", "SI=F": "ì€", "HG=F": "êµ¬ë¦¬", 
            "BTC-USD": "ë¹„íŠ¸ì½”ì¸"
        }
    }
    
    all_symbols = [s for cat in tickers.values() for s in cat.keys()]
    report = f"### [ ğŸŒ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„° ({r_type.upper()} ê¸°ì¤€ ë³€ë™) ]\n"
    
    try:
        df = yf.download(all_symbols, start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), progress=False)['Close']
        for cat_name, items in tickers.items():
            report += f"#### {cat_name}\n"
            for sym, name in items.items():
                try:
                    if sym in df.columns:
                        series = df[sym].dropna()
                        if len(series) < 2: continue
                        
                        curr = float(series.iloc[-1])
                        target_idx = comp_idx if len(series) >= abs(comp_idx) else 0
                        prev = float(series.iloc[target_idx])
                        
                        chg_pct = ((curr - prev) / prev) * 100
                        
                        # ì‹œê³„ì—´ ìˆ˜ì¹˜ ì¶”ì¶œ (ìµœëŒ€ days ê°œ)
                        ts_values = [f"{float(val):,.2f}" for val in series.tolist()[-days:]]
                        ts_str = " -> ".join(ts_values)
                        
                        report += f"- **{name}**: {curr:,.2f} ({chg_pct:+.2f}%, {days}ì¼ ë²”ìœ„: {series.min():,.2f}~{series.max():,.2f})\n"
                        report += f"  â”” ì‹œê³„ì—´(ê³¼ê±°->í˜„ì¬): {ts_str}\n"
                except: continue
            report += "\n"
        return report
    except Exception as e:
        return f"âš ï¸ ê¸€ë¡œë²Œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"

def is_kr_market_open():
    now = get_now_kst()
    if now.weekday() >= 5: return False # í† /ì¼ ì œì™¸
    
    # 09:00 ~ 15:30 (15ì‹œ 30ë¶„)
    current_minutes = now.hour * 60 + now.minute
    if not (540 <= current_minutes <= 930): return False
    
    try:
        from pykrx import stock
        today_str = now.strftime("%Y%m%d")
        b_days = stock.get_business_days_dates(today_str, today_str)
        if len(b_days) == 0: return False
    except: pass
    return True

def is_us_market_open():
    now = get_now_kst()
    current_minutes = now.hour * 60 + now.minute
    # ë„“ì€ ì¸ë¨¸íƒ€ì„/í‘œì¤€ì‹œ êµ¬ê°„ í¬ê´„: 22:30 ~ 06:00
    is_open_time = (current_minutes >= 1350) or (current_minutes <= 360) 
    if not is_open_time: return False
    
    if now.weekday() == 6: return False # ì¼ìš”ì¼ ì „ì²´ íœ´ì¥ (KST)
    if now.weekday() == 0 and current_minutes <= 360: return False # ì›”ìš”ì¼ ìƒˆë²½ (ë¯¸êµ­ ì¼ìš”ì¼)
    if now.weekday() == 5 and current_minutes >= 1350: return False # í† ìš”ì¼ ë°¤ (ë¯¸êµ­ í† /ì¼)
    return True

def get_global_financials_raw(ignore_cache=False, fetch_type="all"):
    """ëŒ€ì‹œë³´ë“œìš© ê¸€ë¡œë²Œ ì§€ìˆ˜, í™˜ìœ¨, ì›ìì¬, ê¸ˆë¦¬ ë°ì´í„°ë¥¼ í†µí•© ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    print(f"ğŸ” [DEBUG] get_global_financials_raw ì§„ì… (fetch_type: {fetch_type})")
    
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, "global_financials.json")
    
    results = {}
    if os.path.exists(cache_path):
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                results = json.load(f)
            if not ignore_cache and time.time() - os.path.getmtime(cache_path) < 600:
                print("ğŸ” [DEBUG] get_global_financials_raw ìºì‹œ ì‚¬ìš©")
                return results
        except: pass

    if not yf: return results
    
    tickers = {
        "USD/KRW": "KRW=X", "USD/JPY": "JPY=X", 
        "WTI": "CL=F", "Gold": "GC=F", "Bitcoin": "BTC-USD"
    }
    if fetch_type == "all":
        tickers.update({
            "S&P500": "^GSPC", "Dow Jones": "^DJI", "Nasdaq": "^IXIC", "VIX": "^VIX",
            "US10Y": "^TNX", "US2Y": "^IRX"
        })
    
    try:
        print("ğŸ” [DEBUG] get_global_financials_raw yfinance ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        end_dt = get_now_kst()
        start_dt = end_dt - timedelta(days=7)
        df = yf.download(list(tickers.values()), start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), progress=False)['Close']

        for name, sym in tickers.items():
            if sym in df.columns:
                series = df[sym].dropna()
                if len(series) >= 2:
                    curr = float(series.iloc[-1])
                    prev = float(series.iloc[-2])
                    diff = curr - prev
                    pct = (diff / prev) * 100
                    results[name] = {
                        "price": curr, "diff": diff, "pct": pct,
                        "val_str": f"{curr:,.2f}",
                        "delta_str": f"{diff:+.2f} ({pct:+.2f}%)"
                    }
        
        # ìºì‹œ ì €ì¥ (non_equities ëª¨ë“œì¼ ë•Œ ê¸°ì¡´ ì£¼ì‹ ë°ì´í„° ë³´ì¡´)
        if fetch_type != "all" and os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                existing.update(results)  # ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì“°ë˜, ê¸°ì¡´ ì£¼ì‹ ë°ì´í„°ëŠ” ë³´ì¡´
                results = existing
            except: pass
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False)
        print("ğŸ” [DEBUG] get_global_financials_raw ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ get_global_financials_raw ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        pass
        
    return results

def get_fed_liquidity_raw():
    """FRED ë°ì´í„° ì›ë³¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (Dashboardìš©)"""
    print("ğŸ” [DEBUG] get_fed_liquidity_raw ì§„ì…")
    # ğŸ¯ [NEW] ìºì‹± ì„¤ì • (24ì‹œê°„ - í•˜ë£¨ 1íšŒ)
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    print("ğŸ” [DEBUG] get_fed_liquidity_raw ìºì‹œ ê²½ë¡œ:", cache_dir)
    cache_path = os.path.join(cache_dir, "fed_liquidity.json")
    
    if os.path.exists(cache_path):
        try:
            if time.time() - os.path.getmtime(cache_path) < 3600: # 1ì‹œê°„ ì£¼ê¸°ë¡œ ê°±ì‹  ë³€ê²½
                with open(cache_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except: pass

    results = []
    
    # (Series ID, ì´ë¦„, ë‹¨ìœ„ë³€í™˜ê³„ìˆ˜, ë‹¨ìœ„ë¬¸ìì—´)
    indicators = [
        ("RRPONTSYD", "RRP", 1.0, "B$"),
        ("WRESBAL", "Reserves", 0.001, "B$"), # ë°±ë§Œ ë‹¨ìœ„ -> B(Billion) ë‹¨ìœ„ ë³€í™˜
        ("WTREGEN", "TGA", 0.001, "B$"),
        ("M2SL", "M2", 1.0, "B$"),
        ("CPIAUCSL", "CPI", 1.0, "Idx"),      # ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
        ("UNRATE", "Unemployment", 1.0, "%"), # ì‹¤ì—…ë¥ 
        ("FEDFUNDS", "FedRate", 1.0, "%"),    # ê¸°ì¤€ê¸ˆë¦¬
        ("BAMLH0A0HYM2", "HighYield", 1.0, "%"), # í•˜ì´ì¼ë“œ ìŠ¤í”„ë ˆë“œ
        ("T10YIE", "ExpInf", 1.0, "%"),       # ê¸°ëŒ€ì¸í”Œë ˆì´ì…˜ (10ë…„)
        ("GDPNOW", "GDPNow", 1.0, "%")        # ì• í‹€ë€íƒ€ ì—°ì€ GDP Now
    ]
    
    base_url = "https://fred.stlouisfed.org/graph/fredgraph.csv?id={}"
    try:
        print("ğŸ” [DEBUG] get_fed_liquidity_raw FRED ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        for code, name, scale, unit in indicators:
            try:
                # FREDëŠ” ë³„ë„ API í‚¤ ì—†ì´ CSV ì§ì ‘ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
                res = requests.get(base_url.format(code), timeout=5)
                if res.status_code == 200:
                    # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
                    df = pd.read_csv(io.StringIO(res.text), index_col=0, parse_dates=True)
                    if not df.empty:
                        series = df.iloc[:, 0].dropna()
                        if series.empty: continue
                        
                        curr_val = float(series.iloc[-1]) * scale
                        curr_date = series.index[-1].strftime("%Y-%m-%d")
                        
                        # ğŸ¯ 1ë…„ ì „ ë°ì´í„° ê³„ì‚° (ì•½ 252 ê±°ë˜ì¼ or 12ê°œì›”)
                        idx_1y = -252 if len(series) > 252 else (-12 if len(series) > 12 else 0)
                        val_1y = float(series.iloc[idx_1y]) * scale
                        diff_1y = curr_val - val_1y
                        pct_1y = (diff_1y / val_1y) * 100 if val_1y != 0 else 0.0
                        
                        # ì „ì¡°(Previous) ëŒ€ë¹„ ì¦ê°
                        diff_str = "-"
                        if len(series) > 1:
                            prev_val = float(series.iloc[-2]) * scale
                            diff = curr_val - prev_val
                            diff_str = f"{diff:+.1f}"
                        
                        # ë‘ë‹¬ì¹˜(ìµœëŒ€ 60ì¼) ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ 5ì¼ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                        sixty_days_ago = series.index[-1] - pd.Timedelta(days=60)
                        recent_series = series.loc[series.index >= sixty_days_ago]
                        ts_values = [f"{float(v * scale):.2f}" for v in recent_series.iloc[::5]]

                        # ë‹¨ìœ„ì— ë”°ë¥¸ í¬ë§·íŒ… ë¯¸ì„¸ ì¡°ì •
                        fmt = ",.2f" if unit in ["%", "Idx", "B$"] else ",.1f"
                        results.append({
                            "name": name, "value": curr_val, "diff": diff, 
                            "diff_str": diff_str, "date": curr_date,
                            "val_str": f"{curr_val:{fmt}}{unit}",
                            "delta_str": f"{diff_str} (ì§ì „)",
                            "diff_1y": diff_1y,
                            "pct_1y": pct_1y,
                            "ts_values": ts_values
                        })
                print(f"ğŸ” [DEBUG] get_fed_liquidity_raw {name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                continue
                
        # ìºì‹œ ì €ì¥
        if results:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False)
        print("ğŸ” [DEBUG] get_fed_liquidity_raw ìºì‹œ ì €ì¥")
    except: pass
    
    return results

def get_fed_liquidity_data():
    """FRED ë°ì´í„°ë¥¼ ë³´ê³ ì„œ ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    raw_data = get_fed_liquidity_raw()
    summary = "### [ ğŸ¦ ì—°ì¤€(Fed) ê±°ì‹œ/ìœ ë™ì„± ì§€í‘œ ]\n"
    try:
        for item in raw_data:
            ts_str = ", ".join(item.get('ts_values', []))
            summary += f"- **{item['name']}**: {item['val_str']} (ì§ì „: {item['diff_str']} | 1ë…„ ë³€ë™: {item['pct_1y']:+.1f}%) | ìµœê·¼ ë‘ë‹¬ì¹˜ ì¶”ì´: [{ts_str}]\n"
        return summary + "\n"
    except Exception as e:
        return f"âš ï¸ ì—°ì¤€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}\n"

def get_past_reports(section, count=1):
    """íŠ¹ì • ì„¹ì…˜ì˜ ê³¼ê±° ë³´ê³ ì„œ(ë‚ ì§œë³„ íŒŒì¼)ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {'daily': '01_daily', 'weekly': '02_weekly', 'monthly': '03_monthly'}
    target_dir = os.path.join(base_dir, dir_map.get(section, "05_etc"))
    
    content = ""
    if os.path.exists(target_dir):
        # latest.txt ì œì™¸í•˜ê³  ë‚ ì§œ í˜•ì‹ íŒŒì¼ë§Œ ì •ë ¬í•´ì„œ ê°€ì ¸ì˜´
        files = sorted([f for f in os.listdir(target_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        for f_name in files[:count]:
            try:
                with open(os.path.join(target_dir, f_name), 'r', encoding='utf-8') as f:
                    content += f"\n--- [ ê³¼ê±° ë¦¬í¬íŠ¸: {f_name} ] ---\n{f.read()}\n"
            except: pass
    return content

def get_ai_summary(title, content, system_instruction=None, role="filter", custom_config=None):
    """ë‰´ìŠ¤ íŒë… ë˜ëŠ” ìš”ì•½ì„ ìœ„í•´ AI ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. (í†µí•©ë¨)"""
    now_time = get_now_kst().strftime('%Y-%m-%d %H:%M:%S')
    
    # ì„¤ì • ë¡œë“œ (custom_configê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ common.data ì‚¬ìš©)
    cfg_data = custom_config if custom_config else data
    cfg = cfg_data.get("filter_model") if role == "filter" else cfg_data.get("analyst_model")
    
    base_url = cfg.get("url", "").rstrip('/')
    model_name = cfg.get("name")
    
    # ì§€ì¹¨ ì„¤ì •
    user_prompt = system_instruction if system_instruction else cfg.get("prompt", "")
    final_role = f"í˜„ì¬ ì‹œê°: {now_time}\në¶„ì„ ì§€ì¹¨: {user_prompt}"

    # í´ë¼ìš°ë“œ(Google ì§ì ‘ í˜¸ì¶œ) ì—¬ë¶€ íŒë³„
    is_direct_google = "generativelanguage.googleapis.com" in base_url
    
    if is_direct_google:
        api_key = config.get("gemini_api_key", "")
    else:
        api_key = cfg.get("key") if cfg.get("key") else config.get("openai_api_key", "")

    # í˜¸ì¶œ ë°©ì‹ ë¶„ê¸°
    if is_direct_google:
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": f"ì‹œìŠ¤í…œ ì§€ì¹¨: {final_role}\n\nì‚¬ìš©ì ì…ë ¥:\nì œëª©: {title}\në³¸ë¬¸: {content}"}]}],
            "generationConfig": {"temperature": cfg.get("temperature", 0.3)}
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key: headers["Authorization"] = f"Bearer {api_key}"
        payload = {
            "model": model_name,
            "messages": [{"role": "system", "content": final_role}, {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {content}"}],
            "temperature": cfg.get("temperature", 0.3)
        }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=600)
        resp.raise_for_status()
        result = resp.json()
        if "candidates" in result:
            return result['candidates'][0]['content']['parts'][0]['text']
        else:
            return result['choices'][0]['message']['content']
    except Exception as e:
        print(f"[{now_time}] AI ë¶„ì„ ì—ëŸ¬: {str(e)}")
        return f"âŒ [ERROR] AI ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"

def prepare_report_data(r_type, config_data):
    """ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°(KRX ì§€í‘œ + ë‰´ìŠ¤/ê³¼ê±°ë¦¬í¬íŠ¸)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    global_data = get_global_market_data(r_type)
    fed_data = get_fed_liquidity_data() # ì—°ì¤€ ì§€í‘œ ì¶”ê°€
    
    # KRX ë°ì´í„° ê³µí†µ ìˆ˜ì§‘ (ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œì—ë„ í˜„ì¬ ì‹œì¥ ìƒí™© ë°˜ì˜)
    market_summary = get_krx_market_data(r_type)

    if r_type == "daily":
        print(f"ğŸ” [Daily] ë°ì´í„° ìˆ˜ì§‘ (KRX ì§€í‘œ & ë‰´ìŠ¤ í•„í„°ë§) ì‹œì‘...")
        top_purchases = get_krx_top_investors()
        
        news_count = config_data.get("report_news_count", 100)
        raw_news_list = []
        seen_keys = set()
        target_date_limit = (now_kst - timedelta(days=3)).date()
        
        if os.path.exists(PENDING_PATH):
            files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
            for f_name in files:
                try:
                    with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                        news_data = json.load(file)
                        title = news_data.get("title", "").strip()
                        pub_dt_str = news_data.get("pub_dt", "")
                        if not title: continue
                        try: f_dt = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S').date()
                        except: f_dt = now_kst.date()
                        if f_dt < target_date_limit: continue
                        # scraper.pyì™€ ë™ì¼í•œ MD5 í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€
                        import hashlib
                        clean_key = hashlib.md5(title.encode()).hexdigest()[:16]
                        if clean_key not in seen_keys:
                            seen_keys.add(clean_key)
                            summary = news_data.get("summary", "").strip()
                            if summary and summary != "ë‚´ìš© ì—†ìŒ":
                                raw_news_list.append(f"[{pub_dt_str[5:16]}] {title} â€” {summary[:200]}")
                            else:
                                raw_news_list.append(f"[{pub_dt_str[5:16]}] {title}")
                        if len(raw_news_list) >= news_count: break
                except: continue
        
        news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n" + "\n".join([f"- {t}" for t in raw_news_list])
        return (f"{market_summary}\n{global_data}\n{fed_data}\n{top_purchases}\n\n{news_ctx}", "ì¼ê°„(Daily)")
    else:
        # Weekly: ì´ë²ˆ ì£¼ ì¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 7ì¼)
        # Monthly: ì´ë²ˆ ë‹¬ ì£¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 5ê°œ)
        if r_type == "weekly":
            source_docs = get_past_reports('daily', 7)
            label = "ì£¼ê°„(Weekly)"
        else:
            source_docs = get_past_reports('weekly', 5)
            label = "ì›”ê°„(Monthly)"
            
        if not source_docs:
            source_docs = "âš ï¸ ë¶„ì„í•  í•˜ìœ„ ì£¼ê¸° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        return f"{source_docs}\n\n{market_summary}\n{global_data}\n{fed_data}", label

def generate_invest_report(r_type, input_content, config_data):
    """AIë¥¼ í˜¸ì¶œí•˜ì—¬ íˆ¬ì ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    
    if r_type == "daily":
        # ì¼ê°„: ë¯¸ë˜ ì „ëµ ì˜ˆìƒ (ìµœê·¼ 3ì¼ì¹˜ ì¼ê°„ + ìƒìœ„ ì£¼ê¸° ì°¸ì¡°)
        past_daily = get_past_reports('daily', 3)
        past_weekly = get_past_reports('weekly', 1)
        past_monthly = get_past_reports('monthly', 1)
        
        historical_context = (
            f"### [ ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ ]\n{past_daily}\n\n"
            f"### [ ìƒìœ„ ì£¼ê¸°(ì£¼ê°„/ì›”ê°„) íë¦„ ì°¸ì¡° ]\n{past_weekly}\n{past_monthly}"
        )
        
        base_prompt = REPORT_PROMPTS["daily"]["base_prompt"]
        specific_guideline = REPORT_PROMPTS["daily"]["specific_guideline"]
        structure_instruction = REPORT_PROMPTS["daily"]["structure_instruction"]
        
    elif r_type == "weekly":
        # ì£¼ê°„: í˜„ìƒ ì›ì¸ ê¸°ë¡ (ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_weekly = get_past_reports('weekly', 1)
        historical_context = f"### [ ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_weekly}"
        
        base_prompt = REPORT_PROMPTS["weekly"]["base_prompt"]
        specific_guideline = REPORT_PROMPTS["weekly"]["specific_guideline"]
        structure_instruction = REPORT_PROMPTS["weekly"]["structure_instruction"]
        
    else: # monthly
        # ì›”ê°„: êµ¬ì¡°ì  ë³€í™” ê¸°ë¡ (ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_monthly = get_past_reports('monthly', 1)
        historical_context = f"### [ ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_monthly}"
        
        base_prompt = REPORT_PROMPTS["monthly"]["base_prompt"]
        specific_guideline = REPORT_PROMPTS["monthly"]["specific_guideline"]
        structure_instruction = REPORT_PROMPTS["monthly"]["structure_instruction"]

    analysis_guideline = f"### [ {r_type} ë¶„ì„ ì§€ì¹¨ ]\n{specific_guideline}"

    system_prompt = (
        f"í˜„ì¬ ì„ë¬´: {r_type} íˆ¬ì ë³´ê³ ì„œ ì‘ì„±\n"
        f"ê¸°ì¤€ ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M')}\n\n"
        f"ë‹¹ì‹ ì€ {base_prompt}ì´ë©°, ì•„ë˜ ì§€ì¹¨ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"{analysis_guideline}\n\n"
        f"--- [ ì¤‘ìš” ì‚¬í•­ ] ---\n"
        f"* ì…ë ¥ëœ ì‹œì¥ ë°ì´í„°(KOSPI, ê¸€ë¡œë²Œ ì§€ìˆ˜ ë“±)ì—ëŠ” 'ì‹œê³„ì—´(ê³¼ê±°->í˜„ì¬)' ë³€í™” íë¦„ì´ í™”ì‚´í‘œ(->)ë¡œ ë‚˜ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        f"* ì´ ì‹œê³„ì—´ ì¶”ì´(Time-series)ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ê¸°ê°„(7ì¼, 14ì¼, 60ì¼) ë™ì•ˆì˜ ì¶”ì„¸(í•˜ë½ í›„ ë°˜ë“±, ì§€ì† ìƒìŠ¹ ë“±)ë¥¼ ë°˜ë“œì‹œ íŒŒì•…í•˜ê³  ë³´ê³ ì„œì— ë°˜ì˜í•˜ì‹­ì‹œì˜¤.\n\n"
        f"--- [ ì°¸ê³  ìë£Œ (Context) ] ---\n{historical_context}\n\n"
        f"--- [ ìµœì¢… ì§€ì‹œ ] ---\n"
        f"ì œê³µëœ ì‹œê³„ì—´ ì…ë ¥ ë°ì´í„°(Input Data)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"{structure_instruction}"
    )
    
    return get_ai_summary(title=f"{date.today()} {r_type.upper()} ë³´ê³ ì„œ", content=input_content, system_instruction=system_prompt, role="analyst", custom_config=config_data)