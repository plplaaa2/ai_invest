import json
import os
import re
import requests
import time
import math
import feedparser
from constants import *
from datetime import datetime, timedelta, date, timezone
from bs4 import BeautifulSoup
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS

try:
    from pykrx import stock
    HAS_PYKRX = True
except ImportError:
    HAS_PYKRX = False


KST = timezone(timedelta(hours=9))

def get_now_kst():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

# --- [0. ì‹œìŠ¤í…œ ê³µí†µ ê²½ë¡œ ì„¤ì •] ---
OPTIONS_PATH = "/data/options.json"
BASE_PATH = "/share/local_ai_analyst"
CONFIG_PATH = os.path.join(BASE_PATH, "rss_config.json")
PENDING_PATH = os.path.join(BASE_PATH, "pending")
REPORT_DIR = os.path.join(BASE_PATH, "reports")

def load_addon_config():
    if os.path.exists(OPTIONS_PATH):
        try:
            with open(OPTIONS_PATH, "r", encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {}

config = load_addon_config() 


# ì§€í‘œ ê´€ë ¨ ë³€ìˆ˜ (HA Addon êµ¬ì„±ì—ì„œ ë¡œë“œ)
INFLUX_URL = config.get("influx_url", "http://192.168.1.105:8086")
INFLUX_TOKEN = config.get("influx_token", "")
INFLUX_ORG = "home_assistant"
INFLUX_BUCKET = "financial_data"

client = None
write_api = None
query_api = None

print(f"âœ… InfluxDB ì„¤ì • ë¡œë“œ ì™„ë£Œ: {INFLUX_URL}")

if INFLUX_TOKEN:
    try:
        client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
        write_api = client.write_api(write_options=SYNCHRONOUS)
        query_api = client.query_api()
        print(f"âœ… InfluxDB ì—°ê²° ì„±ê³µ: {INFLUX_URL}")
    except Exception as e:
        print(f"âŒ InfluxDB ì—°ê²° ì‹¤íŒ¨: {e}")


# í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§
openai_key = config.get("openai_api_key", "")
gemini_key = config.get("gemini_api_key", "")
headers = {"Content-Type": "application/json"}

# ğŸ¯ 2. Cloud LLM ëª¨ë“œ íŒì • ë¡œì§ ë³´ì™„
if openai_key or gemini_key:
    if openai_key:
        headers["Authorization"] = f"Bearer {openai_key}"
    print(f"ğŸš€ Cloud LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤. (OpenAI: {'OK' if openai_key else 'NO'}, Gemini: {'OK' if gemini_key else 'NO'})")
else:
    print("ğŸ  Local LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤ (API í‚¤ ì—†ìŒ).")
        

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def safe_float(v):
    if v is None or v == "" or v == "-": return 0.0
    try:
        clean_v = re.sub(r'[^\d.-]', '', str(v))
        return float(clean_v) if clean_v else 0.0
    except: return 0.0

def save_to_influx(symbol, data, current_time):
    point = Point("financial_metrics").tag("symbol", symbol)
    for f, v in data.items(): point.field(f, float(v))
    point.time(current_time)
    if write_api:
        try:
            write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
            return True
        except Exception as e:
            print(f"âš ï¸ InfluxDB ì“°ê¸° ì—ëŸ¬ ({symbol}): {e}")
    return False
    
def save_report_to_file(content, section_name):
    # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ì„¸ë¶„í™”
    base_dir = REPORT_DIR
    dir_map = {
        'daily': '01_daily', 'weekly': '02_weekly', 
        'monthly': '03_monthly', 'yearly': '04_yearly'
    }
    # section_nameì´ ë§µì— ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë” ì‚¬ìš©
    subdir = dir_map.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(base_dir, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    # 2. íŒŒì¼ëª… ìƒì„± ë° ì €ì¥ (ê¸°ë¡ìš©)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"{timestamp}_{section_name.replace(' ', '_')}.txt"
    filepath = os.path.join(report_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    # 3. ğŸ¯ AI ì°¸ì¡°ìš© Latest íŒŒì¼ ê°±ì‹  (ê³ ì • ê²½ë¡œ)
    latest_path = os.path.join(report_dir, "latest.txt")
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(content)

    # 4. ğŸ§¹ ê³„ì¸µí˜• ìë™ ì •ì œ (Purge) ë¡œì§
    # ê·œì¹™: Daily(7ì¼), Weekly(30ì¼), Monthly(365ì¼) ë³´ê´€
    purge_rules = {'01_daily': 9, '02_weekly': 35, '03_monthly': 370}
    if subdir in purge_rules:
        limit_days = purge_rules[subdir]
        threshold = time.time() - (limit_days * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue # ìµœì‹  ë§¥ë½ì€ ë³´í˜¸
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
                
    return filepath
    
def load_historical_contexts():
    """íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì‘ë™í•˜ë©°, AIì—ê²Œ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•©ë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    
    # 1. ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ ë¡œë“œ (DAILY_LOG í™•ì¥)
    daily_context = ""
    daily_dir = os.path.join(base_dir, '01_daily')
    
    if os.path.exists(daily_dir):
        # latest.txt ì œì™¸í•˜ê³  ë‚ ì§œ ì—­ìˆœ ì •ë ¬
        files = sorted([f for f in os.listdir(daily_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        recent_files = files[:3] # ìµœê·¼ 3ê°œ
        
        if recent_files:
            daily_context += "\n<RECENT_DAILY_LOGS (Last 3 Days)>\n"
            for fname in recent_files:
                fpath = os.path.join(daily_dir, fname)
                try:
                    with open(fpath, "r", encoding="utf-8") as f:
                        content = f.read()
                        # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í† í° ì ˆì•½ (ê° 1000ì)
                        daily_context += f"--- [ {fname} ] ---\n{content[:1000]}...\n\n"
                except: pass
        else:
            daily_context += "\n<DAILY_LOG>: ì•„ì§ ìƒì„±ëœ ì¼ê°„ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n"
    else:
        daily_context += "\n<DAILY_LOG>: ì¼ê°„ ë³´ê³ ì„œ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

    # 2. ìƒìœ„ ì£¼ê¸° ë¦¬í¬íŠ¸ (ì£¼ê°„, ì›”ê°„, ì—°ê°„)
    # latest.txtë¥¼ ìš°ì„  ì°¸ì¡°í•˜ë˜, ì—†ìœ¼ë©´ í´ë” ë‚´ ê°€ì¥ ìµœì‹  íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    period_map = {
        'WEEKLY_MOMENTUM': '02_weekly',
        'MONTHLY_THEME': '03_monthly',
        'YEARLY_STRATEGY': '04_yearly'
    }
    
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    context_text += daily_context
    
    for label, folder_name in period_map.items():
        folder_path = os.path.join(base_dir, folder_name)
        target_content = ""
        found_file = ""

        if os.path.exists(folder_path):
            # 1ìˆœìœ„: latest.txt ì‹œë„
            latest_p = os.path.join(folder_path, 'latest.txt')
            if os.path.exists(latest_p):
                try:
                    with open(latest_p, "r", encoding="utf-8") as f:
                        target_content = f.read()
                        found_file = "latest.txt"
                except: pass
            
            # 2ìˆœìœ„: ì‹¤íŒ¨ ì‹œ ê°€ì¥ ìµœì‹  ë‚ ì§œ íŒŒì¼ ê²€ìƒ‰
            if not target_content:
                try:
                    files = sorted([f for f in os.listdir(folder_path) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
                    if files:
                        with open(os.path.join(folder_path, files[0]), "r", encoding="utf-8") as f:
                            target_content = f.read()
                            found_file = files[0]
                except: pass
        
        # ë‚´ìš© ì¶”ê°€
        if len(target_content.strip()) > 10:
            # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í† í° ì ˆì•½ (ì£¼ê°„/ì›”ê°„ì€ ì¤‘ìš”í•˜ë¯€ë¡œ 2000ì ì •ë„)
            context_text += f"\n<{label} - {found_file}>\n{target_content[:2000]}\n"
        else:
            context_text += f"\n<{label}>: í•´ë‹¹ ì£¼ê¸°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.\n"
            
    return context_text
    
def load_data():
    """ì„œë¹„ìŠ¤ ì„¤ì •(RSS, AI ëª¨ë¸ ë“±)ì„ ë¡œë“œí•˜ê³  ë¯¸ì¡´ì¬ ì‹œ ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    default_structure = {
        "feeds": [], 
        "update_interval": 10, 
        "view_range": "ì‹¤ì‹œê°„", 
        "retention_days": 7,
        "report_news_count": 100, 
        "report_auto_gen": True, 
        "report_gen_time": "08:00", 
        "report_days": 3,
        
        # ğŸ¯ ë‰´ìŠ¤ íŒë… ëª¨ë¸ ì„¤ì • (Filter)
        "filter_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.1,  # ğŸ’¡ íŒë…ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
            "prompt": "íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ê°€ ê±°ì‹œê²½ì œë‚˜ ìœ ë™ì„±ì— ì¤‘ìš”í•œì§€ íŒë…í•˜ì—¬ 0~5ì ì„ ë§¤ê¸°ì„¸ìš”."
        },
        
        # ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ëª¨ë¸ ì„¤ì • (Analyst)
        "analyst_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.3,  # ğŸ’¡ ë³´ê³ ì„œëŠ” ì•½ê°„ì˜ í†µì°°ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ 0.3~0.5 ê¶Œì¥
            "prompt": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ì§€í‘œì™€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ˜ìµ ì „ëµì„ ì œì‹œí•˜ì„¸ìš”."
        }
    }
    
    # 1. íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ë³µêµ¬)
    if not os.path.exists(CONFIG_PATH):
        try:
            os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump(default_structure, f, indent=4, ensure_ascii=False)
            print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: {CONFIG_PATH}")
            return default_structure
        except:
            return default_structure

    # 2. íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ ë° ëˆ„ë½ëœ í‚¤ ë³´ì •
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            loaded = json.load(f)
            # ìƒˆë¡œìš´ ê¸°ëŠ¥(ì˜¨ë„ ë“±)ì´ ì¶”ê°€ë˜ì–´ í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ ë³‘í•©
            for key, val in default_structure.items():
                if key not in loaded: 
                    loaded[key] = val
                elif isinstance(val, dict): # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬(ëª¨ë¸ ì„¤ì •) ë‚´ë¶€ í‚¤ ë³´ì •
                    for sub_key, sub_val in val.items():
                        if sub_key not in loaded[key]:
                            loaded[key][sub_key] = sub_val
            return loaded
    except:
        return default_structure

# ê³µí†µ ë°ì´í„° ê°ì²´ (ëª¨ë“  ëª¨ë“ˆì—ì„œ ê³µìœ )
data = load_data()

# common.py ì— ì¶”ê°€
def calculate_and_save_sgi(write_api, bucket, sgi_data_dict):
    """
    SGI 2.0 ë¬¼ë¦¬ ëª¨ë¸ ê³„ì‚° ë° InfluxDB ì €ì¥ ëª¨ë“ˆ
    """
    # 1. ë¬¼ë¦¬ëŸ‰ ê³„ì‚°
    delta_idx = sgi_data_dict['KOSPI']['curr'] - sgi_data_dict['KOSPI']['prev']
    safe_delta = delta_idx if abs(delta_idx) > 0.1 else (0.1 if delta_idx >= 0 else -0.1)

    g_f = max(-100, min(100, sgi_data_dict['KOR_NET_FOR']['curr'] / safe_delta))
    g_i = max(-100, min(100, sgi_data_dict['KOR_NET_INST']['curr'] / safe_delta))
    g_r = max(-100, min(100, sgi_data_dict['KOR_NET_RETAIL']['curr'] / safe_delta))
    
    # 3ê°œì›” í‰ê·  í™˜ìœ¨ ê¸°ë°˜ ì˜¤ë©”ê°€ ì‚°ì¶œ
    fx_hist = sgi_data_dict['USD_KRW']['hist']
    curr_fx = sgi_data_dict['USD_KRW']['curr']
    avg_fx_3m = sum([curr_fx] + fx_hist) / (len(fx_hist) + 1) if fx_hist else 1440.0
    omega = max(0.5, min(1.5, avg_fx_3m / curr_fx)) if curr_fx > 0 else 1.0
    
    sgi_score = ((g_f * 0.6) + (g_i * 0.3) - (g_r * 0.1)) * omega

# --- [common.py ë‚´ë¶€: 250ë¼ì¸ ë¶€ê·¼ ì €ì¥ ë¡œì§ êµì •] ---
    # point ë³€ìˆ˜ë¶€í„° write_apiê¹Œì§€ ì•ë¶€ë¶„ ê³µë°±ì„ ë™ì¼í•˜ê²Œ ë§ì¶”ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

    point = Point("market_physics") \
        .tag("symbol", "KOSPI_SGI") \
        .field("sgi_score", float(sgi_score)) \
        .field("g_foreign", float(g_f)) \
        .field("g_inst", float(g_i)) \
        .field("g_retail", float(g_r)) \
        .field("omega", float(omega)) \
        .field("avg_fx_3m", float(avg_fx_3m)) \
        .time(datetime.utcnow(), WritePrecision.S) 
    
    if write_api:
        write_api.write(bucket=bucket, record=point)
    
    return sgi_score, g_f, g_i, g_r, omega, avg_fx_3m
    
def get_sgi_inertia(query_api, bucket, days=5):
    """
    InfluxDBì—ì„œ ê³¼ê±° nì¼ì¹˜ SGI ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ 'ê´€ì„±(Inertia)'ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    [2026-02-07] ì¥ì´ ì—´ë¦¬ì§€ ì•ŠëŠ” ë‚ ì„ ê³ ë ¤í•˜ì—¬ ìµœê·¼ nì¼ì˜ í‰ê·  ì—ë„ˆì§€ í•©ì„ ì‚°ì¶œí•©ë‹ˆë‹¤.
    """
    # ğŸ¯ ì¿¼ë¦¬ ì„¤ëª…: ìµœê·¼ 'days'ì¼ ë™ì•ˆì˜ sgi_score í•„ë“œë¥¼ ê°€ì ¸ì™€ ì¼ë³„ í‰ê· ì„ ë‚¸ ë’¤ í•©ì‚°í•©ë‹ˆë‹¤.
    query = f'''
    from(bucket: "{bucket}")
    |> range(start: -{days}d)
    |> filter(fn: (r) => r._measurement == "market_physics" and r._field == "sgi_score")
    |> aggregateWindow(every: 1d, fn: mean, createEmpty: false)
    '''
    
    try:
        result = query_api.query(query)
        # ëª¨ë“  í…Œì´ë¸”ê³¼ ë ˆì½”ë“œë¥¼ ìˆœíšŒí•˜ë©° ê°’ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŠµë‹ˆë‹¤.
        scores = []
        for table in result:
            for record in table.records:
                val = record.get_value()
                if val is not None:
                    scores.append(val)
        
        # ğŸ¯ ê´€ì„± ì‚°ì¶œ: ëˆ„ì ëœ ì—ë„ˆì§€ì˜ ì´í•©
        # ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš° 0.0ì„ ë°˜í™˜í•˜ì—¬ UI ì—ëŸ¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        inertia_val = sum(scores) if scores else 0.0
        return inertia_val
        
    except Exception as e:
        # DB ì—°ê²° ì‹¤íŒ¨ ë“± ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  0.0 ë°˜í™˜
        print(f"SGI ê´€ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return 0.0

def get_metric_data(symbol, days=2):
    """
    InfluxDBì—ì„œ íŠ¹ì • ì‹¬ë³¼ì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    UI(app.py)ì™€ ë°±ì—”ë“œ(stock_collector.py)ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    try:
        # common.py ì „ì—­ query_api ì‚¬ìš©
        if query_api is None:
            return {}, [], "N/A", None

        # 1. ìˆ˜ê¸‰/ê¸ˆë¦¬/ì§€ìˆ˜ ì„±ê²©ì— ë”°ë¥¸ í•„í„° ìµœì í™”
        # ê¸ˆë¦¬(RATE)ë‚˜ ë§¤í¬ë¡œ(MACRO) ì§€í‘œëŠ” ì£¼ë¡œ price í•„ë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        field_filter = 'r._field == "price" or r._field == "value" or r._field == "volume"'
        if "RATE" in symbol or "UNRATE" in symbol or "CPI" in symbol:
            field_filter = 'r._field == "price"'

        query = (
            f'from(bucket: "{INFLUX_BUCKET}") '
            f'|> range(start: -{days}d) '
            f'|> filter(fn: (r) => r.symbol == "{symbol}") '
            f'|> filter(fn: (r) => {field_filter}) '
            f'|> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")'
        )
        
        result = query_api.query(query)
        p_history, m, l_time = [], {}, "N/A"
        p_records = [record for table in result for record in table.records]
        
        now_kst = datetime.utcnow() + timedelta(hours=9)
        today_kst = now_kst.date()
        
        # ë¯¸êµ­ ì‹œì¥ ì§€í‘œ ì—¬ë¶€ íŒë‹¨
        is_us_market = any(x in symbol for x in ["NASDAQ", "DJI", "SP500", "SOX", "US_", "USA_", "FED_", "RRP", "TGA"])
        prev_val = None
        
        for record in p_records:
            r_time_kst = record.get_time().replace(tzinfo=None) + timedelta(hours=9)
            
            # 2. ë°ì´í„° ì¶”ì¶œ (ìˆ˜ê¸‰ ë°ì´í„°ëŠ” ë³´í†µ 'value'ì—, ì§€ìˆ˜ ë°ì´í„°ëŠ” 'price'ì— ì €ì¥ë¨)
            p_val = record.values.get('price')
            if p_val is None:
                p_val = record.values.get('value')
                
            if p_val is not None:
                p_history.append(p_val)
                
                # ì „ì¼ ì¢…ê°€(ê¸°ì¤€ê°€) íŒì •
                if is_us_market:
                    if r_time_kst < (now_kst - timedelta(hours=3)):
                        prev_val = p_val
                else:
                    if r_time_kst.date() < today_kst:
                        prev_val = p_val
                
                # ì „ì²´ ë ˆì½”ë“œ ë³µì‚¬ (volume, value ë“± í¬í•¨)
                m = record.values.copy()
                if 'price' not in m: m['price'] = p_val # UI í˜¸í™˜ì„± ìœ ì§€
                    
                l_time = r_time_kst.strftime('%m-%d %H:%M')

        # 3. ê¸°ì¤€ê°€ í™•ì •
        final_prev = prev_val if prev_val is not None else (p_history[0] if p_history else 0)
        m['prev_close'] = final_prev 
        
        return m, p_history, l_time, query_api

    except Exception as e:
        print(f"âŒ {symbol} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}, [], "N/A", None

def clean_html(raw_html):
    if not raw_html: return "ìš”ì•½ ë‚´ìš© ì—†ìŒ"
    soup = BeautifulSoup(raw_html, "html.parser")
    for s in soup(['style', 'script', 'span']): s.decompose()
    return re.sub(r'\s+', ' ', soup.get_text()).strip()

def is_filtered(title, g_inc, g_exc, l_inc="", l_exc=""):
    """ì œëª©(Title)ë§Œ ê²€ì‚¬í•˜ëŠ” ì´ˆê²½ëŸ‰ í•„í„°"""
    text = title.lower().strip()
    
    # ì œì™¸ í•„í„° (Exclude)
    exc_tags = [t.strip().lower() for t in (g_exc + "," + l_exc).split(",") if t.strip()]
    if any(t in text for t in exc_tags): 
        return False
    
    # í¬í•¨ í•„í„° (Include)
    g_inc_tags = [t.strip().lower() for t in g_inc.split(",") if t.strip()]
    if g_inc_tags and not any(t in text for t in g_inc_tags):
        return False
        
    l_inc_tags = [t.strip().lower() for t in l_inc.split(",") if t.strip()]
    if l_inc_tags and not any(t in text for t in l_inc_tags):
        return False
    
    return True

def save_data(new_data):
    """ë³€ê²½ëœ ì„¤ì • ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
    with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)

def load_pending_files(range_type, target_feed=None, config_data=None):
    """JSON ì €ì¥ ë°©ì‹ì— ìµœì í™”ëœ ë¡œë” (ì„¤ì • ê°ì²´ ì£¼ì… ê°€ëŠ¥)"""
    news_list = []
    if not os.path.exists(PENDING_PATH): return news_list
    
    # ì„¤ì • ì†ŒìŠ¤ ê²°ì • (ì¸ì ìš°ì„ , ì—†ìœ¼ë©´ ì „ì—­ data)
    cfg = config_data if config_data else data
        
    now_kst = get_now_kst()
    today_date = now_kst.date()
    one_week_ago = now_kst - timedelta(days=7)
    
    all_files = sorted(os.listdir(PENDING_PATH), reverse=True)

    for filename in all_files:
        if not filename.endswith(".json"): continue
        try:
            with open(os.path.join(PENDING_PATH, filename), 'r', encoding='utf-8') as f:
                data_json = json.load(f)
                pub_dt = datetime.strptime(data_json['pub_dt'], '%Y-%m-%d %H:%M:%S').replace(tzinfo=KST)
                
                if range_type == "ì˜¤ëŠ˜" and pub_dt.date() != today_date: continue
                if range_type == "ì¼ì£¼ì¼" and pub_dt < one_week_ago: continue
                
                l_inc = target_feed.get('include', "") if target_feed else ""
                l_exc = target_feed.get('exclude', "") if target_feed else ""
                
                if not is_filtered(data_json['title'], cfg.get("global_include", ""), cfg.get("global_exclude", ""), l_inc, l_exc):
                    continue
                
                news_list.append({
                    "title": data_json['title'], "link": data_json['link'], 
                    "published": data_json['pub_dt'], "summary": data_json['summary'], 
                    "pub_dt": pub_dt, "source": data_json['source']
                })
        except: continue
            
    news_list.sort(key=lambda x: x['pub_dt'], reverse=True)
    return news_list

def get_ai_summary(title, content, system_instruction=None, role="filter", config_data=None):
    """ë‰´ìŠ¤ íŒë… ë˜ëŠ” ìš”ì•½ì„ ìœ„í•´ AI ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. (ì„¤ì • ê°ì²´ ì£¼ì… ê°€ëŠ¥)"""
    cfg_source = config_data if config_data else data
    now_time = get_now_kst().strftime('%Y-%m-%d %H:%M:%S')
    
    cfg = cfg_source.get("filter_model") if role == "filter" else cfg_source.get("analyst_model")
    base_url = cfg.get("url", "").rstrip('/')
    model_name = cfg.get("name")
    user_prompt = system_instruction if system_instruction else cfg.get("prompt", "")
    final_role = f"í˜„ì¬ ì‹œê°: {now_time}\në¶„ì„ ì§€ì¹¨: {user_prompt}"

    is_direct_google = "googleapis.com" in base_url
    api_key = cfg.get("key")
    if not api_key:
        api_key = config.get("gemini_api_key", "") if (is_direct_google or "gemini" in model_name.lower()) else config.get("openai_api_key", "")

    if is_direct_google:
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": f"ì‹œìŠ¤í…œ ì§€ì¹¨: {final_role}\n\nì‚¬ìš©ì ì…ë ¥:\nì œëª©: {title}\në³¸ë¬¸: {content}"}]}],
            "generationConfig": {"temperature": cfg.get("temperature", 0.3)}
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key: headers["Authorization"] = f"Bearer {api_key}"
        payload = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": final_role},
                {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {content}"}
            ],
            "temperature": cfg.get("temperature", 0.3)
        }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=600)
        resp.raise_for_status()
        result = resp.json()
        if "candidates" in result: return result['candidates'][0]['content']['parts'][0]['text']
        else: return result['choices'][0]['message']['content']
    except Exception as e:
        return f"âŒ [ERROR] AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

def get_trading_ranking(start_dt, end_dt):
    """pykrxë¥¼ ì´ìš©í•´ ì™¸êµ­ì¸/ê¸°ê´€ ìˆœë§¤ìˆ˜/ë§¤ë„ ìƒìœ„ ì¢…ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not HAS_PYKRX: return ""
    
    try:
        s_str = start_dt.strftime("%Y%m%d")
        e_str = end_dt.strftime("%Y%m%d")
        
        # 1. ì™¸êµ­ì¸ (ì „ì²´ ì‹œì¥)
        df_for = stock.get_market_net_purchases_of_equities_by_ticker(s_str, e_str, "ALL", "ì™¸êµ­ì¸")
        if df_for.empty: return ""
        
        top_for_buy = df_for.sort_values(by='ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ', ascending=False).head(10)
        top_for_sell = df_for.sort_values(by='ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ', ascending=True).head(10)
        
        # 2. ê¸°ê´€ (ì „ì²´ ì‹œì¥)
        df_inst = stock.get_market_net_purchases_of_equities_by_ticker(s_str, e_str, "ALL", "ê¸°ê´€í•©ê³„")
        top_inst_buy = df_inst.sort_values(by='ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ', ascending=False).head(10)
        top_inst_sell = df_inst.sort_values(by='ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ', ascending=True).head(10)
        
        def fmt(df):
            return ", ".join([f"{row['ì¢…ëª©ëª…']}({row['ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']/1e8:+.1f}ì–µ)" for _, row in df.iterrows()])

        res = f"### [ ìˆ˜ê¸‰ ì£¼ë„ì£¼ Top 10 ({start_dt.strftime('%m-%d')} ~ {end_dt.strftime('%m-%d')}) ]\n"
        res += f"- ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜: {fmt(top_for_buy)}\n"
        res += f"- ì™¸êµ­ì¸ ìˆœë§¤ë„: {fmt(top_for_sell)}\n"
        res += f"- ê¸°ê´€ ìˆœë§¤ìˆ˜: {fmt(top_inst_buy)}\n"
        res += f"- ê¸°ê´€ ìˆœë§¤ë„: {fmt(top_inst_sell)}\n"
        return res + "\n"
    except Exception as e:
        print(f"âš ï¸ pykrx ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return ""

def generate_market_report(r_type, config_data):
    """
    í†µí•© ë³´ê³ ì„œ ìƒì„± ì—”ì§„
    UI(app.py)ì™€ ë°±ì—”ë“œ(stock_collector.py)ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    now_kst = get_now_kst()
    
    # 1. ê¸°ê°„ ì„¤ì •
    # ì§€í‘œ: ì¶”ì„¸ë¥¼ ë³´ê¸° ìœ„í•´ ë„‰ë„‰í•˜ê²Œ ì¡ìŒ
    metric_lookback_map = {"daily": 7, "weekly": 30, "monthly": 365}
    m_days = metric_lookback_map.get(r_type, 7)
    
    # ë‰´ìŠ¤: í•´ë‹¹ ì£¼ê¸° ë™ì•ˆì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_lookback_days = 3 if (r_type == "daily" and now_kst.weekday() in [5, 6, 0]) else 1
    if r_type == "weekly": news_lookback_days = 7
    if r_type == "monthly": news_lookback_days = 30
    
    # 2. ë°ì´í„° ìˆ˜ì§‘
    # [A] ì—­ì‚¬ì  ë§¥ë½
    historical_context = load_historical_contexts()
    
    # [B] ì§€í‘œ ë°ì´í„° (InfluxDB)
    metric_context = f"### [ ì£¼ìš” ì‹œì¥ ì§€í‘œ ë¶„ì„ ({r_type.upper()}, ì§€ë‚œ {m_days}ì¼ ì¶”ì„¸) ]\n"
    for sym in ALL_SYMBOLS:
        m_data, p_hist, _, _ = get_metric_data(sym, days=m_days + 1)
        if m_data and 'price' in m_data and len(p_hist) >= 2:
            curr = m_data['price']
            prev_close = p_hist[-2] # ì „ì¼ ì¢…ê°€
            start_val = p_hist[0]   # ê¸°ê°„ ì‹œì´ˆê°€
            
            daily_diff = ((curr - prev_close) / prev_close * 100) if prev_close != 0 else 0
            period_diff = ((curr - start_val) / start_val * 100) if start_val != 0 else 0
            
            name = display_names.get(sym, sym)
            metric_context += f"- {name}: {curr:,.2f} (ì „ì¼: {daily_diff:+.2f}%, ê¸°ê°„: {period_diff:+.2f}%)\n"

    # [C] ìˆ˜ê¸‰ ë­í‚¹ (pykrx)
    ranking_context = ""
    if HAS_PYKRX:
        target_date = now_kst - timedelta(days=news_lookback_days)
        ranking_context = get_trading_ranking(target_date, now_kst)

    # [D] ë‰´ìŠ¤ ë°ì´í„°
    raw_news = load_pending_files("ì¼ì£¼ì¼" if r_type != "monthly" else "ì „ì²´", config_data=config_data)
    target_dt = now_kst - timedelta(days=news_lookback_days)
    
    recent_news = [n for n in raw_news if n['pub_dt'] >= target_dt]
    recent_news.sort(key=lambda x: x['pub_dt'], reverse=True)
    
    news_limit = config_data.get("report_news_count", 100)
    final_news = recent_news[:news_limit]
    
    news_context = f"### [ ìµœê·¼ {news_lookback_days}ì¼ ì£¼ìš” ë‰´ìŠ¤ ]\n"
    for n in final_news:
        t_str = n['pub_dt'].strftime('%Y-%m-%d %H:%M')
        summary = clean_html(n.get('summary', ''))[:150]
        news_context += f"[{t_str}] {n['title']}\n   > {summary}\n"

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (app.pyì˜ ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ ì±„ìš©)
    council_instruction = config_data.get("council_prompt", "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
    
    if r_type == "daily":
        # [ì¼ê°„] ì‹¤ì „ ë§¤ë§¤ ë° ì¦‰ê°ì  ëŒ€ì‘ ì¤‘ì‹¬
        role_desc = (
            f"{council_instruction}\n"
            "ë‹¹ì‹ ì€ 'ì‹¤ì „ íˆ¬ì ì „ëµê°€'ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‹œì¥ì´ ê³¼ê±°ì˜ íë¦„(ì£¼ê°„/ì›”ê°„ ë§¥ë½)ì—ì„œ ë²—ì–´ë‚¬ëŠ”ì§€, "
            "ì•„ë‹ˆë©´ ì¶”ì„¸ë¥¼ ê°•í™”í–ˆëŠ”ì§€ íŒë‹¨í•˜ê³  ë‚´ì¼ì˜ êµ¬ì²´ì ì¸ í–‰ë™ ì§€ì¹¨ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤."
        )
        analysis_guideline = (
            "### [ ìë£Œ ë¶„ì„ ì§€ì¹¨ (Daily) ]\n"
            "1. ìˆ˜ì¹˜ ì ˆëŒ€ ìš°ì„ : ë‰´ìŠ¤ í†¤ë³´ë‹¤ 'ì›ì²œ ìˆ˜ê¸‰ ì§€í‘œ'ì˜ ìˆ˜ì¹˜ë¥¼ ìµœìš°ì„  íŒ©íŠ¸ë¡œ ì‚¼ìœ¼ì„¸ìš”.\n"
            "2. ì—°ì†ì„± ê²€ì¦: 'ê³¼ê±° ë¶„ì„ ê¸°ë¡'ì˜ ì „ë§ê³¼ ì˜¤ëŠ˜ ì§€í‘œë¥¼ ë¹„êµí•˜ì—¬ ì˜ˆì¸¡ ì ì¤‘ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ì„¸ìš”.\n"
            "3. ì¦‰ê°ì  ëŒ€ì‘: ë‚´ì¼ ì‹œì´ˆê°€ ê³µëµ, ë¹„ì¤‘ ì¶•ì†Œ ë“± êµ¬ì²´ì ì¸ ì•¡ì…˜ í”Œëœì„ ì œì‹œí•˜ì„¸ìš”.\n"
        )
        structure_instruction = (
            "### [ ì¼ê°„ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ ]\n"
            "1. ì‹œí™© ë¸Œë¦¬í•‘\n"
            "2. ì£¼ìš” ë‰´ìŠ¤ ë° ì˜¤í”¼ë‹ˆì–¸:ê²½ì œì  ì˜í–¥ë ¥ì´ í° ë‰´ìŠ¤ë‚˜ ì£¼ìš”ì¸ì‚¬ ë°œì–¸\n"
            "3. ìœ ë™ì„± ë¶„ì„: ìœ ë™ì„± ê´€ë ¨ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ìœ ë™ì„± ë¶„ì„(ì˜ˆ: í•œêµ­ -> ë¯¸êµ­, ìœ„í—˜ -> ì•ˆì „, AI -> ë°”ì´ì˜¤)\n"
            "4. ì¦ì‹œ ë¶„ì„: ì¦ì‹œ ê° ì‚°ì—…ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "5. ìì‚° ë¶„ì„: ì¦ì‹œ ì™¸ ìì‚°ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "6. í˜„ ì£¼ë ¥ì‚°ì—… ë° ë¯¸ë˜ìœ ë§ì‚°ì—… ì „ë§\n"
            "7. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘: ë‹¨ê¸°ì  ìœ„í—˜ ìš”ì†Œì™€ íšŒí”¼ ì „ëµ\n"
            "8. í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ë° íˆ¬ì ì „ëµ\n"
        )
    else:
        # [ì£¼ê°„/ì›”ê°„] íë¦„ ê¸°ë¡ ë° ë¯¸ë˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì‚¬ë£Œí™”
        period_label = "ì£¼ê°„" if r_type == "weekly" else "ì›”ê°„"
        role_desc = (
            f"{council_instruction}\n"
            f"ë‹¹ì‹ ì€ 'ê²½ì œ íë¦„ ê¸°ë¡ê´€'ì…ë‹ˆë‹¤. ì´ {period_label} ë³´ê³ ì„œëŠ” ë¯¸ë˜ ì‹œì ì—ì„œ í˜„ì¬ë¥¼ ë³µê¸°í•  ë•Œ "
            "ì°¸ê³ í•  ì¤‘ìš”í•œ 'ì‚¬ë£Œ(Historical Record)'ê°€ ë©ë‹ˆë‹¤. ë‹¨ìˆœ ë‚˜ì—´ë³´ë‹¤ëŠ” "
            "ì‹œì¥ì„ ì§€ë°°í–ˆë˜ 'í•µì‹¬ ì„œì‚¬(Narrative)'ì™€ 'êµ¬ì¡°ì  ë³€í™”'ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì¸ê³¼ê´€ê³„ë¥¼ ëª…í™•íˆ ê¸°ë¡í•˜ì„¸ìš”."
        )
        analysis_guideline = (
            f"### [ ìë£Œ ë¶„ì„ ì§€ì¹¨ ({r_type.title()}) ]\n"
            "1. íë¦„ íŒŒì•…: í•˜ë£¨í•˜ë£¨ì˜ ë“±ë½ë³´ë‹¤ ê¸°ê°„ ì „ì²´ë¥¼ ê´€í†µí•˜ëŠ” ì¶”ì„¸ë¥¼ ì½ì–´ë‚´ì„¸ìš”.\n"
            "2. ë³€ê³¡ì  ê¸°ë¡: ì¶”ì„¸ê°€ ë°”ë€Œê±°ë‚˜ ê°•í™”ëœ ê²°ì •ì  ì‚¬ê±´(Event)ì„ ì°¾ì•„ ê¸°ë¡í•˜ì„¸ìš”.\n"
            "3. ë¯¸ë˜ ì˜ˆì¸¡ì˜ ê·¼ê±°: ì´ íë¦„ì´ ë‹¤ìŒ ì£¼ê¸°ë¡œ ì–´ë–»ê²Œ ì´ì–´ì§ˆì§€ ë…¼ë¦¬ì  ê·¼ê±°ë¥¼ ë‚¨ê¸°ì„¸ìš”.\n"
        )
        structure_instruction = (
            f"### [ {period_label} ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ ]\n"
            "1. ê¸°ê°„ í•µì‹¬ ìš”ì•½: ì´ë²ˆ ê¸°ê°„ì„ ê´€í†µí•˜ëŠ” í•œ ë¬¸ì¥ ì •ì˜ ë° ì´í‰\n"
            "2. ì£¼ìš” íƒ€ì„ë¼ì¸: ì‹œì¥ì˜ ë°©í–¥ì„ ê²°ì •ì§€ì€ ê²°ì •ì  ë‰´ìŠ¤ë‚˜ ì‚¬ê±´ ë³µê¸°\n"
            "3. ë§¤í¬ë¡œ ë° ìˆ˜ê¸‰ ë³€í™”: ê¸°ê°„ ë™ì•ˆì˜ ê¸ˆë¦¬, í™˜ìœ¨, ìˆ˜ê¸‰ ì£¼ì²´ì˜ íƒœë„ ë³€í™” ë¶„ì„\n"
            "4. ì£¼ë„ ì„¹í„° ë° ì†Œì™¸ ì„¹í„°: ìê¸ˆì´ ì ë¦° ê³³ê³¼ ë¹ ì ¸ë‚˜ê°„ ê³³ì˜ êµ¬ì¡°ì  ì´ìœ \n"
            "5. ë‹¤ìŒ ì£¼ê¸° ì „ë§: í˜„ì¬ íë¦„ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ (ìƒìŠ¹/í•˜ë½/íš¡ë³´)\n"
            "6. ì¤‘ì¥ê¸° ëŒ€ì‘ ì „ëµ: ê¸´ í˜¸í¡ì—ì„œì˜ ìì‚° ë°°ë¶„ ë° ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì¡°ì–¸\n"
        )

    full_instruction = (
        f"{role_desc}\n"
        f"í˜„ì¬ ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M:%S')}\n"
        f"{analysis_guideline}\n"
        f"--- [ 1. ê³¼ê±° ë¶„ì„ ê¸°ë¡ ] ---\n{historical_context}\n\n"
        f"--- [ 2. ìˆ˜ê¸‰ ë° ë‰´ìŠ¤ ë°ì´í„° ] ---\n{ranking_context}\n{news_context}\n\n"
        f"--- [ 3. ì›ì²œ ìˆ˜ê¸‰ ì§€í‘œ (ìµœìš°ì„ ) ] ---\n{metric_context}\n\n"
        f"{structure_instruction}"
    )

    # 4. AI í˜¸ì¶œ
    return get_ai_summary(
        title=f"{r_type.upper()} Report", content=news_context, 
        system_instruction=full_instruction, role="analyst", config_data=config_data
    )