
import json
import os
import re
import requests
import time
import math
import io
import pandas as pd
import feedparser
from datetime import datetime, timedelta, date, timezone
from bs4 import BeautifulSoup

try:
    import yfinance as yf
except ImportError:
    yf = None

KST = timezone(timedelta(hours=9))

def get_now_kst():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

# --- [0. ì‹œìŠ¤í…œ ê³µí†µ ê²½ë¡œ ì„¤ì •] ---
OPTIONS_PATH = "/data/options.json"
BASE_PATH = "/share/ai_analyst"
CONFIG_PATH = os.path.join(BASE_PATH, "rss_config.json")
PENDING_PATH = os.path.join(BASE_PATH, "pending")
REPORT_DIR = os.path.join(BASE_PATH, "reports")

def load_addon_config():
    if os.path.exists(OPTIONS_PATH):
        try:
            with open(OPTIONS_PATH, "r", encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {}

config = load_addon_config() 


# í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§
openai_key = config.get("openai_api_key", "")
gemini_key = config.get("gemini_api_key", "")
headers = {"Content-Type": "application/json"}

# ğŸ¯ 2. Cloud LLM ëª¨ë“œ íŒì • ë¡œì§ ë³´ì™„
if openai_key or gemini_key:
    if openai_key:
        headers["Authorization"] = f"Bearer {openai_key}"
    print(f"ğŸš€ Cloud LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤. (OpenAI: {'OK' if openai_key else 'NO'}, Gemini: {'OK' if gemini_key else 'NO'})")
else:
    print("ğŸ  Local LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤ (API í‚¤ ì—†ìŒ).")
        

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def safe_float(v):
    if v is None or v == "" or v == "-": return 0.0
    try:
        clean_v = re.sub(r'[^\d.-]', '', str(v))
        return float(clean_v) if clean_v else 0.0
    except: return 0.0

def save_to_influx(symbol, data, current_time):
    point = Point("financial_metrics").tag("symbol", symbol)
    for f, v in data.items(): point.field(f, float(v))
    point.time(current_time)
    if write_api:
        try:
            write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
            return True
        except Exception as e:
            print(f"âš ï¸ InfluxDB ì“°ê¸° ì—ëŸ¬ ({symbol}): {e}")
    return False
    
def save_report_to_file(content, section_name):
    # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ì„¸ë¶„í™”
    base_dir = REPORT_DIR
    dir_map = {
        'daily': '01_daily', 'weekly': '02_weekly', 
        'monthly': '03_monthly', 'yearly': '04_yearly'
    }
    # section_nameì´ ë§µì— ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë” ì‚¬ìš©
    subdir = dir_map.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(base_dir, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    # 2. íŒŒì¼ëª… ìƒì„± ë° ì €ì¥ (ê¸°ë¡ìš©)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"{timestamp}_{section_name.replace(' ', '_')}.txt"
    filepath = os.path.join(report_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    # 3. ğŸ¯ AI ì°¸ì¡°ìš© Latest íŒŒì¼ ê°±ì‹  (ê³ ì • ê²½ë¡œ)
    latest_path = os.path.join(report_dir, "latest.txt")
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(content)

    # 4. ğŸ§¹ ê³„ì¸µí˜• ìë™ ì •ì œ (Purge) ë¡œì§
    # ê·œì¹™: Daily(7ì¼), Weekly(30ì¼), Monthly(365ì¼) ë³´ê´€
    purge_rules = {'01_daily': 9, '02_weekly': 35, '03_monthly': 370}
    if subdir in purge_rules:
        limit_days = purge_rules[subdir]
        threshold = time.time() - (limit_days * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue # ìµœì‹  ë§¥ë½ì€ ë³´í˜¸
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
                
    return filepath
    
def load_historical_contexts():
    """íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì‘ë™í•˜ë©°, AIì—ê²Œ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•©ë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    
    for label, rel_path in dir_map.items():
        full_path = os.path.join(base_dir, rel_path)
        
        # ğŸ›¡ï¸ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë¡ì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: í•´ë‹¹ ì£¼ê¸°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        else:
            # ğŸ’¡ íŒŒì¼ì´ ì—†ì„ ë•Œ AIì—ê²Œ ì¤„ ë©”ì‹œì§€
            # AIê°€ "ê³¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë‹ˆ ì˜¤ëŠ˜ ìˆ˜ì¹˜ì— ë” ì§‘ì¤‘í•´ì„œ ë¶„ì„í•´ë¼"ë¼ê³  íŒë‹¨í•˜ê²Œ ìœ ë„í•©ë‹ˆë‹¤.
            context_text += f"\n<{label}>: ì‹œìŠ¤í…œ ë„ì… ì´ˆê¸° ë‹¨ê³„ë¡œ, ì•„ì§ {label} ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ê°€ìš©í•œ ìµœì‹  ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
            
    return context_text
    
def get_krx_summary_raw(ignore_cache=False):
    """KOSPI/KOSDAQ ì§€ìˆ˜ ë° KOSPI 3ëŒ€ ì£¼ì²´(ê°œì¸/ì™¸ì¸/ê¸°ê´€) ì¢…í•© ë¶„ì„"""
    results = {}
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, "krx_summary_v2.json")
    
    # ğŸ¯ ìºì‹œ ì²˜ë¦¬ (10ë¶„)
    if not ignore_cache and os.path.exists(cache_path):
        try:
            if time.time() - os.path.getmtime(cache_path) < 600:
                with open(cache_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except: pass

    try:
        from pykrx import stock
        from pykrx import bond
        now = get_now_kst()
        
        # ğŸ¯ í•œêµ­ ì¥ ì‹œê°„(09:00) ì „ì´ë©´ ì–´ì œ ë‚ ì§œë¥¼ ê¸°ì¤€ì¼ë¡œ ì„¤ì •
        if now.hour < 9:
            target_date = (now - timedelta(days=1)).strftime("%Y%m%d")
        else:
            target_date = now.strftime("%Y%m%d")
            
        # íœ´ì¼ ë“±ì„ ê³ ë ¤í•˜ì—¬ ë„‰ë„‰í•˜ê²Œ 14ì¼ ì „ë¶€í„° ì¡°íšŒ
        start_dt = (now - timedelta(days=14)).strftime("%Y%m%d")
        
        # 1. ì§€ìˆ˜ ë°ì´í„° (KOSPI/KOSDAQ)
        for code, name in [("1001", "KOSPI"), ("2001", "KOSDAQ")]:
            df = stock.get_index_ohlcv(start_dt, target_date, code)
            if not df.empty:
                last = df.iloc[-1]
                price = float(last['ì¢…ê°€'])
                pct = float(last['ë“±ë½ë¥ ']) if 'ë“±ë½ë¥ ' in df.columns else 0.0
                
                # ë“±ë½í­ ê³„ì‚° (ì¢…ê°€ì™€ ë“±ë½ë¥  ì—­ì‚°)
                prev = price / (1 + (pct / 100))
                diff = price - prev
                
                results[name] = {
                    "price": price, "pct": pct,
                    "amount": float(last['ê±°ë˜ëŒ€ê¸ˆ']) / 100_000_000,
                    "date": last.name.strftime("%m-%d"),
                    # ëŒ€ì‹œë³´ë“œ í˜¸í™˜ìš© í‚¤ ì¶”ê°€
                    "value": price,
                    "val_str": f"{price:,.2f}",
                    "delta_str": f"{diff:+.2f} ({pct:+.2f}%)"
                }

        # 2. KOSPI/KOSDAQ ì£¼ì²´ë³„ ê±°ë˜ëŒ€ê¸ˆ ë° Top 10 ì¢…ëª©
        if "KOSPI" in results:
            actual_date = df.index[-1].strftime("%Y%m%d") # ì‹¤ì œ ë°ì´í„° ë‚ ì§œ
            
            for mkt in ["KOSPI", "KOSDAQ"]:
                try:
                    # (A) ê±°ë˜ëŒ€ê¸ˆ í•©ê³„
                    df_inv = stock.get_market_trading_value_by_date(actual_date, actual_date, mkt)
                    if not df_inv.empty:
                        row = df_inv.iloc[-1]
                        for kor, eng in [('ê°œì¸', 'Individual'), ('ì™¸êµ­ì¸í•©ê³„', 'Foreigner'), ('ê¸°ê´€í•©ê³„', 'Institution')]:
                            val_bill = float(row[kor]) / 100_000_000
                            results[f"{mkt}_{eng}"] = {
                                "value": val_bill,
                                "val_str": f"{val_bill/10000:,.2f}ì¡°" if abs(val_bill) >= 10000 else f"{val_bill:,.0f}ì–µ"
                            }

                    # (B) ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ì¢…ëª©
                    for kor, eng in [("ê°œì¸", "Top_Individual"), ("ì™¸êµ­ì¸", "Top_Foreigner"), ("ê¸°ê´€í•©ê³„", "Top_Institution")]:
                        df_top = stock.get_market_net_purchases_of_equities(actual_date, actual_date, mkt, kor)
                        if not df_top.empty:
                            items = [f"{r['ì¢…ëª©ëª…']}({float(r['ì¢…ëª©ë³„ìˆœë§¤ìˆ˜ê¸ˆì•¡'])/100_000_000:,.0f}ì–µ)" for _, r in df_top.head(10).iterrows()]
                            results[f"{mkt}_{eng}"] = ", ".join(items)

                    # (C) ê³µë§¤ë„ ê±°ë˜ëŸ‰
                    df_short = stock.get_shorting_investor_volume_by_date(actual_date, actual_date, mkt)
                    if not df_short.empty:
                        s_row = df_short.iloc[-1]
                        results[f'{mkt}_Short'] = {"total": f"{s_row['í•©ê³„']:,.0f}ì£¼", "for": f"{s_row['ì™¸êµ­ì¸']:,.0f}ì£¼"}
                except: pass

            # (D) ì±„ê¶Œ ê¸ˆë¦¬
            try:
                df_bond = bond.get_otc_treasury_yields(actual_date)
                if not df_bond.empty:
                    for label, key in [("KR_3Y", "êµ­ê³ ì±„ 3ë…„"), ("KR_10Y", "êµ­ê³ ì±„ 10ë…„")]:
                        if key in df_bond.index:
                            val = float(df_bond.loc[key, "ìˆ˜ìµë¥ "])
                            diff = float(df_bond.loc[key, "ëŒ€ë¹„"])
                            results[label] = {
                                "value": val, "diff": diff,
                                "val_str": f"{val:.2f}%", "delta_str": f"{diff:+.2f}"
                            }
            except: pass

        # ìºì‹œ ì €ì¥ í›„ ë°˜í™˜
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False)
        return results

    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return results
    
def get_krx_market_data(r_type="daily"):
    """(í†µí•©) ì§€ìˆ˜, ìˆ˜ê¸‰, ê¸ˆë¦¬ ìš”ì•½ ë³´ê³ ì„œ (ê¸°ê°„ë³„ ë§ì¶¤)"""
    # ğŸ¯ ë³´ê³ ì„œ ìœ í˜•ë³„ ê¸°ê°„ ì„¤ì •
    if r_type == 'daily':
        fetch_days = 7      # ë°ì´í„° í™•ë³´: 1ì£¼ì¼
        comp_idx = -2       # ë³€í™” ê¸°ì¤€: ì „ì¼ ëŒ€ë¹„ (Daily Change)
        period_name = "ì¼ê°„(1D)"
    elif r_type == 'weekly':
        fetch_days = 14     # ë°ì´í„° í™•ë³´: 2ì£¼ì¼
        comp_idx = -6       # ë³€í™” ê¸°ì¤€: 1ì£¼ ì „ ëŒ€ë¹„ (Weekly Change, approx 5 trading days)
        period_name = "ì£¼ê°„(1W)"
    else: # monthly
        fetch_days = 60     # ë°ì´í„° í™•ë³´: 2ë‹¬
        comp_idx = -21      # ë³€í™” ê¸°ì¤€: 1ë‹¬ ì „ ëŒ€ë¹„ (Monthly Change, approx 20 trading days)
        period_name = "ì›”ê°„(1M)"

    data = get_krx_summary_raw() # ìµœì‹  ìˆ˜ê¸‰/ê¸ˆë¦¬ìš© (Snapshot)
    summary = f"### [ KRX ì‹œì¥ ì§€í‘œ ({period_name} ë³€ë™) ]\n"

    try:
        from pykrx import stock
        now = get_now_kst()
        target_date = now.strftime("%Y%m%d")
        if now.hour < 9: target_date = (now - timedelta(days=1)).strftime("%Y%m%d")
        start_dt = (now - timedelta(days=fetch_days)).strftime("%Y%m%d")

        for code, name in [("1001", "KOSPI"), ("2001", "KOSDAQ")]:
            df = stock.get_index_ohlcv(start_dt, target_date, code)
            if not df.empty and len(df) >= abs(comp_idx):
                curr = float(df.iloc[-1]['ì¢…ê°€'])
                # comp_idxê°€ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê°€ì¥ ì²« ë°ì´í„° ì‚¬ìš©
                prev_idx = comp_idx if len(df) >= abs(comp_idx) else 0
                prev = float(df.iloc[prev_idx]['ì¢…ê°€'])
                
                diff = curr - prev
                pct = (diff / prev) * 100
                summary += f"- {name}: {curr:,.2f} ({pct:+.2f}% / {period_name} ë³€ë™)\n"
    except Exception as e:
        summary += f"âš ï¸ ì§€ìˆ˜ ë°ì´í„° ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}\n"

    summary += f"- KOSPI ìˆ˜ê¸‰(ìˆœë§¤ìˆ˜): ê°œì¸ {data.get('KOSPI_Individual',{}).get('val_str','0ì–µ')}, ì™¸êµ­ì¸ {data.get('KOSPI_Foreigner',{}).get('val_str','0ì–µ')}, ê¸°ê´€ {data.get('KOSPI_Institution',{}).get('val_str','0ì–µ')}\n"
    summary += f"- KOSDAQ ìˆ˜ê¸‰(ìˆœë§¤ìˆ˜): ê°œì¸ {data.get('KOSDAQ_Individual',{}).get('val_str','0ì–µ')}, ì™¸êµ­ì¸ {data.get('KOSDAQ_Foreigner',{}).get('val_str','0ì–µ')}, ê¸°ê´€ {data.get('KOSDAQ_Institution',{}).get('val_str','0ì–µ')}\n"
    k3 = data.get('KR_3Y', {}).get('val_str', 'N/A')
    k10 = data.get('KR_10Y', {}).get('val_str', 'N/A')
    summary += f"- êµ­ê³ ì±„ ê¸ˆë¦¬: 3ë…„ë¬¼ {k3} | 10ë…„ë¬¼ {k10}\n"
    return summary

def get_krx_top_investors():
    """(í†µí•©) 3ëŒ€ ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ ìƒìœ„ ë° ê³µë§¤ë„ ë³´ê³ ì„œ"""
    data = get_krx_summary_raw()
    if not data: return ""
    
    report = "### [ KOSPI ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ]\n"
    report += f"- ğŸ‘¤ ê°œì¸: {data.get('KOSPI_Top_Individual', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸŒ ì™¸ì¸: {data.get('KOSPI_Top_Foreigner', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸ¢ ê¸°ê´€: {data.get('KOSPI_Top_Institution', 'ë°ì´í„° ì—†ìŒ')}\n"
    
    s_total = data.get('KOSPI_Short', {}).get('total', 'N/A')
    s_for = data.get('KOSPI_Short', {}).get('for', 'N/A')
    report += f"ğŸ“Š ê³µë§¤ë„: ì´ {s_total} (ì™¸ì¸ {s_for})\n"

    report += "\n### [ KOSDAQ ì£¼ì²´ë³„ ìˆœë§¤ìˆ˜ Top 10 ]\n"
    report += f"- ğŸ‘¤ ê°œì¸: {data.get('KOSDAQ_Top_Individual', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸŒ ì™¸ì¸: {data.get('KOSDAQ_Top_Foreigner', 'ë°ì´í„° ì—†ìŒ')}\n"
    report += f"- ğŸ¢ ê¸°ê´€: {data.get('KOSDAQ_Top_Institution', 'ë°ì´í„° ì—†ìŒ')}\n"
    
    s_total_kq = data.get('KOSDAQ_Short', {}).get('total', 'N/A')
    s_for_kq = data.get('KOSDAQ_Short', {}).get('for', 'N/A')
    report += f"ğŸ“Š ê³µë§¤ë„: ì´ {s_total_kq} (ì™¸ì¸ {s_for_kq})\n"
    return report

def load_data():
    """ì„œë¹„ìŠ¤ ì„¤ì •(RSS, AI ëª¨ë¸ ë“±)ì„ ë¡œë“œí•˜ê³  ë¯¸ì¡´ì¬ ì‹œ ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    default_structure = {
        "feeds": [], 
        "update_interval": 10, 
        "view_range": "ì‹¤ì‹œê°„", 
        "retention_days": 7,
        "report_news_count": 100, 
        "report_auto_gen": True, 
        "report_gen_time": "08:00", 
        "report_days": 3,
        
        # ğŸ¯ ë‰´ìŠ¤ íŒë… ëª¨ë¸ ì„¤ì • (Filter)
        "filter_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.1,  # ğŸ’¡ íŒë…ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
            "prompt": "íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ê°€ ê±°ì‹œê²½ì œë‚˜ ìœ ë™ì„±ì— ì¤‘ìš”í•œì§€ íŒë…í•˜ì—¬ 0~5ì ì„ ë§¤ê¸°ì„¸ìš”."
        },
        
        # ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ëª¨ë¸ ì„¤ì • (Analyst)
        "analyst_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.3,  # ğŸ’¡ ë³´ê³ ì„œëŠ” ì•½ê°„ì˜ í†µì°°ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ 0.3~0.5 ê¶Œì¥
            "prompt": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ì „ëµì„ ì œì‹œí•˜ì„¸ìš”."
        }
    }
    
    # 1. íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ë³µêµ¬)
    if not os.path.exists(CONFIG_PATH):
        try:
            os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump(default_structure, f, indent=4, ensure_ascii=False)
            print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: {CONFIG_PATH}")
            return default_structure
        except:
            return default_structure

    # 2. íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ ë° ëˆ„ë½ëœ í‚¤ ë³´ì •
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            loaded = json.load(f)
            # ìƒˆë¡œìš´ ê¸°ëŠ¥(ì˜¨ë„ ë“±)ì´ ì¶”ê°€ë˜ì–´ í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ ë³‘í•©
            for key, val in default_structure.items():
                if key not in loaded: 
                    loaded[key] = val
                elif isinstance(val, dict): # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬(ëª¨ë¸ ì„¤ì •) ë‚´ë¶€ í‚¤ ë³´ì •
                    for sub_key, sub_val in val.items():
                        if sub_key not in loaded[key]:
                            loaded[key][sub_key] = sub_val
            return loaded
    except:
        return default_structure

# ê³µí†µ ë°ì´í„° ê°ì²´ (ëª¨ë“  ëª¨ë“ˆì—ì„œ ê³µìœ )
data = load_data()


def get_global_market_data(r_type="daily"):
    """yfinanceë¥¼ í†µí•´ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not yf: return "âš ï¸ yfinance ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    end_dt = get_now_kst()
    
    # ğŸ¯ ë³´ê³ ì„œ ìœ í˜•ë³„ ê¸°ê°„ ë° ë¹„êµ ì‹œì  ì„¤ì •
    if r_type == 'daily': 
        days = 7
        comp_idx = -2 # ì „ì¼ ëŒ€ë¹„
    elif r_type == 'weekly': 
        days = 14
        comp_idx = -6 # 1ì£¼ ì „ ëŒ€ë¹„ (ì•½ 5ê±°ë˜ì¼)
    else: 
        days = 60
        comp_idx = -21 # 1ë‹¬ ì „ ëŒ€ë¹„ (ì•½ 20ê±°ë˜ì¼)
    
    start_dt = end_dt - timedelta(days=days + 5) # ì—¬ìœ  ìˆê²Œ ì¡°íšŒ
    
    tickers = {
        "ğŸ‡ºğŸ‡¸ ë¯¸êµ­ 3ëŒ€ ì§€ìˆ˜ & VIX": {
            "^GSPC": "S&P500", "^DJI": "Dow Jones", "^IXIC": "Nasdaq", 
            "^SOX": "SOX(ë°˜ë„ì²´)", "^VIX": "VIX"
        },
        "ğŸŒ ê¸€ë¡œë²Œ ì§€ìˆ˜": {
            "^N225": "Nikkei 225", "^GDAXI": "DAX", "^HSI": "Hang Seng"
        },
        "ğŸ’µ ê¸ˆë¦¬ & í™˜ìœ¨": {
            "^TNX": "ë¯¸êµ­ì±„ 10ë…„", "^TYX": "ë¯¸êµ­ì±„ 30ë…„", "^FVX": "ë¯¸êµ­ì±„ 5ë…„", 
            "KRW=X": "USD/KRW", "DX-Y.NYB": "ë‹¬ëŸ¬ ì¸ë±ìŠ¤", "JPY=X": "USD/JPY"
        },
        "ğŸ›¢ï¸ ì›ìì¬ & ì½”ì¸": {
            "CL=F": "WTI ì›ìœ ", "GC=F": "ê¸ˆ", "SI=F": "ì€", "HG=F": "êµ¬ë¦¬", 
            "BTC-USD": "ë¹„íŠ¸ì½”ì¸"
        }
    }
    
    all_symbols = [s for cat in tickers.values() for s in cat.keys()]
    report = f"### [ ğŸŒ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„° ({r_type.upper()} ê¸°ì¤€ ë³€ë™) ]\n"
    
    try:
        df = yf.download(all_symbols, start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), progress=False)['Close']
        for cat_name, items in tickers.items():
            report += f"#### {cat_name}\n"
            for sym, name in items.items():
                try:
                    if sym in df.columns:
                        series = df[sym].dropna()
                        if len(series) < 2: continue
                        
                        curr = float(series.iloc[-1])
                        # ë¹„êµ ëŒ€ìƒ ì¸ë±ìŠ¤ ì„¤ì • (ë°ì´í„° ë¶€ì¡± ì‹œ ì²« ë°ì´í„° ì‚¬ìš©)
                        target_idx = comp_idx if len(series) >= abs(comp_idx) else 0
                        prev = float(series.iloc[target_idx])
                        
                        chg_pct = ((curr - prev) / prev) * 100
                        chg_pct = ((curr - start) / start) * 100
                        report += f"- **{name}**: {curr:,.2f} ({chg_pct:+.2f}%, {days}ì¼ ë²”ìœ„: {series.min():,.2f}~{series.max():,.2f})\n"
                except: continue
            report += "\n"
        return report
    except Exception as e:
        return f"âš ï¸ ê¸€ë¡œë²Œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"

def get_global_financials_raw(ignore_cache=False):
    """ëŒ€ì‹œë³´ë“œìš© ê¸€ë¡œë²Œ ì§€ìˆ˜, í™˜ìœ¨, ì›ìì¬, ê¸ˆë¦¬ ë°ì´í„°ë¥¼ í†µí•© ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    print("ğŸ” [DEBUG] get_global_financials_raw ì§„ì…")
    if not yf: return {}
    
    # ğŸ¯ [NEW] ìºì‹± ì„¤ì • (10ë¶„)
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, "global_financials.json")
    
    if not ignore_cache and os.path.exists(cache_path):
        print("ğŸ” [DEBUG] get_global_financials_raw ìºì‹œ ì‚¬ìš©")
        try:
            if time.time() - os.path.getmtime(cache_path) < 600:
                with open(cache_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except: pass
    
    # ì£¼ìš” í‹°ì»¤ ë§¤í•‘
    tickers = {
        "S&P500": "^GSPC", "Dow Jones": "^DJI", "Nasdaq": "^IXIC", "VIX": "^VIX",
        "USD/KRW": "KRW=X", "USD/JPY": "JPY=X", 
        "WTI": "CL=F", "Gold": "GC=F", "Bitcoin": "BTC-USD",
        "US10Y": "^TNX", "US2Y": "^IRX" # ë¯¸êµ­ì±„ 10ë…„, 2ë…„
    }
    
    results = {}
    try:
        print("ğŸ” [DEBUG] get_global_financials_raw yfinance ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        end_dt = get_now_kst()
        start_dt = end_dt - timedelta(days=7)
        df = yf.download(list(tickers.values()), start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), progress=False)['Close']
        print("ğŸ” [DEBUG] get_global_financials_raw yfinance ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        for name, sym in tickers.items():
            if sym in df.columns:
                series = df[sym].dropna()
                if len(series) >= 2:
                    curr = float(series.iloc[-1])
                    prev = float(series.iloc[-2])
                    diff = curr - prev
                    print(f"ğŸ” [DEBUG] get_global_financials_raw {name} ê°€ê²©: {curr}, ë³€í™”: {diff}")
                    pct = (diff / prev) * 100
                    results[name] = {
                        "price": curr, "diff": diff, "pct": pct,
                        "val_str": f"{curr:,.2f}",
                        "delta_str": f"{diff:+.2f} ({pct:+.2f}%)"
                    }
        
        # ìºì‹œ ì €ì¥
        print("ğŸ” [DEBUG] get_global_financials_raw ìºì‹œ ì €ì¥")
        if results:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False)
    except: pass
    print("ğŸ” [DEBUG] get_global_financials_raw ì™„ë£Œ")
    return results

def get_fed_liquidity_raw():
    """FRED ë°ì´í„° ì›ë³¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (Dashboardìš©)"""
    print("ğŸ” [DEBUG] get_fed_liquidity_raw ì§„ì…")
    # ğŸ¯ [NEW] ìºì‹± ì„¤ì • (24ì‹œê°„ - í•˜ë£¨ 1íšŒ)
    cache_dir = os.path.join(BASE_PATH, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    print("ğŸ” [DEBUG] get_fed_liquidity_raw ìºì‹œ ê²½ë¡œ:", cache_dir)
    cache_path = os.path.join(cache_dir, "fed_liquidity.json")
    
    if os.path.exists(cache_path):
        try:
            if time.time() - os.path.getmtime(cache_path) < 86400: # 24ì‹œê°„
                with open(cache_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except: pass

    results = []
    
    # (Series ID, ì´ë¦„, ë‹¨ìœ„ë³€í™˜ê³„ìˆ˜, ë‹¨ìœ„ë¬¸ìì—´)
    indicators = [
        ("RRPONTSYD", "RRP", 1.0, "B$"),
        ("WRESBAL", "Reserves", 1.0, "B$"),
        ("WTREGEN", "TGA", 1.0, "B$"),
        ("M2SL", "M2", 1.0, "B$"),
        ("CPIAUCSL", "CPI", 1.0, "Idx"),      # ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
        ("UNRATE", "Unemployment", 1.0, "%"), # ì‹¤ì—…ë¥ 
        ("FEDFUNDS", "FedRate", 1.0, "%")     # ê¸°ì¤€ê¸ˆë¦¬
    ]
    
    base_url = "https://fred.stlouisfed.org/graph/fredgraph.csv?id={}"
    try:
        print("ğŸ” [DEBUG] get_fed_liquidity_raw FRED ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        for code, name, scale, unit in indicators:
            try:
                # FREDëŠ” ë³„ë„ API í‚¤ ì—†ì´ CSV ì§ì ‘ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
                res = requests.get(base_url.format(code), timeout=5)
                if res.status_code == 200:
                    # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
                    df = pd.read_csv(io.StringIO(res.text), index_col=0, parse_dates=True)
                    if not df.empty:
                        series = df.iloc[:, 0].dropna()
                        if series.empty: continue
                        
                        curr_val = float(series.iloc[-1]) * scale
                        curr_date = series.index[-1].strftime("%Y-%m-%d")
                        
                        # ğŸ¯ 1ë…„ ì „ ë°ì´í„° ê³„ì‚° (ì•½ 252 ê±°ë˜ì¼ or 12ê°œì›”)
                        idx_1y = -252 if len(series) > 252 else (-12 if len(series) > 12 else 0)
                        val_1y = float(series.iloc[idx_1y]) * scale
                        diff_1y = curr_val - val_1y
                        pct_1y = (diff_1y / val_1y) * 100 if val_1y != 0 else 0.0
                        
                        # ì „ì¡°(Previous) ëŒ€ë¹„ ì¦ê°
                        diff_str = "-"
                        if len(series) > 1:
                            prev_val = float(series.iloc[-2]) * scale
                            diff = curr_val - prev_val
                            diff_str = f"{diff:+.1f}"
                        
                        # ë‹¨ìœ„ì— ë”°ë¥¸ í¬ë§·íŒ… ë¯¸ì„¸ ì¡°ì •
                        fmt = ",.2f" if unit in ["%", "Idx"] else ",.1f"
                        results.append({
                            "name": name, "value": curr_val, "diff": diff, 
                            "diff_str": diff_str, "date": curr_date,
                            "val_str": f"{curr_val:{fmt}}{unit}",
                            "delta_str": f"{diff_str} (ì „ì¡°)",
                            "diff_1y": diff_1y,
                            "pct_1y": pct_1y
                        })
                print(f"ğŸ” [DEBUG] get_fed_liquidity_raw {name} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                continue
                
        # ìºì‹œ ì €ì¥
        if results:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False)
        print("ğŸ” [DEBUG] get_fed_liquidity_raw ìºì‹œ ì €ì¥")
    except: pass
    
    return results

def get_fed_liquidity_data():
    """FRED ë°ì´í„°ë¥¼ ë³´ê³ ì„œ ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    raw_data = get_fed_liquidity_raw()
    summary = "### [ ğŸ¦ ì—°ì¤€(Fed) ê±°ì‹œ/ìœ ë™ì„± ì§€í‘œ ]\n"
    try:
        for item in raw_data:
            summary += f"- **{item['name']}**: {item['val_str']} (ì „ì¡°: {item['diff_str']} | 1ë…„ ë³€ë™: {item['pct_1y']:+.1f}%)\n"
        return summary + "\n"
    except Exception as e:
        return f"âš ï¸ ì—°ì¤€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}\n"

def get_past_reports(section, count=1):
    """íŠ¹ì • ì„¹ì…˜ì˜ ê³¼ê±° ë³´ê³ ì„œ(ë‚ ì§œë³„ íŒŒì¼)ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {'daily': '01_daily', 'weekly': '02_weekly', 'monthly': '03_monthly'}
    target_dir = os.path.join(base_dir, dir_map.get(section, "05_etc"))
    
    content = ""
    if os.path.exists(target_dir):
        # latest.txt ì œì™¸í•˜ê³  ë‚ ì§œ í˜•ì‹ íŒŒì¼ë§Œ ì •ë ¬í•´ì„œ ê°€ì ¸ì˜´
        files = sorted([f for f in os.listdir(target_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        for f_name in files[:count]:
            try:
                with open(os.path.join(target_dir, f_name), 'r', encoding='utf-8') as f:
                    content += f"\n--- [ ê³¼ê±° ë¦¬í¬íŠ¸: {f_name} ] ---\n{f.read()}\n"
            except: pass
    return content

def get_ai_summary(title, content, system_instruction=None, role="filter", custom_config=None):
    """ë‰´ìŠ¤ íŒë… ë˜ëŠ” ìš”ì•½ì„ ìœ„í•´ AI ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. (í†µí•©ë¨)"""
    now_time = get_now_kst().strftime('%Y-%m-%d %H:%M:%S')
    
    # ì„¤ì • ë¡œë“œ (custom_configê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ common.data ì‚¬ìš©)
    cfg_data = custom_config if custom_config else data
    cfg = cfg_data.get("filter_model") if role == "filter" else cfg_data.get("analyst_model")
    
    base_url = cfg.get("url", "").rstrip('/')
    model_name = cfg.get("name")
    
    # ì§€ì¹¨ ì„¤ì •
    user_prompt = system_instruction if system_instruction else cfg.get("prompt", "")
    final_role = f"í˜„ì¬ ì‹œê°: {now_time}\në¶„ì„ ì§€ì¹¨: {user_prompt}"

    # í´ë¼ìš°ë“œ(Google ì§ì ‘ í˜¸ì¶œ) ì—¬ë¶€ íŒë³„
    is_direct_google = "generativelanguage.googleapis.com" in base_url
    
    if is_direct_google:
        api_key = config.get("gemini_api_key", "")
    else:
        api_key = cfg.get("key") if cfg.get("key") else config.get("openai_api_key", "")

    # í˜¸ì¶œ ë°©ì‹ ë¶„ê¸°
    if is_direct_google:
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": f"ì‹œìŠ¤í…œ ì§€ì¹¨: {final_role}\n\nì‚¬ìš©ì ì…ë ¥:\nì œëª©: {title}\në³¸ë¬¸: {content}"}]}],
            "generationConfig": {"temperature": cfg.get("temperature", 0.3)}
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key: headers["Authorization"] = f"Bearer {api_key}"
        payload = {
            "model": model_name,
            "messages": [{"role": "system", "content": final_role}, {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {content}"}],
            "temperature": cfg.get("temperature", 0.3)
        }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=600)
        resp.raise_for_status()
        result = resp.json()
        if "candidates" in result:
            return result['candidates'][0]['content']['parts'][0]['text']
        else:
            return result['choices'][0]['message']['content']
    except Exception as e:
        print(f"[{now_time}] AI ë¶„ì„ ì—ëŸ¬: {str(e)}")
        return f"âŒ [ERROR] AI ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"

def prepare_report_data(r_type, config_data):
    """ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°(KRX ì§€í‘œ + ë‰´ìŠ¤/ê³¼ê±°ë¦¬í¬íŠ¸)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    global_data = get_global_market_data(r_type)
    fed_data = get_fed_liquidity_data() # ì—°ì¤€ ì§€í‘œ ì¶”ê°€
    
    # KRX ë°ì´í„° ê³µí†µ ìˆ˜ì§‘ (ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œì—ë„ í˜„ì¬ ì‹œì¥ ìƒí™© ë°˜ì˜)
    market_summary = get_krx_market_data(r_type)

    if r_type == "daily":
        print(f"ğŸ” [Daily] ë°ì´í„° ìˆ˜ì§‘ (KRX ì§€í‘œ & ë‰´ìŠ¤ í•„í„°ë§) ì‹œì‘...")
        top_purchases = get_krx_top_investors()
        
        news_count = config_data.get("report_news_count", 100)
        raw_news_list = []
        seen_keys = set()
        target_date_limit = (now_kst - timedelta(days=3)).date()
        
        if os.path.exists(PENDING_PATH):
            files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
            for f_name in files:
                try:
                    with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                        news_data = json.load(file)
                        title = news_data.get("title", "").strip()
                        pub_dt_str = news_data.get("pub_dt", "")
                        if not title: continue
                        try: f_dt = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S').date()
                        except: f_dt = now_kst.date()
                        if f_dt < target_date_limit: continue
                        clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
                        if clean_key not in seen_keys:
                            seen_keys.add(clean_key)
                            raw_news_list.append(f"[{pub_dt_str[5:16]}] {title}")
                        if len(raw_news_list) >= news_count: break
                except: continue
        
        news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n" + "\n".join([f"- {t}" for t in raw_news_list])
        return (f"{market_summary}\n{global_data}\n{fed_data}\n{top_purchases}\n\n{news_ctx}", "ì¼ê°„(Daily)")
    else:
        # Weekly: ì´ë²ˆ ì£¼ ì¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 7ì¼)
        # Monthly: ì´ë²ˆ ë‹¬ ì£¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 5ê°œ)
        if r_type == "weekly":
            source_docs = get_past_reports('daily', 7)
            label = "ì£¼ê°„(Weekly)"
        else:
            source_docs = get_past_reports('weekly', 5)
            label = "ì›”ê°„(Monthly)"
            
        if not source_docs:
            source_docs = "âš ï¸ ë¶„ì„í•  í•˜ìœ„ ì£¼ê¸° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        return f"{source_docs}\n\n{market_summary}\n{global_data}\n{fed_data}", label

def generate_invest_report(r_type, input_content, config_data):
    """AIë¥¼ í˜¸ì¶œí•˜ì—¬ íˆ¬ì ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    
    if r_type == "daily":
        # ì¼ê°„: ë¯¸ë˜ ì „ëµ ì˜ˆìƒ (ìµœê·¼ 3ì¼ì¹˜ ì¼ê°„ + ìƒìœ„ ì£¼ê¸° ì°¸ì¡°)
        past_daily = get_past_reports('daily', 3)
        past_weekly = get_past_reports('weekly', 1)
        past_monthly = get_past_reports('monthly', 1)
        
        historical_context = (
            f"### [ ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ ]\n{past_daily}\n\n"
            f"### [ ìƒìœ„ ì£¼ê¸°(ì£¼ê°„/ì›”ê°„) íë¦„ ì°¸ì¡° ]\n{past_weekly}\n{past_monthly}"
        )
        
        base_prompt = "ë‹¹ì‹ ì€ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ëŒ€ì‘ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” 'ì „ëµê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì¶”ì„¸ ì—°ì†ì„± í™•ì¸**: ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ íë¦„ì„ ë¶„ì„í•˜ì—¬ ë‹¨ê¸° ì¶”ì„¸ê°€ ìœ ì§€ë˜ëŠ”ì§€ ë°˜ì „ë˜ëŠ”ì§€ íŒë‹¨í•˜ë¼.\n"
            "2. **ìƒìœ„ í”„ë ˆì„ ì •ë ¬**: í˜„ì¬ì˜ ë‹¨ê¸° ì›€ì§ì„ì´ ì£¼ê°„/ì›”ê°„ì˜ í° íë¦„ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€(ë™ì¡°í™”) ì•„ë‹ˆë©´ ë²—ì–´ë‚˜ëŠ”ì§€(ì´íƒˆ) ë¶„ì„í•˜ë¼.\n"
            "3. **ë¯¸ë˜ ì „ëµ ìˆ˜ë¦½**: ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‚´ì¼ì˜ ì‹œì¥ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì´ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ë§¤ë§¤ ì „ëµì„ ì œì‹œí•˜ë¼."
        )
        structure_instruction = (
            "### [ ì¼ê°„ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ ]\n"
            "1. ì‹œí™© ë¸Œë¦¬í•‘\n"
            "2. ì£¼ìš” ë‰´ìŠ¤ ë° ì˜¤í”¼ë‹ˆì–¸: ê²½ì œì  ì˜í–¥ë ¥ì´ í° ë‰´ìŠ¤ë‚˜ ì£¼ìš”ì¸ì‚¬ ë°œì–¸\n"
            "3. ìœ ë™ì„± ë¶„ì„: ìœ ë™ì„± ê´€ë ¨ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ìœ ë™ì„± íë¦„ íŒŒì•… (ì˜ˆ: í•œêµ­ -> ë¯¸êµ­, ìœ„í—˜ -> ì•ˆì „, AI -> ë°”ì´ì˜¤)\n"
            "4. ì¶”ì„¸ ì—°ì†ì„± ë¶„ì„\n"
            "5. ì¦ì‹œ ë¶„ì„: ì¦ì‹œ ê° ì‚°ì—…ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "6. ìì‚° ë¶„ì„: ì¦ì‹œ ì™¸ ìì‚°ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "7. í˜„ ì£¼ë ¥ì‚°ì—… ë° ë¯¸ë˜ìœ ë§ì‚°ì—… ì „ë§\n"
            "8. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘: ë‹¨ê¸°ì  ìœ„í—˜ ìš”ì†Œì™€ íšŒí”¼ ì „ëµ\n"
            "9. í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ë° íˆ¬ì ì „ëµ"
        )
        
    elif r_type == "weekly":
        # ì£¼ê°„: í˜„ìƒ ì›ì¸ ê¸°ë¡ (ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_weekly = get_past_reports('weekly', 1)
        historical_context = f"### [ ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_weekly}"
        
        base_prompt = "ë‹¹ì‹ ì€ ì‹œì¥ì˜ í˜„ìƒì„ ê¸°ë¡í•˜ê³  ì›ì¸ì„ ê·œëª…í•˜ëŠ” 'ì‹œì¥ ì—­ì‚¬ê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì¸ê³¼ê´€ê³„ ê·œëª…**: ì´ë²ˆ ì£¼ ì¼ê°„ ë¦¬í¬íŠ¸ë“¤ì— ê¸°ë¡ëœ ì‹œì¥ ë³€ë™ì˜ ê·¼ë³¸ì ì¸ ì›ì¸(ì¬ë£Œ, ìˆ˜ê¸‰, ë§¤í¬ë¡œ ë“±)ì„ ì¢…í•©í•˜ì—¬ ê·œëª…í•˜ë¼.\n"
            "2. **ì£¼ê°„ íë¦„ ìš”ì•½**: ì›”ìš”ì¼ë¶€í„° ê¸ˆìš”ì¼ê¹Œì§€ì˜ ì‹œì¥ ì‹¬ë¦¬ ë³€í™”ì™€ ì£¼ìš” ì´ìŠˆë¥¼ íƒ€ì„ë¼ì¸ í˜•íƒœë¡œ ìš”ì•½í•˜ë¼.\n"
            "3. **ë³€í™” ê¸°ë¡**: ì§€ë‚œì£¼ ë¦¬í¬íŠ¸ì™€ ë¹„êµí•˜ì—¬ ì‹œì¥ì˜ ìƒ‰ê¹”ì´ ì–´ë–»ê²Œ ë³€í–ˆëŠ”ì§€ ê¸°ë¡í•˜ë¼."
        )
        structure_type = "ì£¼ê°„ ì‹œì¥ ì›ì¸ ë° íë¦„ ë¶„ì„"
        structure_instruction = f"### [ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹: {structure_type} ]\n(ê° ë¦¬í¬íŠ¸ ì„±ê²©ì— ë§ëŠ” ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì—¬ ì‘ì„±í•  ê²ƒ)"
        
    else: # monthly
        # ì›”ê°„: êµ¬ì¡°ì  ë³€í™” ê¸°ë¡ (ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_monthly = get_past_reports('monthly', 1)
        historical_context = f"### [ ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_monthly}"
        
        base_prompt = "ë‹¹ì‹ ì€ ê±°ì‹œê²½ì œì™€ ì‹œì¥ì˜ êµ¬ì¡°ì  ë³€í™”ë¥¼ ê¸°ë¡í•˜ëŠ” 'ë§¤í¬ë¡œ ë¶„ì„ê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì›”ê°„ ë§¤í¬ë¡œ í‰ê°€**: ì´ë²ˆ ë‹¬ ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤ì„ ê´€í†µí•˜ëŠ” í•µì‹¬ ê±°ì‹œê²½ì œ í‚¤ì›Œë“œë¥¼ ë½‘ê³ , ê·¸ ì˜í–¥ì„ í‰ê°€í•˜ë¼.\n"
            "2. **êµ¬ì¡°ì  ë³€í™” í¬ì°©**: í•œ ë‹¬ê°„ ë°œìƒí•œ ì‚¬ê±´ë“¤ì´ ì‹œì¥ì˜ í€ë”ë©˜í„¸ì´ë‚˜ ì¥ê¸° ì¶”ì„¸ì— ì–´ë–¤ ë³€í™”ë¥¼ ì£¼ì—ˆëŠ”ì§€ ê¸°ë¡í•˜ë¼.\n"
            "3. **ì—­ì‚¬ì  ê¸°ë¡**: í›—ë‚  ì´ ë‹¬ì„ íšŒê³ í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê¸°ì–µë  ì‚¬ê±´ê³¼ ê·¸ ì˜ë¯¸ë¥¼ ì •ì˜í•˜ë¼."
        )
        structure_type = "ì›”ê°„ ê±°ì‹œê²½ì œ ë° êµ¬ì¡°ì  ë³€í™” ë¶„ì„"
        structure_instruction = f"### [ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹: {structure_type} ]\n(ê° ë¦¬í¬íŠ¸ ì„±ê²©ì— ë§ëŠ” ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì—¬ ì‘ì„±í•  ê²ƒ)"

    analysis_guideline = f"### [ {r_type} ë¶„ì„ ì§€ì¹¨ ]\n{specific_guideline}"

    system_prompt = (
        f"í˜„ì¬ ì„ë¬´: {r_type} íˆ¬ì ë³´ê³ ì„œ ì‘ì„±\n"
        f"ê¸°ì¤€ ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M')}\n\n"
        f"ë‹¹ì‹ ì€ {base_prompt}ì´ë©°, ì•„ë˜ ì§€ì¹¨ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"{analysis_guideline}\n\n"
        f"--- [ ì°¸ê³  ìë£Œ (Context) ] ---\n{historical_context}\n\n"
        f"--- [ ìµœì¢… ì§€ì‹œ ] ---\n"
        f"ì œê³µëœ ì…ë ¥ ë°ì´í„°(Input Data)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"{structure_instruction}"
    )
    
    return get_ai_summary(title=f"{date.today()} {r_type.upper()} ë³´ê³ ì„œ", content=input_content, system_instruction=system_prompt, role="analyst", custom_config=config_data)