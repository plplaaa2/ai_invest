
import json
import os
import re
import requests
import time
import math
import feedparser
from datetime import datetime, timedelta, date, timezone
from bs4 import BeautifulSoup

try:
    import yfinance as yf
except ImportError:
    yf = None

KST = timezone(timedelta(hours=9))

def get_now_kst():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

# --- [0. ì‹œìŠ¤í…œ ê³µí†µ ê²½ë¡œ ì„¤ì •] ---
OPTIONS_PATH = "/data/options.json"
BASE_PATH = "/share/ai_analyst"
CONFIG_PATH = os.path.join(BASE_PATH, "rss_config.json")
PENDING_PATH = os.path.join(BASE_PATH, "pending")
REPORT_DIR = os.path.join(BASE_PATH, "reports")

def load_addon_config():
    if os.path.exists(OPTIONS_PATH):
        try:
            with open(OPTIONS_PATH, "r", encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return {}

config = load_addon_config() 


# í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§
openai_key = config.get("openai_api_key", "")
gemini_key = config.get("gemini_api_key", "")
headers = {"Content-Type": "application/json"}

# ğŸ¯ 2. Cloud LLM ëª¨ë“œ íŒì • ë¡œì§ ë³´ì™„
if openai_key or gemini_key:
    if openai_key:
        headers["Authorization"] = f"Bearer {openai_key}"
    print(f"ğŸš€ Cloud LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤. (OpenAI: {'OK' if openai_key else 'NO'}, Gemini: {'OK' if gemini_key else 'NO'})")
else:
    print("ğŸ  Local LLM ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤ (API í‚¤ ì—†ìŒ).")
        

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def safe_float(v):
    if v is None or v == "" or v == "-": return 0.0
    try:
        clean_v = re.sub(r'[^\d.-]', '', str(v))
        return float(clean_v) if clean_v else 0.0
    except: return 0.0

def save_to_influx(symbol, data, current_time):
    point = Point("financial_metrics").tag("symbol", symbol)
    for f, v in data.items(): point.field(f, float(v))
    point.time(current_time)
    if write_api:
        try:
            write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
            return True
        except Exception as e:
            print(f"âš ï¸ InfluxDB ì“°ê¸° ì—ëŸ¬ ({symbol}): {e}")
    return False
    
def save_report_to_file(content, section_name):
    # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ì„¸ë¶„í™”
    base_dir = REPORT_DIR
    dir_map = {
        'daily': '01_daily', 'weekly': '02_weekly', 
        'monthly': '03_monthly', 'yearly': '04_yearly'
    }
    # section_nameì´ ë§µì— ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë” ì‚¬ìš©
    subdir = dir_map.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(base_dir, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    # 2. íŒŒì¼ëª… ìƒì„± ë° ì €ì¥ (ê¸°ë¡ìš©)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"{timestamp}_{section_name.replace(' ', '_')}.txt"
    filepath = os.path.join(report_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    # 3. ğŸ¯ AI ì°¸ì¡°ìš© Latest íŒŒì¼ ê°±ì‹  (ê³ ì • ê²½ë¡œ)
    latest_path = os.path.join(report_dir, "latest.txt")
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(content)

    # 4. ğŸ§¹ ê³„ì¸µí˜• ìë™ ì •ì œ (Purge) ë¡œì§
    # ê·œì¹™: Daily(7ì¼), Weekly(30ì¼), Monthly(365ì¼) ë³´ê´€
    purge_rules = {'01_daily': 9, '02_weekly': 35, '03_monthly': 370}
    if subdir in purge_rules:
        limit_days = purge_rules[subdir]
        threshold = time.time() - (limit_days * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue # ìµœì‹  ë§¥ë½ì€ ë³´í˜¸
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
                
    return filepath
    
def load_historical_contexts():
    """íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì‘ë™í•˜ë©°, AIì—ê²Œ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•©ë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    
    for label, rel_path in dir_map.items():
        full_path = os.path.join(base_dir, rel_path)
        
        # ğŸ›¡ï¸ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë¡ì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: í•´ë‹¹ ì£¼ê¸°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        else:
            # ğŸ’¡ íŒŒì¼ì´ ì—†ì„ ë•Œ AIì—ê²Œ ì¤„ ë©”ì‹œì§€
            # AIê°€ "ê³¼ê±° ë°ì´í„°ê°€ ì—†ìœ¼ë‹ˆ ì˜¤ëŠ˜ ìˆ˜ì¹˜ì— ë” ì§‘ì¤‘í•´ì„œ ë¶„ì„í•´ë¼"ë¼ê³  íŒë‹¨í•˜ê²Œ ìœ ë„í•©ë‹ˆë‹¤.
            context_text += f"\n<{label}>: ì‹œìŠ¤í…œ ë„ì… ì´ˆê¸° ë‹¨ê³„ë¡œ, ì•„ì§ {label} ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ê°€ìš©í•œ ìµœì‹  ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
            
    return context_text
    
def get_market_summary():
    """Pykrxë¥¼ í™œìš©í•´ KOSPI/KOSDAQ ì§€ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    summary = ""
    try:
        from pykrx import stock
        now = get_now_kst()
        # ìµœê·¼ 5ì¼ ì¡°íšŒ (ì£¼ë§/íœ´ì¼ ëŒ€ë¹„)
        start_dt = (now - timedelta(days=5)).strftime("%Y%m%d")
        end_dt = now.strftime("%Y%m%d")
        
        # 1001: KOSPI, 2001: KOSDAQ
        df_k = stock.get_index_ohlcv(start_dt, end_dt, "1001")
        df_kq = stock.get_index_ohlcv(start_dt, end_dt, "2001")
        
        if not df_k.empty and not df_kq.empty:
            last_k = df_k.iloc[-1]
            last_kq = df_kq.iloc[-1]
            date_str = last_k.name.strftime("%Y-%m-%d")
            
            summary = (
                f"### [ ğŸ“‰ êµ­ë‚´ ì¦ì‹œ ìš”ì•½ ({date_str}) ]\n"
                f"- KOSPI: {last_k['ì¢…ê°€']:,.2f} ({last_k['ë“±ë½ë¥ ']:+.2f}%)\n"
                f"- KOSDAQ: {last_kq['ì¢…ê°€']:,.2f} ({last_kq['ë“±ë½ë¥ ']:+.2f}%)\n\n"
            )
    except Exception as e:
        print(f"âš ï¸ Pykrx ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return summary

def load_data():
    """ì„œë¹„ìŠ¤ ì„¤ì •(RSS, AI ëª¨ë¸ ë“±)ì„ ë¡œë“œí•˜ê³  ë¯¸ì¡´ì¬ ì‹œ ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    default_structure = {
        "feeds": [], 
        "update_interval": 10, 
        "view_range": "ì‹¤ì‹œê°„", 
        "retention_days": 7,
        "report_news_count": 100, 
        "report_auto_gen": True, 
        "report_gen_time": "08:00", 
        "report_days": 3,
        
        # ğŸ¯ ë‰´ìŠ¤ íŒë… ëª¨ë¸ ì„¤ì • (Filter)
        "filter_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.1,  # ğŸ’¡ íŒë…ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
            "prompt": "íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ê°€ ê±°ì‹œê²½ì œë‚˜ ìœ ë™ì„±ì— ì¤‘ìš”í•œì§€ íŒë…í•˜ì—¬ 0~5ì ì„ ë§¤ê¸°ì„¸ìš”."
        },
        
        # ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ëª¨ë¸ ì„¤ì • (Analyst)
        "analyst_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "temperature": 0.3,  # ğŸ’¡ ë³´ê³ ì„œëŠ” ì•½ê°„ì˜ í†µì°°ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ 0.3~0.5 ê¶Œì¥
            "prompt": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ì „ëµì„ ì œì‹œí•˜ì„¸ìš”."
        }
    }
    
    # 1. íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ìƒì„± (ìë™ ë³µêµ¬)
    if not os.path.exists(CONFIG_PATH):
        try:
            os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump(default_structure, f, indent=4, ensure_ascii=False)
            print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ: {CONFIG_PATH}")
            return default_structure
        except:
            return default_structure

    # 2. íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ ë° ëˆ„ë½ëœ í‚¤ ë³´ì •
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            loaded = json.load(f)
            # ìƒˆë¡œìš´ ê¸°ëŠ¥(ì˜¨ë„ ë“±)ì´ ì¶”ê°€ë˜ì–´ í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ ë³‘í•©
            for key, val in default_structure.items():
                if key not in loaded: 
                    loaded[key] = val
                elif isinstance(val, dict): # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬(ëª¨ë¸ ì„¤ì •) ë‚´ë¶€ í‚¤ ë³´ì •
                    for sub_key, sub_val in val.items():
                        if sub_key not in loaded[key]:
                            loaded[key][sub_key] = sub_val
            return loaded
    except:
        return default_structure

# ê³µí†µ ë°ì´í„° ê°ì²´ (ëª¨ë“  ëª¨ë“ˆì—ì„œ ê³µìœ )
data = load_data()

def get_krx_market_indicators():
    """ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì§€ìˆ˜ ë° ìˆ˜ê¸‰í˜„í™© ìš”ì•½ (ë¡œê·¸ ê°•í™”)"""
    try:
        target_date = get_latest_trading_date()
        print(f"ğŸ” [ì§€í‘œ ìˆ˜ì§‘] ëŒ€ìƒ ë‚ ì§œ: {target_date}")
        summary = f"### [ KRX ì‹œì¥ ì§€í‘œ ìš”ì•½ ({target_date}) ]\n"

        for m_name, m_code in [("KOSPI", "1001"), ("KOSDAQ", "2001")]:
            df = stock.get_index_ohlcv_by_date(target_date, target_date, m_code)
            if not df.empty:
                row = df.iloc[0]
                amount_bill = row['ê±°ë˜ëŒ€ê¸ˆ'] / 100_000_000
                summary += f"- {m_name}: {row['ì¢…ê°€']:,.2f} (ê±°ë˜ëŸ‰: {row['ê±°ë˜ëŸ‰']:,.0f}, ê±°ë˜ëŒ€ê¸ˆ: {amount_bill:,.0f}ì–µ)\n"
                print(f"   ğŸ“Š {m_name} ë¡œë“œ ì™„ë£Œ: {row['ì¢…ê°€']:,.2f}")

        df_inv = stock.get_market_net_purchase_of_equities_by_ticker(target_date, target_date, "ALL")
        foreign_bill = df_inv['ì™¸êµ­ì¸'].sum() / 100_000_000
        inst_bill = df_inv['ê¸°ê´€í•©ê³„'].sum() / 100_000_000
        summary += f"- íˆ¬ìì ìˆ˜ê¸‰: ì™¸êµ­ì¸ {foreign_bill:,.0f}ì–µ, ê¸°ê´€ {inst_bill:,.0f}ì–µ (ìˆœë§¤ìˆ˜ ê¸°ì¤€)\n"
        print(f"   ğŸ’° ìˆ˜ê¸‰ ë°ì´í„° í•©ê³„: ì™¸ì¸({foreign_bill:,.0f}ì–µ), ê¸°ê´€({inst_bill:,.0f}ì–µ)", flush=True)
        
        return summary
    except Exception as e:
        print(f"âŒ [ì—ëŸ¬] ì§€ìˆ˜ ìš”ì•½ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "âš ï¸ KRX ì§€ìˆ˜ ìš”ì•½ ë¡œë“œ ì‹¤íŒ¨"

def get_krx_top_investors():
    """ì™¸êµ­ì¸/ê¸°ê´€ ìˆœë§¤ìˆ˜ ìƒìœ„ 10ê°œ ì¢…ëª© (ë¡œê·¸ ê°•í™”)"""
    try:
        target_date = get_latest_trading_date()
        df = stock.get_market_net_purchase_of_equities_by_ticker(target_date, target_date, "ALL")
        
        def get_top_list(data, col):
            top_df = data.sort_values(by=col, ascending=False).head(10)
            items = []
            for ticker, row in top_df.iterrows():
                name = stock.get_market_ticker_name(ticker)
                val_bill = row[col] / 100_000_000
                items.append(f"{name}({val_bill:,.0f}ì–µ)")
            return ", ".join(items)

        f_top = get_top_list(df, 'ì™¸êµ­ì¸')
        i_top = get_top_list(df, 'ê¸°ê´€í•©ê³„')
        
        print(f"ğŸ” [ìˆœë§¤ìˆ˜ Top 10] ì™¸ì¸: {f_top[:50]}...", flush=True)# ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"ğŸ” [ìˆœë§¤ìˆ˜ Top 10] ê¸°ê´€: {i_top[:50]}...", flush=True)
        
        report = "### [ ìˆ˜ê¸‰ ìƒìœ„ ì¢…ëª© (Top 10) ]\n"
        report += f"- ì™¸êµ­ì¸ ë§¤ìˆ˜: {f_top}\n"
        report += f"- ê¸°ê´€ ë§¤ìˆ˜: {i_top}\n"
        return report
    except Exception as e:
        print(f"âŒ [ì—ëŸ¬] ìˆ˜ê¸‰ ì¢…ëª© ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "âš ï¸ ìˆ˜ê¸‰ ì¢…ëª© ë¡œë“œ ì‹¤íŒ¨"

def get_krx_sector_indices():
    """ì£¼ìš” ì‚°ì—…ë³„ ì§€ìˆ˜ í˜„í™© (ë¡œê·¸ ê°•í™”)"""
    try:
        target_date = get_latest_trading_date()
        indices = stock.get_index_ticker_list(target_date, market="KRX")
        print(f"ğŸ­ [ì‚°ì—… ì„¹í„°] ì „ì²´ {len(indices)}ê°œ ì§€ìˆ˜ ì¤‘ ì£¼ìš” í•­ëª© í•„í„°ë§ ì¤‘...")
        
        report = "### [ ì£¼ìš” ì‚°ì—…ë³„ ì§€ìˆ˜ í˜„í™© ]\n"
        count = 0
        for ticker in indices:
            name = stock.get_index_ticker_name(ticker)
            if any(kw in name for kw in ['ë°˜ë„ì²´', 'IT', 'ê¸ˆìœµ', 'ì—ë„ˆì§€', 'ë°”ì´ì˜¤', 'ìë™ì°¨']):
                df = stock.get_index_ohlcv_by_date(target_date, target_date, ticker)
                if not df.empty:
                    val = df.iloc[0]['ì¢…ê°€']
                    report += f"- {name}: {val:,.2f}\n"
                    print(f"   âœ… ì„¹í„° í™•ì¸: {name} ({val:,.2f})", flush=True)
                    count += 1
            if count >= 8: break
        return report
    except Exception as e:
        print(f"âŒ [ì—ëŸ¬] ì‚°ì—… ì§€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "âš ï¸ ì‚°ì—… ì§€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨"

def get_global_market_data(r_type="daily"):
    """yfinanceë¥¼ í†µí•´ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    if not yf: return "âš ï¸ yfinance ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    end_dt = get_now_kst()
    if r_type == 'daily': days = 7
    elif r_type == 'weekly': days = 30
    else: days = 60
    
    start_dt = end_dt - timedelta(days=days)
    
    tickers = {
        "ğŸ‡ºğŸ‡¸ ë¯¸êµ­ 3ëŒ€ ì§€ìˆ˜ & VIX": {
            "^GSPC": "S&P500", "^DJI": "Dow Jones", "^IXIC": "Nasdaq", 
            "^SOX": "SOX(ë°˜ë„ì²´)", "^VIX": "VIX"
        },
        "ğŸŒ ê¸€ë¡œë²Œ ì§€ìˆ˜": {
            "^N225": "Nikkei 225", "^GDAXI": "DAX", "^HSI": "Hang Seng"
        },
        "ğŸ’µ ê¸ˆë¦¬ & í™˜ìœ¨": {
            "^TNX": "ë¯¸êµ­ì±„ 10ë…„", "^TYX": "ë¯¸êµ­ì±„ 30ë…„", "^FVX": "ë¯¸êµ­ì±„ 5ë…„", 
            "KRW=X": "USD/KRW", "DX-Y.NYB": "ë‹¬ëŸ¬ ì¸ë±ìŠ¤", "JPY=X": "USD/JPY"
        },
        "ğŸ›¢ï¸ ì›ìì¬ & ì½”ì¸": {
            "CL=F": "WTI ì›ìœ ", "GC=F": "ê¸ˆ", "SI=F": "ì€", "HG=F": "êµ¬ë¦¬", 
            "BTC-USD": "ë¹„íŠ¸ì½”ì¸"
        }
    }
    
    all_symbols = [s for cat in tickers.values() for s in cat.keys()]
    report = f"### [ ğŸŒ ê¸€ë¡œë²Œ ì‹œì¥ ë°ì´í„° ({days}ì¼ ë³€ë™) ]\n"
    
    try:
        df = yf.download(all_symbols, start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), progress=False)['Close']
        for cat_name, items in tickers.items():
            report += f"#### {cat_name}\n"
            for sym, name in items.items():
                try:
                    if sym in df.columns:
                        series = df[sym].dropna()
                        if series.empty: continue
                        curr, start = series.iloc[-1], series.iloc[0]
                        chg_pct = ((curr - start) / start) * 100
                        report += f"- **{name}**: {curr:,.2f} ({chg_pct:+.2f}%, ë²”ìœ„: {series.min():,.2f}~{series.max():,.2f})\n"
                except: continue
            report += "\n"
        return report
    except Exception as e:
        return f"âš ï¸ ê¸€ë¡œë²Œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"

def get_past_reports(section, count=1):
    """íŠ¹ì • ì„¹ì…˜ì˜ ê³¼ê±° ë³´ê³ ì„œ(ë‚ ì§œë³„ íŒŒì¼)ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    base_dir = REPORT_DIR
    dir_map = {'daily': '01_daily', 'weekly': '02_weekly', 'monthly': '03_monthly'}
    target_dir = os.path.join(base_dir, dir_map.get(section, "05_etc"))
    
    content = ""
    if os.path.exists(target_dir):
        # latest.txt ì œì™¸í•˜ê³  ë‚ ì§œ í˜•ì‹ íŒŒì¼ë§Œ ì •ë ¬í•´ì„œ ê°€ì ¸ì˜´
        files = sorted([f for f in os.listdir(target_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        for f_name in files[:count]:
            try:
                with open(os.path.join(target_dir, f_name), 'r', encoding='utf-8') as f:
                    content += f"\n--- [ ê³¼ê±° ë¦¬í¬íŠ¸: {f_name} ] ---\n{f.read()}\n"
            except: pass
    return content

def get_ai_summary(title, content, system_instruction=None, role="filter", custom_config=None):
    """ë‰´ìŠ¤ íŒë… ë˜ëŠ” ìš”ì•½ì„ ìœ„í•´ AI ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. (í†µí•©ë¨)"""
    now_time = get_now_kst().strftime('%Y-%m-%d %H:%M:%S')
    
    # ì„¤ì • ë¡œë“œ (custom_configê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ common.data ì‚¬ìš©)
    cfg_data = custom_config if custom_config else data
    cfg = cfg_data.get("filter_model") if role == "filter" else cfg_data.get("analyst_model")
    
    base_url = cfg.get("url", "").rstrip('/')
    model_name = cfg.get("name")
    
    # ì§€ì¹¨ ì„¤ì •
    user_prompt = system_instruction if system_instruction else cfg.get("prompt", "")
    final_role = f"í˜„ì¬ ì‹œê°: {now_time}\në¶„ì„ ì§€ì¹¨: {user_prompt}"

    # í´ë¼ìš°ë“œ(Google ì§ì ‘ í˜¸ì¶œ) ì—¬ë¶€ íŒë³„
    is_direct_google = "generativelanguage.googleapis.com" in base_url
    
    if is_direct_google:
        api_key = config.get("gemini_api_key", "")
    else:
        api_key = cfg.get("key") if cfg.get("key") else config.get("openai_api_key", "")

    # í˜¸ì¶œ ë°©ì‹ ë¶„ê¸°
    if is_direct_google:
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": f"ì‹œìŠ¤í…œ ì§€ì¹¨: {final_role}\n\nì‚¬ìš©ì ì…ë ¥:\nì œëª©: {title}\në³¸ë¬¸: {content}"}]}],
            "generationConfig": {"temperature": cfg.get("temperature", 0.3)}
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key: headers["Authorization"] = f"Bearer {api_key}"
        payload = {
            "model": model_name,
            "messages": [{"role": "system", "content": final_role}, {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {content}"}],
            "temperature": cfg.get("temperature", 0.3)
        }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=600)
        resp.raise_for_status()
        result = resp.json()
        if "candidates" in result:
            return result['candidates'][0]['content']['parts'][0]['text']
        else:
            return result['choices'][0]['message']['content']
    except Exception as e:
        print(f"[{now_time}] AI ë¶„ì„ ì—ëŸ¬: {str(e)}")
        return f"âŒ [ERROR] AI ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"

def prepare_report_data(r_type, config_data):
    """ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°(KRX ì§€í‘œ + ë‰´ìŠ¤/ê³¼ê±°ë¦¬í¬íŠ¸)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    global_data = get_global_market_data(r_type)
    
    if r_type == "daily":
        print(f"ğŸ” [Daily] ë°ì´í„° ìˆ˜ì§‘ (KRX ì§€í‘œ & ë‰´ìŠ¤ í•„í„°ë§) ì‹œì‘...")
        market_summary = get_krx_market_indicators()
        top_purchases = get_krx_top_investors()
        industry_indices = get_krx_sector_indices()
        
        news_count = config_data.get("report_news_count", 100)
        raw_news_list = []
        seen_keys = set()
        target_date_limit = (now_kst - timedelta(days=3)).date()
        
        if os.path.exists(PENDING_PATH):
            files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
            for f_name in files:
                try:
                    with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                        news_data = json.load(file)
                        title = news_data.get("title", "").strip()
                        pub_dt_str = news_data.get("pub_dt", "")
                        if not title: continue
                        try: f_dt = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S').date()
                        except: f_dt = now_kst.date()
                        if f_dt < target_date_limit: continue
                        clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
                        if clean_key not in seen_keys:
                            seen_keys.add(clean_key)
                            raw_news_list.append(f"[{pub_dt_str[5:16]}] {title}")
                        if len(raw_news_list) >= news_count: break
                except: continue
        
        news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n" + "\n".join([f"- {t}" for t in raw_news_list])
        return (f"{market_summary}\n{global_data}\n{top_purchases}\n{industry_indices}\n\n{news_ctx}", "ì¼ê°„(Daily)")
    else:
        # Weekly: ì´ë²ˆ ì£¼ ì¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 7ì¼)
        # Monthly: ì´ë²ˆ ë‹¬ ì£¼ê°„ ë³´ê³ ì„œ ì „ë¶€ (ìµœëŒ€ 5ê°œ)
        if r_type == "weekly":
            source_docs = get_past_reports('daily', 7)
            label = "ì£¼ê°„(Weekly)"
        else:
            source_docs = get_past_reports('weekly', 5)
            label = "ì›”ê°„(Monthly)"
            
        if not source_docs:
            source_docs = "âš ï¸ ë¶„ì„í•  í•˜ìœ„ ì£¼ê¸° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        return f"{source_docs}\n\n{global_data}", label

def generate_invest_report(r_type, input_content, config_data):
    """AIë¥¼ í˜¸ì¶œí•˜ì—¬ íˆ¬ì ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    now_kst = get_now_kst()
    
    if r_type == "daily":
        # ì¼ê°„: ë¯¸ë˜ ì „ëµ ì˜ˆìƒ (ìµœê·¼ 3ì¼ì¹˜ ì¼ê°„ + ìƒìœ„ ì£¼ê¸° ì°¸ì¡°)
        past_daily = get_past_reports('daily', 3)
        past_weekly = get_past_reports('weekly', 1)
        past_monthly = get_past_reports('monthly', 1)
        
        historical_context = (
            f"### [ ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ ]\n{past_daily}\n\n"
            f"### [ ìƒìœ„ ì£¼ê¸°(ì£¼ê°„/ì›”ê°„) íë¦„ ì°¸ì¡° ]\n{past_weekly}\n{past_monthly}"
        )
        
        base_prompt = "ë‹¹ì‹ ì€ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ëŒ€ì‘ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” 'ì „ëµê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì¶”ì„¸ ì—°ì†ì„± í™•ì¸**: ìµœê·¼ 3ì¼ê°„ì˜ ì¼ê°„ ë¦¬í¬íŠ¸ íë¦„ì„ ë¶„ì„í•˜ì—¬ ë‹¨ê¸° ì¶”ì„¸ê°€ ìœ ì§€ë˜ëŠ”ì§€ ë°˜ì „ë˜ëŠ”ì§€ íŒë‹¨í•˜ë¼.\n"
            "2. **ìƒìœ„ í”„ë ˆì„ ì •ë ¬**: í˜„ì¬ì˜ ë‹¨ê¸° ì›€ì§ì„ì´ ì£¼ê°„/ì›”ê°„ì˜ í° íë¦„ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€(ë™ì¡°í™”) ì•„ë‹ˆë©´ ë²—ì–´ë‚˜ëŠ”ì§€(ì´íƒˆ) ë¶„ì„í•˜ë¼.\n"
            "3. **ë¯¸ë˜ ì „ëµ ìˆ˜ë¦½**: ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‚´ì¼ì˜ ì‹œì¥ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì´ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ë§¤ë§¤ ì „ëµì„ ì œì‹œí•˜ë¼."
        )
        structure_instruction = (
            "### [ ì¼ê°„ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹ ]\n"
            "1. ì‹œí™© ë¸Œë¦¬í•‘\n"
            "2. ì£¼ìš” ë‰´ìŠ¤ ë° ì˜¤í”¼ë‹ˆì–¸: ê²½ì œì  ì˜í–¥ë ¥ì´ í° ë‰´ìŠ¤ë‚˜ ì£¼ìš”ì¸ì‚¬ ë°œì–¸\n"
            "3. ìœ ë™ì„± ë¶„ì„: ìœ ë™ì„± ê´€ë ¨ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ìœ ë™ì„± íë¦„ íŒŒì•… (ì˜ˆ: í•œêµ­ -> ë¯¸êµ­, ìœ„í—˜ -> ì•ˆì „, AI -> ë°”ì´ì˜¤)\n"
            "4. ì¶”ì„¸ ì—°ì†ì„± ë¶„ì„\n"
            "5. ì¦ì‹œ ë¶„ì„: ì¦ì‹œ ê° ì‚°ì—…ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "6. ìì‚° ë¶„ì„: ì¦ì‹œ ì™¸ ìì‚°ë³„ 0~5ì  ë¶„ì„ ë° ìš”ì•½\n"
            "7. í˜„ ì£¼ë ¥ì‚°ì—… ë° ë¯¸ë˜ìœ ë§ì‚°ì—… ì „ë§\n"
            "8. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘: ë‹¨ê¸°ì  ìœ„í—˜ ìš”ì†Œì™€ íšŒí”¼ ì „ëµ\n"
            "9. í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ë° íˆ¬ì ì „ëµ"
        )
        
    elif r_type == "weekly":
        # ì£¼ê°„: í˜„ìƒ ì›ì¸ ê¸°ë¡ (ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_weekly = get_past_reports('weekly', 1)
        historical_context = f"### [ ì§€ë‚œ ì£¼ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_weekly}"
        
        base_prompt = "ë‹¹ì‹ ì€ ì‹œì¥ì˜ í˜„ìƒì„ ê¸°ë¡í•˜ê³  ì›ì¸ì„ ê·œëª…í•˜ëŠ” 'ì‹œì¥ ì—­ì‚¬ê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì¸ê³¼ê´€ê³„ ê·œëª…**: ì´ë²ˆ ì£¼ ì¼ê°„ ë¦¬í¬íŠ¸ë“¤ì— ê¸°ë¡ëœ ì‹œì¥ ë³€ë™ì˜ ê·¼ë³¸ì ì¸ ì›ì¸(ì¬ë£Œ, ìˆ˜ê¸‰, ë§¤í¬ë¡œ ë“±)ì„ ì¢…í•©í•˜ì—¬ ê·œëª…í•˜ë¼.\n"
            "2. **ì£¼ê°„ íë¦„ ìš”ì•½**: ì›”ìš”ì¼ë¶€í„° ê¸ˆìš”ì¼ê¹Œì§€ì˜ ì‹œì¥ ì‹¬ë¦¬ ë³€í™”ì™€ ì£¼ìš” ì´ìŠˆë¥¼ íƒ€ì„ë¼ì¸ í˜•íƒœë¡œ ìš”ì•½í•˜ë¼.\n"
            "3. **ë³€í™” ê¸°ë¡**: ì§€ë‚œì£¼ ë¦¬í¬íŠ¸ì™€ ë¹„êµí•˜ì—¬ ì‹œì¥ì˜ ìƒ‰ê¹”ì´ ì–´ë–»ê²Œ ë³€í–ˆëŠ”ì§€ ê¸°ë¡í•˜ë¼."
        )
        structure_type = "ì£¼ê°„ ì‹œì¥ ì›ì¸ ë° íë¦„ ë¶„ì„"
        structure_instruction = f"### [ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹: {structure_type} ]\n(ê° ë¦¬í¬íŠ¸ ì„±ê²©ì— ë§ëŠ” ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì—¬ ì‘ì„±í•  ê²ƒ)"
        
    else: # monthly
        # ì›”ê°„: êµ¬ì¡°ì  ë³€í™” ê¸°ë¡ (ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡°)
        past_monthly = get_past_reports('monthly', 1)
        historical_context = f"### [ ì§€ë‚œ ì›”ê°„ ë¦¬í¬íŠ¸ (ë¹„êµìš©) ]\n{past_monthly}"
        
        base_prompt = "ë‹¹ì‹ ì€ ê±°ì‹œê²½ì œì™€ ì‹œì¥ì˜ êµ¬ì¡°ì  ë³€í™”ë¥¼ ê¸°ë¡í•˜ëŠ” 'ë§¤í¬ë¡œ ë¶„ì„ê°€'ì…ë‹ˆë‹¤."
        specific_guideline = (
            "1. **ì›”ê°„ ë§¤í¬ë¡œ í‰ê°€**: ì´ë²ˆ ë‹¬ ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤ì„ ê´€í†µí•˜ëŠ” í•µì‹¬ ê±°ì‹œê²½ì œ í‚¤ì›Œë“œë¥¼ ë½‘ê³ , ê·¸ ì˜í–¥ì„ í‰ê°€í•˜ë¼.\n"
            "2. **êµ¬ì¡°ì  ë³€í™” í¬ì°©**: í•œ ë‹¬ê°„ ë°œìƒí•œ ì‚¬ê±´ë“¤ì´ ì‹œì¥ì˜ í€ë”ë©˜í„¸ì´ë‚˜ ì¥ê¸° ì¶”ì„¸ì— ì–´ë–¤ ë³€í™”ë¥¼ ì£¼ì—ˆëŠ”ì§€ ê¸°ë¡í•˜ë¼.\n"
            "3. **ì—­ì‚¬ì  ê¸°ë¡**: í›—ë‚  ì´ ë‹¬ì„ íšŒê³ í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê¸°ì–µë  ì‚¬ê±´ê³¼ ê·¸ ì˜ë¯¸ë¥¼ ì •ì˜í•˜ë¼."
        )
        structure_type = "ì›”ê°„ ê±°ì‹œê²½ì œ ë° êµ¬ì¡°ì  ë³€í™” ë¶„ì„"
        structure_instruction = f"### [ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹: {structure_type} ]\n(ê° ë¦¬í¬íŠ¸ ì„±ê²©ì— ë§ëŠ” ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì—¬ ì‘ì„±í•  ê²ƒ)"

    analysis_guideline = f"### [ {r_type} ë¶„ì„ ì§€ì¹¨ ]\n{specific_guideline}"

    system_prompt = (
        f"í˜„ì¬ ì„ë¬´: {r_type} íˆ¬ì ë³´ê³ ì„œ ì‘ì„±\n"
        f"ê¸°ì¤€ ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M')}\n\n"
        f"ë‹¹ì‹ ì€ {base_prompt}ì´ë©°, ì•„ë˜ ì§€ì¹¨ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"{analysis_guideline}\n\n"
        f"--- [ ì°¸ê³  ìë£Œ (Context) ] ---\n{historical_context}\n\n"
        f"--- [ ìµœì¢… ì§€ì‹œ ] ---\n"
        f"ì œê³µëœ ì…ë ¥ ë°ì´í„°(Input Data)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"{structure_instruction}"
    )
    
    return get_ai_summary(title=f"{date.today()} {r_type.upper()} ë³´ê³ ì„œ", content=input_content, system_instruction=system_prompt, role="analyst", custom_config=config_data)