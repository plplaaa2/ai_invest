import streamlit as st
import pandas as pd
import json
import os
import feedparser
import re
import requests
from datetime import datetime, timedelta, date
from bs4 import BeautifulSoup
import time
import math

# --- 1. ê²½ë¡œ ë° ì„¤ì • ë¡œë“œ ---
CONFIG_PATH = "/share/ai_analyst/rss_config.json"
PENDING_PATH = "/share/ai_analyst/pending"
OPTIONS_PATH = "/data/options.json"

# --- 2. ë‰´ìŠ¤ ì²˜ë¦¬ í•µì‹¬ í•¨ìˆ˜ ---
def load_data():
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ë©°, ë©€í‹° ëª¨ë¸ êµ¬ì¡°ë¥¼ ì§€ì›í•˜ë„ë¡ ìƒì„±í•©ë‹ˆë‹¤."""
    default_structure = {
        "feeds": [], 
        "update_interval": 10, 
        "view_range": "ì‹¤ì‹œê°„",
        "retention_days": 7,
        
        # ğŸ¯ ë‰´ìŠ¤ íŒë… ëª¨ë¸ ì„¤ì • (Filter)
        "filter_model": {
            "provider": "Local",      # Local, Gemini, OpenAI ì„ íƒ ê°€ëŠ¥
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.2:1234/v1",
            "key": "",
            "prompt": "íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë‰´ìŠ¤ë¥¼ ê±°ì‹œê²½ì œ, ì¦ì‹œ, ì±„ê¶Œ, í™˜ìœ¨, ì›ìì¬ë¡œ ë¶„ë¥˜í•˜ê³  ìš”ì•½ í›„ 0~5ì ì„ ë§¤ê¹ë‹ˆë‹¤. 4ì  ì´ìƒì€ ìƒì„¸ ìš”ì•½ì„ í•˜ë©° ìš”ì•½ êµ¬ì¡°ëŠ” ì œëª©, ë‚ ì§œ, ì¶œì²˜, ë¶„ë¥˜, ìš”ì•½, ì ìˆ˜ ìˆœìœ¼ë¡œ í•©ë‹ˆë‹¤."
        },
        
        # ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ëª¨ë¸ ì„¤ì • (Analyst)
        "analyst_model": {
            "provider": "Local",
            "name": "openai/gpt-oss-20b",
            "url": "http://192.168.1.105:11434/v1",
            "key": "",
            "prompt": "íˆ¬ì ì „ëµê°€ë¡œì„œ ì œê³µëœ ë‰´ìŠ¤ì˜ ì§€í‘œë¥¼ ìˆ˜ì§‘í•˜ì—¬ í‘œë¡œ ë§Œë“¤ê³  ê° ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ ì „ì²´ ì‹œí™©ê³¼ ìœ ë™ì„± ìœ„ê¸°ë¥¼ ì§„ë‹¨í•˜ê³  íˆ¬ììë¥¼ ìœ„í•œ ì„¹í„°ë³„ ì¡°ì–¸ ë° ì´í‰ì„ í•˜ì‹œì˜¤"
        },

        "report_news_count": 30,
        "report_auto_gen": False,
        "report_gen_time": "08:00",
        "report_days": 3
    }
    
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
                # ğŸ’¡ ìƒˆë¡œìš´ ë©€í‹° ëª¨ë¸ êµ¬ì¡°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›Œë„£ìŒ
                for key, val in default_structure.items():
                    if key not in loaded: 
                        loaded[key] = val
                return loaded
        except: pass
    return default_structure

# ì´ˆê¸° ì„¤ì • ë¡œë“œ
data = load_data()

# app.py ë‚´ì˜ is_filtered í•¨ìˆ˜ë¥¼ ì´ ë‚´ìš©ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.
def is_filtered(title, summary, g_inc, g_exc, l_inc="", l_exc=""):
    # ë³€ìˆ˜ ë³´ì¡´: ì œëª©(title) ê¸°ì¤€ í•„í„°ë§
    text = title.lower().strip()
    
    # 1. ì œì™¸ í•„í„°: ì „ì—­/ê°œë³„ ì œì™¸ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì— ìˆìœ¼ë©´ ì¦‰ì‹œ íƒˆë½
    exc_tags = [t.strip().lower() for t in (g_exc + "," + l_exc).split(",") if t.strip()]
    if any(t in text for t in exc_tags): 
        return False
    
    # 2. ì „ì—­ í¬í•¨ì–´: ê°’ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ì œëª©ì— í•´ë‹¹ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í†µê³¼
    g_inc_tags = [t.strip().lower() for t in g_inc.split(",") if t.strip()]
    if g_inc_tags and not any(t in text for t in g_inc_tags):
        return False
        
    # 3. ê°œë³„ í¬í•¨ì–´: ê°’ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ì œëª©ì— í•´ë‹¹ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í†µê³¼
    l_inc_tags = [t.strip().lower() for t in l_inc.split(",") if t.strip()]
    if l_inc_tags and not any(t in text for t in l_inc_tags):
        return False
    
    return True

def load_historical_contexts():
    """ê³¼ê±° ë¦¬í¬íŠ¸ ë§¥ë½ ë¡œë“œ ë¡œì§ [ë³´ì¡´]"""
    base_dir = REPORTS_BASE_DIR
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    for label, rel_path in dir_map.items():
        full_path = os.path.join(base_dir, rel_path)
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: í•´ë‹¹ ì£¼ê¸°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        else:
            context_text += f"\n<{label}>: ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
    return context_text

# ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì‹¤í–‰
data = load_data()

# --- 2. ë‰´ìŠ¤ ì²˜ë¦¬ ë° AI ë¶„ì„ í•¨ìˆ˜ ---
def get_ai_summary(title, content, system_instruction=None, role="filter"):
    # ğŸ•’ í˜„ì¬ ì‹œê°„ í™•ë³´ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    now_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ì„¤ì • ë°ì´í„°(data)ì—ì„œ ì—­í• ì— ë§ëŠ” ëª¨ë¸ ì„¤ì • ë¡œë“œ
    cfg = data.get("filter_model") if role == "filter" else data.get("analyst_model")
    
    base_url = cfg.get("url", "").rstrip('/')
    url = f"{base_url}/chat/completions"
    
    # ğŸ’¡ [ë³´ì•ˆ/ì¸ì¦] API Keyê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ í—¤ë”ì— ì¶”ê°€ (OpenAI ë“± ê³µìš© API ëŒ€ì‘)
    headers = {}
    if cfg.get("key"):
        headers["Authorization"] = f"Bearer {cfg['key']}"
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±: í˜„ì¬ ì‹œê°„ì„ ì£¼ì…í•˜ì—¬ AIê°€ ì‹œì  ì •ë³´ë¥¼ ì¸ì§€í•˜ê²Œ í•¨
    user_prompt = system_instruction if system_instruction else cfg["prompt"]
    final_role = f"í˜„ì¬ ì‹œê°: {now_time}\në¶„ì„ ì§€ì¹¨: {user_prompt}"

    payload = {
        "model": cfg["name"],
        "messages": [
            {"role": "system", "content": final_role},
            {"role": "user", "content": f"ë¶„ì„ ê¸°ì¤€ ì‹œê°: {now_time}\nì œëª©: {title}\në³¸ë¬¸: {content}"}
        ],
        "temperature": 0.3
    }

    try:
        # ëŒ€ëŸ‰ ë‰´ìŠ¤ ì²˜ë¦¬ë¥¼ ìœ„í•´ íƒ€ì„ì•„ì›ƒ 600ì´ˆ ìœ ì§€
        resp = requests.post(url, json=payload, headers=headers, timeout=600)
        resp.raise_for_status() 
        return resp.json()['choices'][0]['message']['content']

    except requests.exceptions.Timeout:
        error_msg = f"âŒ [TIMEOUT] AI ë¶„ì„ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. (ì„œë²„ ì‘ë‹µ í™•ì¸ í•„ìš”)"
        print(f"[{now_time}] {error_msg}")
        return error_msg

    except requests.exceptions.ConnectionError:
        error_msg = f"âŒ [CONNECTION] AI ì„œë²„({base_url})ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        print(f"[{now_time}] {error_msg}")
        return error_msg

    except Exception as e:
        error_msg = f"âŒ [ERROR] AI ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
        print(f"[{now_time}] {error_msg}")
        return error_msg
        
# [ìˆ˜ì •] ì¸ìì— pub_dt(ë‚ ì§œ)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
@st.dialog("ğŸ“Š AI ì •ë°€ ë¶„ì„ ë¦¬í¬íŠ¸")
def show_analysis_dialog(title, summary_text, pub_dt, role="filter"): 
    with st.spinner("AIê°€ ë‰´ìŠ¤ë¥¼ ì‹¬ì¸µ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        # ğŸ’¡ [ì „ëµ] ê¸°ì‚¬ ì‘ì„±ì¼(pub_dt)ê³¼ ë¶„ì„ ì‹œì (í˜„ì¬)ì˜ ê°„ê·¹ì„ AIê°€ ì¸ì§€í•˜ë„ë¡ ì œëª© êµ¬ì„±
        enhanced_title = f"(ê¸°ì‚¬ì‘ì„±ì¼: {pub_dt}) {title}"
        analysis = get_ai_summary(enhanced_title, summary_text, role=role)
    
    # ìƒë‹¨ í—¤ë” ì„¹ì…˜
    st.markdown(f"### {title}")
    st.caption(f"ğŸ“… ê¸°ì‚¬ ì‘ì„±ì¼: {pub_dt}") 
    st.divider()
    
    # AI ë¶„ì„ ë³¸ë¬¸
    st.markdown(analysis)
    st.divider()
    
    # í•˜ë‹¨ ì •ë³´ ë° ì›ë¬¸ ì„¹ì…˜
    with st.expander("ê¸°ì‚¬ ì›ë¬¸ ìš”ì•½ ë³´ê¸°"):
        st.write(summary_text)

    # ğŸ¤– ëª¨ë¸ ì •ë³´ ë° ë¶„ì„ ì‹œê° (ë””ë²„ê¹… ë° ì‹ ë¢°ë„ìš©)
    model_cfg = data.get("filter_model" if role == "filter" else "analyst_model", {})
    model_name = model_cfg.get("name", "Unknown Model")
    
    # ğŸ•’ í˜„ì¬ ë¶„ì„ ì‹œê°ì„ êµ¬í•´ì„œ ìº¡ì…˜ì— ì¶”ê°€
    analysis_time = datetime.now().strftime('%H:%M:%S')
    
    st.caption(
        f"ğŸ¤– ë¶„ì„ ëª¨ë¸: {model_name} | "
        f"ğŸ•’ ë¶„ì„ ì™„ë£Œ ì‹œê°: {analysis_time} | "
        f"ğŸ“Š ì—­í• : {'ë‰´ìŠ¤ í•„í„°ë§' if role == 'filter' else 'ì‹¬ì¸µ ë¶„ì„'}"
    )

def check_filters(title, include_str, exclude_str):
    title = title.lower().strip()
    if exclude_str:
        exc_tags = [t.strip().lower() for t in exclude_str.split(",") if t.strip()]
        if any(t in title for t in exc_tags): return False
    if include_str:
        inc_tags = [t.strip().lower() for t in include_str.split(",") if t.strip()]
        if not any(t in title for t in inc_tags): return False
    return True

def clean_html(raw_html):
    if not raw_html: return "ìš”ì•½ ë‚´ìš© ì—†ìŒ"
    soup = BeautifulSoup(raw_html, "html.parser")
    for s in soup(['style', 'script', 'span']): s.decompose()
    return re.sub(r'\s+', ' ', soup.get_text()).strip()

def parse_rss_date(date_str):
    try:
        p = feedparser._parse_date(date_str)
        return datetime.fromtimestamp(time.mktime(p))
    except: return datetime.now()

def format_korean_unit(num):
    """ìˆ«ìë¥¼ ì¡°, ì–µ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if num is None or num == 0: return "0"
    if num >= 1e12:
        return f"{num / 1e12:.2f}ì¡°"
    elif num >= 1e8:
        return f"{num / 1e8:.2f}ì–µ"
    elif num >= 1e4:
        return f"{num / 1e4:.1f}ë§Œ"
    return f"{num:,.0f}"

def load_pending_files(range_type, target_feed=None):
    news_list = []
    if not os.path.exists(PENDING_PATH): return news_list
    today_date = date.today()
    one_week_ago = datetime.now() - timedelta(days=7)
    for filename in os.listdir(PENDING_PATH):
        if filename.endswith(".txt"):
            try:
                with open(os.path.join(PENDING_PATH, filename), 'r', encoding='utf-8') as f:
                    lines = f.read().splitlines()
                    title = lines[0].replace("ì œëª©: ", "")
                    link = lines[1].replace("ë§í¬: ", "")
                    pub_str = lines[2].replace("ë‚ ì§œ: ", "")
                    summary = "\n".join(lines[3:]).replace("ìš”ì•½: ", "")
                    pub_dt = parse_rss_date(pub_str)
                    if range_type == "ì˜¤ëŠ˜" and pub_dt.date() != today_date: continue
                    if range_type == "ì¼ì£¼ì¼" and pub_dt < one_week_ago: continue
                    if target_feed:
                        if not check_filters(title, target_feed.get('include', ""), target_feed.get('exclude', "")): continue
                    news_list.append({"title": title, "link": link, "published": pub_str, "summary": summary, "pub_dt": pub_dt, "source": "ì €ì¥ëœ ë°ì´í„°"})
            except: continue
    news_list.sort(key=lambda x: x['pub_dt'], reverse=True)
    return news_list
    
def save_report_to_file(content, section_name):
    """AI ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì£¼ê¸°ì— ë”°ë¼ ì˜¤ë˜ëœ íŒŒì¼ì„ ì •ì œí•©ë‹ˆë‹¤."""
    # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ì„¸ë¶„í™” (ê¸°ì¡´ ê²½ë¡œ ìœ ì§€)
    base_dir = "/share/ai_analyst/reports"
    dir_map = {
        'daily': '01_daily', 
        'weekly': '02_weekly', 
        'monthly': '03_monthly', 
        'yearly': '04_yearly'
    }
    
    # section_nameì´ ë§µì— ì—†ìœ¼ë©´ ê¸°ë³¸(etc) í´ë” ì‚¬ìš©
    subdir = dir_map.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(base_dir, subdir)
    os.makedirs(report_dir, exist_ok=True) # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    
    # 2. íŒŒì¼ëª… ìƒì„± ë° ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê¸°ë¡ìš©)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"{timestamp}_{section_name.replace(' ', '_')}.txt"
    filepath = os.path.join(report_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)

    # 3. ğŸ¯ AI ì°¸ì¡°ìš© Latest íŒŒì¼ ê°±ì‹  (RAG ë¶„ì„ìš© ê³ ì • ê²½ë¡œ)
    # ì´ íŒŒì¼ì€ load_historical_contexts()ì—ì„œ ìµœì‹  ë§¥ë½ì„ ì½ì„ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    latest_path = os.path.join(report_dir, "latest.txt")
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(content)

    # 4. ğŸ§¹ ê³„ì¸µí˜• ìë™ ì •ì œ (Purge) ë¡œì§
    # ë³´ê´€ ê·œì¹™: Daily(7ì¼), Weekly(30ì¼), Monthly(365ì¼)
    purge_rules = {'01_daily': 7, '02_weekly': 30, '03_monthly': 365}
    
    if subdir in purge_rules:
        limit_days = purge_rules[subdir]
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³´ê´€ í•œê³„ ì‹œì  ê³„ì‚°
        threshold = time.time() - (limit_days * 86400)
        
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue # ìµœì‹  ë§¥ë½ íŒŒì¼ì€ ë³´í˜¸
            f_p = os.path.join(report_dir, f)
            # ìˆ˜ì • ì‹œê°„(mtime)ì´ í•œê³„ì ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                try:
                    os.remove(f_p)
                except Exception as e:
                    print(f"íŒŒì¼ ì‚­ì œ ì—ëŸ¬ ({f}): {e}")
                
    return filepath
    
def save_data(data):
    """ë³€ê²½ëœ ì„¤ì • ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥í•©ë‹ˆë‹¤."""
    # í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)
    
    # íŒŒì¼ì„ ì—´ì–´ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
    with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
        # í•œê¸€ ê¹¨ì§ ë°©ì§€ ë° ê°€ë…ì„±ì„ ìœ„í•´ ì˜µì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        json.dump(data, f, ensure_ascii=False, indent=2)

    # 2. í‘œì‹œìš© ì´ë¦„ ë”•ì…”ë„ˆë¦¬
    
# --- 3. UI ë° CSS ì„¤ì • ---
st.set_page_config(page_title="AI Analyst", layout="wide")

st.markdown("""
    <style>
    [data-testid="stPopoverBody"] { width: 170px !important; padding: 10px !important; }
    [data-testid="stPopoverBody"] button { padding: 2px 5px !important; margin-bottom: 2px !important; height: auto !important; font-size: 14px !important; }
    [data-testid="stSidebar"] { display: none; }
    /* ì§€í‘œ ê´€ë ¨ CSSëŠ” í•„ìš” ì—†ìœ¼ë¯€ë¡œ stMetricValue ìŠ¤íƒ€ì¼ì€ ì‚­ì œí•˜ê±°ë‚˜ ìœ ì§€í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤ */
    </style>
    """, unsafe_allow_html=True)

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì • (ê¸°ë³¸ ë©”ë‰´ë¥¼ "ë‰´ìŠ¤"ë¡œ ë³€ê²½)
if 'active_menu' not in st.session_state: 
    st.session_state.active_menu = "ë‰´ìŠ¤"
if 'current_feed_idx' not in st.session_state: 
    st.session_state.current_feed_idx = "all"
if 'page_number' not in st.session_state: 
    st.session_state.page_number = 1

# --- 4. ìµœìƒë‹¨ ëŒ€ë©”ë‰´ (ì‹œì¥ ì§€í‘œ ì œê±°) ---
st.title("ğŸ¤– AI Analyst System")

# ë©”ë‰´ê°€ 3ê°œì´ë¯€ë¡œ ì»¬ëŸ¼ì„ 3ê°œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
m_cols = st.columns(3)
menu_items = [
    ("ğŸ“¡ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë°", "ë‰´ìŠ¤"), 
    ("ğŸ›ï¸ AI íˆ¬ì ë³´ê³ ì„œ", "AI"), 
    ("âš™ï¸ ì„¤ì •", "ì„¤ì •")
]

for i, (label, m_key) in enumerate(menu_items):
    if m_cols[i].button(label, use_container_width=True, type="primary" if st.session_state.active_menu == m_key else "secondary"):
        st.session_state.active_menu = m_key
        st.rerun()

st.divider()

# --- 5. ë©”ë‰´ë³„ ë³¸ë¬¸ í™”ë©´ êµ¬ì„± ---

if st.session_state.active_menu == "ì„¤ì •":
    st.subheader("âš™ï¸ ë¡œì»¬ ë©€í‹° AI ì„œë²„ ë° ì‹œìŠ¤í…œ ì„¤ì •")
    
    # ì„¸ ê°€ì§€ ì„¤ì • íƒ­ìœ¼ë¡œ í†µí•© ê´€ë¦¬
    tab_f, tab_a, tab_g = st.tabs(["ğŸ¯ ë‰´ìŠ¤ íŒë… (Filter)", "ğŸ›ï¸ íˆ¬ì ë¶„ì„ (Analyst)", "ğŸŒ ì¼ë°˜ ì„¤ì •"])

    with tab_f:
        st.markdown("#### ğŸ“¡ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° ìš”ì•½ìš© ëª¨ë¸")
        f_cfg = data.get("filter_model")
        # ê³ ìœ  í‚¤: f_url_input
        f_url = st.text_input("API ì„œë²„ ì£¼ì†Œ (URL)", value=f_cfg.get("url"), help="ì˜ˆ: http://192.168.1.2:1234/v1", key="f_url_input")
        f_name = st.text_input("ëª¨ë¸ëª…", value=f_cfg.get("name"), key="f_name_input")
        f_prompt = st.text_area("ê¸°ë³¸ ìš”ì•½ ì§€ì¹¨", value=f_cfg.get("prompt"), height=100, key="f_prompt_input")
        
        if st.button("ğŸ’¾ íŒë… ëª¨ë¸ ì„¤ì • ì €ì¥", use_container_width=True):
            data["filter_model"].update({"url": f_url, "name": f_name, "prompt": f_prompt})
            save_data(data); st.success("âœ… íŒë… ëª¨ë¸ ì„¤ì • ì €ì¥ ì™„ë£Œ!")

    with tab_a:
        st.markdown("#### ğŸ›ï¸ íˆ¬ì ë³´ê³ ì„œ ìƒì„±ìš© ëª¨ë¸")
        a_cfg = data.get("analyst_model")
        # ê³ ìœ  í‚¤: a_url_input
        a_url = st.text_input("API ì„œë²„ ì£¼ì†Œ (URL)", value=a_cfg.get("url"), help="ì˜ˆ: http://192.168.1.105:11434/v1", key="a_url_input")
        a_name = st.text_input("ëª¨ë¸ëª…", value=a_cfg.get("name"), key="a_name_input")
        
        if st.button("ğŸ’¾ ë¶„ì„ ëª¨ë¸ ì„¤ì • ì €ì¥", use_container_width=True):
            data["analyst_model"].update({"url": a_url, "name": a_name})
            save_data(data); st.success("âœ… ë¶„ì„ ëª¨ë¸ ì„¤ì • ì €ì¥ ì™„ë£Œ!")

    with tab_g:
        st.markdown("#### âš™ï¸ ì‹œìŠ¤í…œ ê³µí†µ ë° ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¤ì •")
        col1, col2 = st.columns(2)
        
        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë³´ê´€ ì„¤ì •
        new_retention = col1.slider("ë‰´ìŠ¤ íŒŒì¼ ë³´ê´€ ê¸°ê°„ (ì¼)", 1, 30, value=data.get("retention_days", 7), key="cfg_retention_days")
        new_interval = col2.number_input("RSS ìˆ˜ì§‘ ì£¼ê¸° (ë¶„)", 1, value=data.get("update_interval", 10), key="cfg_update_interval")
        
        st.divider()
        
        st.markdown("#### ğŸ“‘ AI íˆ¬ì ë³´ê³ ì„œ ìë™í™”")
        # 2. ìë™ ìƒì„± ë° ì‹œê°„ ì„¤ì •
        col_auto, col_time = st.columns([0.4, 0.6])
        auto_gen = col_auto.toggle("ë§¤ì¼ ë³´ê³ ì„œ ìë™ ìƒì„±", value=data.get("report_auto_gen", False), key="cfg_report_auto_gen")
        gen_time = col_time.text_input("ìƒì„± ì‹œê°„ (24ì‹œê°„ì œ, ì˜ˆ: 08:00)", value=data.get("report_gen_time", "08:00"), key="cfg_report_gen_time")
        
        # 3. ë¶„ì„ ë‰´ìŠ¤ ê°œìˆ˜ ì„¤ì • (ìµœëŒ€ 500ê°œë¡œ í™•ì¥ ë° ë‚ ì§œ ë²”ìœ„ ì œê±°)
        # ì´ì œ AIëŠ” ë‚ ì§œ ë²”ìœ„ ëŒ€ì‹  'ìµœì‹  ë‰´ìŠ¤ Nê°œ'ì™€ 'ê³¼ê±° ë¦¬í¬íŠ¸ ë§¥ë½'ìœ¼ë¡œë§Œ ë¶„ì„í•©ë‹ˆë‹¤.
        report_news_count = st.slider("ë¶„ì„ í¬í•¨ ë‰´ìŠ¤ ê°œìˆ˜ (ìµœëŒ€ 500ê°œ)", 10, 500, value=data.get("report_news_count", 100), key="cfg_report_news_count")

        if st.button("ğŸ’¾ ëª¨ë“  ì‹œìŠ¤í…œ ì„¤ì • ì €ì¥", use_container_width=True, type="primary"):
            # ë°ì´í„° êµ¬ì¡° ì—…ë°ì´íŠ¸ (report_days í•­ëª© ì œê±°)
            data.update({
                "retention_days": new_retention,
                "update_interval": new_interval,
                "report_auto_gen": auto_gen,
                "report_gen_time": gen_time,
                "report_news_count": report_news_count
            })
            # í•„ìš” ì—†ëŠ” êµ¬í˜• ì„¤ì • í‚¤ ì‚­ì œ
            if "report_days" in data:
                del data["report_days"]
                
            save_data(data)
            st.success("âœ… ë¶ˆí•„ìš”í•œ ë²”ìœ„ë¥¼ ì œê±°í•˜ê³  ë‰´ìŠ¤ ì²˜ë¦¬ëŸ‰ì´ 500ê°œë¡œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun() 

    st.write("") # ê°„ê²© ì¡°ì ˆ
        

# [2. ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë°]
elif st.session_state.active_menu == "ë‰´ìŠ¤":    
    col_side, col_main = st.columns([0.22, 0.78])
    
    with col_side:
        st.markdown("#### ğŸ“Œ RSS ê´€ë¦¬")
        # ì „ì²´ ë³´ê¸° ë²„íŠ¼
        if st.button("ğŸ  ì „ì²´ ë³´ê¸°", use_container_width=True, type="primary" if st.session_state.current_feed_idx == "all" else "secondary"):
            st.session_state.current_feed_idx = "all"; st.session_state.page_number = 1; st.rerun()
        
# ê° í”¼ë“œ ë¦¬ìŠ¤íŠ¸ ë° ê´€ë¦¬ ë©”ë‰´
        for i, f in enumerate(data.get('feeds', [])):
            btn_col, pop_col = st.columns([0.8, 0.2])
            with btn_col:
                if st.button(f['name'], key=f"f_{i}", use_container_width=True, type="primary" if st.session_state.current_feed_idx == i else "secondary"):
                    st.session_state.current_feed_idx = i; st.session_state.page_number = 1; st.rerun()
            with pop_col:
                # ì•„ì´ì½˜ ì—†ì´ 'ì„¤ì •' í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ë” ì‘ê²Œ ì¤„ì¸ íŒì—…
                with st.popover(""):
                    # ì•„ì´ì½˜(âœï¸, ğŸ”, ğŸ—‘ï¸)ì„ ëª¨ë‘ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¡œë§Œ êµ¬ì„±
                    if st.button("í¸ì§‘", key=f"ed_{i}", use_container_width=True):
                        @st.dialog("í”¼ë“œ ìˆ˜ì •")
                        def ed_diag(idx=i):
                            fe = data['feeds'][idx]
                            n = st.text_input("ì´ë¦„", value=fe['name'])
                            u = st.text_input("URL", value=fe['url'])
                            if st.button("ì €ì¥"):
                                data['feeds'][idx].update({"name": n, "url": u}); save_data(data); st.rerun()
                        ed_diag()
                    
                    if st.button("í•„í„°", key=f"fi_{i}", use_container_width=True):
                        @st.dialog("í‚¤ì›Œë“œ í•„í„°")
                        def fi_diag(idx=i):
                            fe = data['feeds'][idx]
                            inc = st.text_area("í¬í•¨ í‚¤ì›Œë“œ", value=fe.get('include', ""))
                            exc = st.text_area("ì œì™¸ í‚¤ì›Œë“œ", value=fe.get('exclude', ""))
                            if st.button("í•„í„° ì ìš©"):
                                data['feeds'][idx].update({"include": inc, "exclude": exc}); save_data(data); st.rerun()
                        fi_diag()
                        
                    if st.button("ì‚­ì œ", key=f"de_{i}", use_container_width=True):
                        data['feeds'].pop(i); save_data(data); st.rerun()
        
        st.divider()
        # [ë³µêµ¬] í”¼ë“œ ì¶”ê°€ ë²„íŠ¼
        if st.button("â• ìƒˆ RSS ì¶”ê°€", use_container_width=True):
            @st.dialog("ìƒˆ RSS ë“±ë¡")
            def add_diag():
                n = st.text_input("í”¼ë“œ ì´ë¦„ (ì˜ˆ: ì—°í•©ë‰´ìŠ¤)")
                u = st.text_input("RSS URL ì£¼ì†Œ")
                if st.button("ë“±ë¡ ì™„ë£Œ"):
                    data['feeds'].append({"name": n, "url": u, "include": "", "exclude": ""})
                    save_data(data); st.rerun()
            add_diag()
    with col_side:
        # [ì¶”ê°€] ì „ì—­ í•„í„° ì„¤ì • êµ¬ì—­
        with st.expander("ğŸŒ ì „ì—­ í•„í„° ì„¤ì •", expanded=False):
            g_inc = st.text_area("ì „ì—­ í¬í•¨ í‚¤ì›Œë“œ", value=data.get("global_include", ""), help="ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„")
            g_exc = st.text_area("ì „ì—­ ì œì™¸ í‚¤ì›Œë“œ", value=data.get("global_exclude", ""), help="ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„")
            if st.button("ì „ì—­ í•„í„° ì €ì¥", use_container_width=True):
                data.update({"global_include": g_inc, "global_exclude": g_exc})
                save_data(data)
                st.toast("ì „ì—­ í•„í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    with col_main:
        full_list = []
        target = data.get('feeds', []) if st.session_state.current_feed_idx == "all" else [data['feeds'][st.session_state.current_feed_idx]]
        
        for f_info in target:
            try:
                parsed = feedparser.parse(f_info['url'])
                for e in parsed.entries:
                    # ê°•í™”ëœ ì œëª© í•„í„° ì ìš© (ë²„ê·¸ ìˆ˜ì •ë¨)
                    if is_filtered(e.title, e.get('summary', ''), 
                                   data.get("global_include", ""), data.get("global_exclude", ""),
                                   f_info.get('include', ""), f_info.get('exclude', "")):
                        e['source'] = f_info['name']
                        full_list.append(e)
            except: continue
            
        full_list.sort(key=lambda x: x.get('published_parsed', 0), reverse=True)
        
        if full_list:
            # 1. í˜ì´ì§€ë„¤ì´ì…˜ ë³€ìˆ˜ ì •ì˜
            items_per_page = 10
            total_pages = math.ceil(len(full_list) / items_per_page)
            
            # 2. í˜„ì¬ í˜ì´ì§€ ìŠ¬ë¼ì´ì‹± ê³„ì‚°
            start_idx = (st.session_state.page_number - 1) * items_per_page
            end_idx = start_idx + items_per_page
            
            # 3. ë‰´ìŠ¤ ê¸°ì‚¬ ë°˜ë³µ ì¶œë ¥
            for entry in full_list[start_idx:end_idx]:
                with st.container(border=True):
                    st.caption(f"ğŸ“ {entry.get('source')} | {entry.get('published', '')}")
                    st.markdown(f"#### {entry.get('title')}")
                    
                    cleaned_summary = clean_html(entry.get('summary', ''))
                    st.write(cleaned_summary[:200] + "...")
                    
                    btn_c1, btn_c2 = st.columns([0.2, 0.8])
                    
                    # 1. ì›ë¬¸ ì½ê¸°
                    btn_c1.link_button("ğŸŒ ì›ë¬¸ ì½ê¸°", entry.get('link', '#'), use_container_width=True)
                    
                    # 2. AI ìš”ì•½ ë¶„ì„ (í´ë¦­ ì¦‰ì‹œ ë¶„ì„ íŒì—… ì‹¤í–‰)
                    if btn_c2.button("ğŸ¤– AI ìš”ì•½ ë¶„ì„", key=f"ai_btn_{entry.get('link')}", use_container_width=True):
                        show_analysis_dialog(entry.get('title'), cleaned_summary, entry.get('published', 'ë‚ ì§œ ë¯¸ìƒ'), role="filter")

            st.divider()
            
            # --- [ 4. ê°œì„ ëœ í˜ì´ì§€ ë‚´ë¹„ê²Œì´í„° ] ---
            if total_pages > 1:
                # 10ë‹¨ìœ„ ë­‰ì¹˜ ê³„ì‚°
                current_group = (st.session_state.page_number - 1) // 10
                start_page = current_group * 10 + 1
                end_page = min(start_page + 9, total_pages)
                
                # ë²„íŠ¼ ë ˆì´ì•„ì›ƒ (ì´ì „ + ìˆ«ì 10ê°œ + ë‹¤ìŒ)
                nav_cols = st.columns([0.6] + [1] * (end_page - start_page + 1) + [0.6])
                
                # [ < ] ì´ì „ 10ê°œ ë­‰ì¹˜ ì´ë™
                if start_page > 1:
                    if nav_cols[0].button("<", key="prev_group"):
                        st.session_state.page_number = start_page - 1
                        st.rerun()
                
                # ìˆ«ì ë²„íŠ¼ë“¤
                for i, page_idx in enumerate(range(start_page, end_page + 1)):
                    if nav_cols[i+1].button(
                        str(page_idx), 
                        key=f"page_{page_idx}",
                        type="primary" if st.session_state.page_number == page_idx else "secondary",
                        use_container_width=True
                    ):
                        st.session_state.page_number = page_idx
                        st.rerun()
                
                # [ > ] ë‹¤ìŒ 10ê°œ ë­‰ì¹˜ ì´ë™
                if end_page < total_pages:
                    if nav_cols[-1].button(">", key="next_group"):
                        st.session_state.page_number = end_page + 1
                        st.rerun()
        else:
            st.warning("ğŸ“¡ í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

elif st.session_state.active_menu == "AI":
    st.subheader("ğŸ“‘ AI íˆ¬ì ë³´ê³ ì„œ")
    
    # 1. ì„¸ì…˜ ë° ê²½ë¡œ ì„¤ì •
    if "report_chat_history" not in st.session_state:
        st.session_state.report_chat_history = []
    if "last_report_content" not in st.session_state:
        st.session_state.last_report_content = ""

    # ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
    REPORT_DIR = "/share/ai_analyst/reports"
    os.makedirs(REPORT_DIR, exist_ok=True)

    # [ì‹ ê·œ ë¡œì§] ì„¸ì…˜ì— ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ì €ì¥ëœ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœì‹  ê²ƒ ë¡œë“œ
    if not st.session_state.last_report_content:
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ë³´ (latest.txt ì œì™¸í•œ ê¸°ë¡ íŒŒì¼ë“¤)
        report_files = sorted([f for f in os.listdir(REPORT_DIR) if f.endswith(".txt") and "latest" not in f], reverse=True)
        if report_files:
            latest_file = report_files[0]
            try:
                # ìµœì‹  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ì„œë¸Œ ë””ë ‰í† ë¦¬ê¹Œì§€ ì°¾ê¸° ìœ„í•´ daily í´ë” í™•ì¸
                daily_dir = os.path.join(REPORT_DIR, "01_daily")
                daily_files = sorted([f for f in os.listdir(daily_dir) if f.endswith(".txt")], reverse=True)
                if daily_files:
                    with open(os.path.join(daily_dir, daily_files[0]), "r", encoding="utf-8") as f:
                        st.session_state.last_report_content = f.read()
            except:
                pass

    # ì„¤ì •ê°’ ë¡œë“œ
    analysis_range = data.get("report_days", 3)
    council_instruction = data.get("council_prompt", "ì‹œë‹ˆì–´ íˆ¬ì ì „ëµê°€ë¡œì„œ ì¢…í•© ì˜ê²¬ì„ ì œì‹œí•˜ë¼.")

    # 2. ë¶„ì„ ì‹¤í–‰ ì„¹ì…˜
    with st.container(border=True):
        st.markdown("### ğŸ›ï¸ ì‹œì¥ ì¢…í•© ì˜ê²¬ ë¶„ì„")
        
        # ë¶„ì„ ì§€ì¹¨ ì…ë ¥ ì˜ì—­
        new_instruction = st.text_area(
            "ë¶„ì„ ì§€ì¹¨ ìˆ˜ì •", 
            value=council_instruction, 
            height=150, 
            key="report_instr_area"
        )
        
        # ë¶„ì„ ì§€ì¹¨ ì €ì¥ ë²„íŠ¼
        if st.button("ğŸ’¾ ë¶„ì„ ì§€ì¹¨ ì €ì¥", use_container_width=True):
            data["council_prompt"] = new_instruction
            save_data(data)
            st.success("âœ… ë¶„ì„ ì§€ì¹¨ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.toast("ì§€ì¹¨ ì €ì¥ ì™„ë£Œ")

        st.divider()

        # ë³´ê³ ì„œ ìƒì„± ë²„íŠ¼
        if st.button("ğŸš€ ìƒˆ ì¢…í•© AI ë³´ê³ ì„œ ìƒì„±", type="primary", use_container_width=True):
            st.session_state.last_report_content = ""
            st.session_state.report_chat_history = []
            
            with st.spinner("ê³¼ê±° ë§¥ë½ ë³µê¸° ë° ìµœì‹  ë‰´ìŠ¤ í†µí•© ë¶„ì„ ì¤‘..."):
                # 1. [RAG] ê³¼ê±° ë³´ê³ ì„œ ë§¥ë½ ë¡œë“œ (ë³´ì¡´ëœ í•¨ìˆ˜)
                historical_context = load_historical_contexts()

                # 2. [News] ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (DB ìˆ˜ì¹˜ ë¡œì§ ì œê±°)
                raw_news = load_pending_files("ì¼ì£¼ì¼") 
                target_date = datetime.now() - timedelta(days=analysis_range)
                
                # ì„¤ì •ëœ ë²”ìœ„ ë‚´ì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                recent_news = [n for n in raw_news if n['pub_dt'] >= target_date]
                news_limit = data.get("report_news_count", 50)
                
                # AIì—ê²Œ ë‚ ì§œ, ì¶œì²˜, ì œëª© ì „ë‹¬
                news_items = []
                for n in recent_news[:news_limit]:
                    time_str = n['pub_dt'].strftime("%m/%d %H:%M")
                    source = n.get('source', 'ë‰´ìŠ¤')
                    news_items.append(f"[{time_str}][{source}] {n['title']}")
                
                news_context = "### [ ìµœì‹  ì£¼ìš” ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ]\n" + "\n".join(news_items)

                if not news_items:
                    st.warning("ğŸ“¡ ë¶„ì„ ë²”ìœ„ ë‚´ì— ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # ğŸ¯ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±: DB ìˆ˜ì¹˜ ëŒ€ì‹  í…ìŠ¤íŠ¸ ë§¥ë½ì— ì§‘ì¤‘
                    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
                    full_instruction = (
                        f"í˜„ì¬ ë¶„ì„ ì‹œì : {now_str}\n"
                        f"ë‹¹ì‹ ì€ {new_instruction}\n\n"
                        f"{historical_context}\n"
                        f"ì§€ì¹¨: ìœ„ì˜ ê³¼ê±° ì „ëµ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬, ì•„ë˜ ë‚˜ì—´ëœ ìµœì‹  ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•˜ê³  ëŒ€ì‘ ì „ëµì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤."
                    )

                    # ì‹¬ì¸µ ë¶„ì„ ëª¨ë¸(analyst role) í˜¸ì¶œ
                    report = get_ai_summary(
                        title=f"{date.today()} ì¢…í•© ì „ëµ ë³´ê³ ì„œ", 
                        content=news_context, 
                        system_instruction=full_instruction,
                        role="analyst"
                    )
                    
                    st.session_state.last_report_content = report   
                    save_report_to_file(report, "daily")
                    st.rerun()

    # 3. ê²°ê³¼ ì¶œë ¥ ë° ëŒ€í™”ì°½
    if st.session_state.last_report_content:
        st.markdown("---")
        st.markdown(f"#### ğŸ“Š íˆ¬ì ë³´ê³ ì„œ")
        
        with st.container(border=True):
            st.markdown(st.session_state.last_report_content)

        # ğŸ’¬ ì§ˆì˜ì‘ë‹µ ë‚´ì—­ í‘œì‹œ
        if st.session_state.report_chat_history:
            st.markdown("#### ğŸ’¬ ì§ˆì˜ì‘ë‹µ ë‚´ì—­")
            for message in st.session_state.report_chat_history:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # âœ‰ï¸ ì±„íŒ… ì…ë ¥ (DB ì§€í‘œ ì£¼ì… ë¡œì§ ì œê±°)
        if chat_input := st.chat_input("ë³´ê³ ì„œ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”."):
            now_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            st.session_state.report_chat_history.append({"role": "user", "content": chat_input})
            
            # ğŸ’¡ [í”„ë¡¬í”„íŠ¸] ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ë§¥ë½ ìœ„ì£¼ë¡œ ë‹µë³€ ìœ ë„
            chat_context = (
                f"ë‹¹ì‹ ì€ ì´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•œ ì „ë¬¸ íˆ¬ì ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
                f"í˜„ì¬ ì‹œê°: {now_time}\n\n"
                f"ğŸ“ [ì‘ì„±ëœ ë³´ê³ ì„œ ë‚´ìš©]\n{st.session_state.last_report_content}\n\n"
                f"ì§€ì¹¨: ì‚¬ìš©ìê°€ ìœ„ ë³´ê³ ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³´ê³ ì„œì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©° ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤."
            )
            
            response = get_ai_summary(title="ë³´ê³ ì„œ ë‚´ìš© ì§ˆì˜", content=chat_input, system_instruction=chat_context, role="analyst")
            st.session_state.report_chat_history.append({"role": "assistant", "content": response})
            st.rerun()

    st.divider()
    st.caption("ğŸ’¾ ìµœê·¼ ìƒì„±ëœ ë³´ê³ ì„œëŠ” /share/ai_analyst/reports ì— ì €ì¥ë©ë‹ˆë‹¤.")


