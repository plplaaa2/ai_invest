import feedparser
import time
import os
import json
import hashlib
import requests
from datetime import datetime, timedelta, date

# --- ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€) ---
CONFIG_PATH = "/share/ai_analyst/rss_config.json"
PENDING_PATH = "/share/ai_analyst/pending"
REPORTS_BASE_DIR = "/share/ai_analyst/reports"

processed_titles = set()

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± ë³´ì¥
os.makedirs(PENDING_PATH, exist_ok=True)
os.makedirs(REPORTS_BASE_DIR, exist_ok=True)

def save_file(entry, feed_name):
    """ì¤‘ë³µì„ ì œê±°í•˜ê³  ë‰´ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    global processed_titles    
    # 1. ì œëª© ì •ì œ ë° ì¤‘ë³µ íŒë‹¨ìš© í‚¤ ìƒì„±
    title = entry.title.strip()
    summary = entry.get('summary', 'ë‚´ìš© ì—†ìŒ')
    current_content_len = len(title) + len(summary)
    # ê³µë°±ê³¼ íŠ¹ì • ë¬¸êµ¬ë¥¼ ì œê±°í•œ ì• 18ìë¡œ ìœ ì‚¬ë„ ì²´í¬
    clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
    
    # 2. 2ì¤‘ ì¤‘ë³µ ì²´í¬ (ë©”ëª¨ë¦¬ ìºì‹œ or ë¬¼ë¦¬ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€)
    title_hash = hashlib.md5(title.encode('utf-8')).hexdigest()
    fname = f"{PENDING_PATH}/{title_hash}.txt"
    
# ğŸ¯ ì¤‘ë³µ íŒë‹¨ ì‹œ 'ë®ì–´ì“°ê¸°' ì „ëµ ë„ì…
    if clean_key in processed_titles or os.path.exists(fname):
        # ì´ë¯¸ íŒŒì¼ì´ ìˆë‹¤ë©´ ê¸°ì¡´ íŒŒì¼ì˜ í¬ê¸°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        if os.path.exists(fname):
            existing_size = os.path.getsize(fname)
            # ğŸ’¡ ìƒˆ ê¸°ì‚¬ê°€ ê¸°ì¡´ ê¸°ì‚¬ë³´ë‹¤ ì •ë³´ëŸ‰(ìš©ëŸ‰)ì´ ë” ë§ì„ ë•Œë§Œ êµì²´í•©ë‹ˆë‹¤.
            if current_content_len > existing_size:
                pass # ì•„ë˜ ì €ì¥ ë¡œì§ìœ¼ë¡œ ì§„í–‰
            else:
                return False # ê¸°ì¡´ ê¸°ì‚¬ê°€ ë” ì•Œì°¨ë¯€ë¡œ ìŠ¤í‚µ
        else:
            return False # ë©”ëª¨ë¦¬ ìºì‹œì—ë§Œ ìˆëŠ” ê²½ìš°ë„ ìŠ¤í‚µ

    pub_date = entry.get('published') or datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")

    # 4. íŒŒì¼ ì“°ê¸°
    try:
        with open(fname, "w", encoding="utf-8") as f:
            f.write(f"ì œëª©: {title}\n")
            f.write(f"ë§í¬: {entry.link}\n")
            f.write(f"ë‚ ì§œ: {pub_date}\n")
            f.write(f"ìš”ì•½: {entry.get('summary', 'ë‚´ìš© ì—†ìŒ')}")
        
        processed_titles.add(clean_key)
        return True
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    if any(x in text for x in exc_list if x):
        return False
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    
    for filename in os.listdir(PENDING_PATH):
        file_path = os.path.join(PENDING_PATH, filename)
        if os.path.isfile(file_path) and filename.endswith(".txt"):
            if (current_time - os.path.getmtime(file_path)) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: pass
    
    # íŒŒì¼ ì‚­ì œ ì‹œ ë©”ëª¨ë¦¬ ìºì‹œë„ í•¨ê»˜ ë¹„ì›Œ ì‹œìŠ¤í…œì„ ê°€ë³ê²Œ ìœ ì§€
    processed_titles.clear()
    if deleted_count > 0:
        print(f"ğŸ§¹ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì •ë¦¬í•˜ê³  ì¤‘ë³µ í•„í„°ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (ì „ì—­ í•„í„°ë§ ë° 2ì¤‘ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ)...")
    
    while True:
        # 1. ì„¤ì • ë¡œë“œ
        config = {"feeds": [], "update_interval": 10, "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except: pass
        
        interval = config.get("update_interval", 10)
        cleanup_old_files(config.get("retention_days", 7))
        
        g_inc = [k.strip().lower() for k in config.get('global_include', "").split(",") if k.strip()]
        g_exc = [k.strip().lower() for k in config.get('global_exclude', "").split(",") if k.strip()]

        # 2. í”¼ë“œ ìˆœíšŒ
        feeds = config.get("feeds", [])
        total_found, new_saved = 0, 0

        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                
                for entry in parsed.entries[:50]:
                    total_found += 1
                    if not check_logic(entry.title, g_inc, g_exc): continue
                    if not check_logic(entry.title, l_inc, l_exc): continue
                    
                    if save_file(entry, feed['name']):
                        new_saved += 1
            except: continue
        
        # 3. ì‹¤ì‹œê°„ ë³´ê³  ë¡œê·¸
        if total_found > 0:
            print(f"ğŸ“Š ìˆ˜ì§‘ í˜„í™©: ë°œê²¬ {total_found}ê°œ | ì‹ ê·œ {new_saved}ê°œ | ì¤‘ë³µ/í•„í„° ì œì™¸ {total_found - new_saved}ê°œ")
        
        print(f"ğŸ’¤ {interval}ë¶„ í›„ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.")
        time.sleep(interval * 60)

def load_historical_contexts():
    """ê³¼ê±° ë¦¬í¬íŠ¸ ë§¥ë½ ë¡œë“œ (RAG ê¸°ëŠ¥)"""
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    for label, rel_path in dir_map.items():
        full_path = os.path.join(REPORTS_BASE_DIR, rel_path)
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: ë°ì´í„° ë¹„ì–´ ìˆìŒ.\n"
        else:
            context_text += f"\n<{label}>: ì´ì „ ê¸°ë¡ ì—†ìŒ. í˜„ì¬ ë‰´ìŠ¤ ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
    return context_text

def save_report_to_file(content, section_name):
    """AI ë³´ê³ ì„œ ê³„ì¸µí˜• ì €ì¥ ë° ì •ì œ"""
    subdir = {'daily': '01_daily', 'weekly': '02_weekly', 'monthly': '03_monthly'}.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(REPORTS_BASE_DIR, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filepath = os.path.join(report_dir, f"{timestamp}_{section_name}.txt")
    
    with open(filepath, "w", encoding="utf-8") as f: f.write(content)
    with open(os.path.join(report_dir, "latest.txt"), "w", encoding="utf-8") as f: f.write(content)

    # ìë™ ì •ì œ
    purge_rules = {'01_daily': 7, '02_weekly': 30, '03_monthly': 365}
    if subdir in purge_rules:
        threshold = time.time() - (purge_rules[subdir] * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
    return filepath

def generate_auto_report(config_data):
    """DB ì—†ì´ ë‰´ìŠ¤ ë° ê³¼ê±° ë§¥ë½ë§Œìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±"""
    if not os.path.exists(CONFIG_PATH): return False
    
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    historical_context = load_historical_contexts() 

    # ğŸ¯ [ë‰´ìŠ¤ ë¡œë“œ] DB ì§€í‘œ ë¡œì§ ì‚­ì œë¨
    news_count = config_data.get("report_news_count", 100)
    news_ctx = f"### [ ê¸ˆì¼ ìµœì‹  ë‰´ìŠ¤ {news_count}ì„  ]\n"
    if os.path.exists(PENDING_PATH):
        pending_files = sorted(os.listdir(PENDING_PATH), reverse=True)[:news_count]
        for f_name in pending_files:
            try:
                with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                    news_ctx += f"- {file.readline().strip()}\n"
            except: continue

    a_cfg = config_data.get("analyst_model", {})
    payload = {
        "model": a_cfg.get("name", "gpt-4o"),
        "messages": [
            {
                "role": "system", 
                "content": f"í˜„ì¬ì‹œê°: {now_str}\n{a_cfg.get('prompt', 'ì „ë¬¸ ì „ëµê°€ë¡œì„œ ë¶„ì„í•˜ë¼.')}\n\n{historical_context}"
            },
            {
                "role": "user", 
                "content": f"ì•„ë˜ ìµœì‹  ë‰´ìŠ¤ì˜ íë¦„ì„ ê³¼ê±° ë§¥ë½ê³¼ ê²°í•©í•˜ì—¬ ì˜¤ëŠ˜ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.\n\n{news_ctx}"
            }
        ], 
        "temperature": 0.3
    }

    try:
        url = f"{a_cfg.get('url').rstrip('/')}/chat/completions"
        headers = {"Authorization": f"Bearer {a_cfg.get('key')}"} if a_cfg.get('key') else {}
        resp = requests.post(url, json=payload, headers=headers, timeout=300)
        report_content = resp.json()['choices'][0]['message']['content']
        save_report_to_file(report_content, "daily") 
        print(f"[{now_str}] ğŸ›ï¸ ë‰´ìŠ¤ ê¸°ë°˜ ìë™ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"ğŸš¨ [ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨] ì›ì¸: {str(e)}")
        return False
        
# --- 4. ë©”ì¸ ë£¨í”„ ê°€ë™ ---

if __name__ == "__main__":
    last_news_time, last_auto_report_date = 0, datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸš€ [AI Analyst Engine] ê°€ë™ ì‹œì‘")

    while True:
        try:
            now, current_ts = datetime.now(), time.time()
            if os.path.exists(CONFIG_PATH):
                with open(CONFIG_PATH, "r", encoding="utf-8") as f: 
                    current_config = json.load(f)
            else: continue

            # ğŸ¯ [T1] ìë™ ë³´ê³ ì„œ ìƒì„± (ì§€í‘œ ë¡œë“œ ë¡œì§ ì‚­ì œ)
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            target_time_str = current_config.get("report_gen_time", "08:00")
            today_date_str = now.strftime("%Y-%m-%d")
            
            if auto_gen_enabled and now.strftime("%H:%M") == target_time_str and last_auto_report_date != today_date_str:
                if generate_auto_report(current_config): 
                    last_auto_report_date = today_date_str

            # ğŸ¯ [T2] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ì œ
            interval_sec = current_config.get("update_interval", 10) * 60
            if current_ts - last_news_time >= interval_sec:
                cleanup_old_files(current_config.get("retention_days", 7))
                
                # RSS í”¼ë“œ ìˆœíšŒ ìˆ˜ì§‘
                feeds = current_config.get("feeds", [])
                g_inc = [k.strip().lower() for k in current_config.get('global_include', "").split(",") if k.strip()]
                g_exc = [k.strip().lower() for k in current_config.get('global_exclude', "").split(",") if k.strip()]

                for feed in feeds:
                    try:
                        parsed = feedparser.parse(feed['url'])
                        l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                        l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                        for entry in parsed.entries[:50]:
                            if not check_logic(entry.title, g_inc, g_exc): continue
                            if not check_logic(entry.title, l_inc, l_exc): continue
                            save_file(entry, feed['name'])
                    except: continue
                last_news_time = current_ts
                
        except Exception as e: 
            print(f"âŒ ë£¨í”„ ì—ëŸ¬: {e}")
        time.sleep(60)






