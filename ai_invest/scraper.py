import hashlib
from common import *

processed_titles = {}  # {clean_key: timestamp} - 3ì¼ TTL ê¸°ë°˜ ì¤‘ë³µ ìºì‹œ
CACHE_TTL = 3 * 86400  # 3ì¼ (ì´ˆ)


def init_processed_cache():
    """ê¸°ì¡´ íŒŒì¼ì—ì„œ ì¤‘ë³µ ìºì‹œë¥¼ ë³µì›í•©ë‹ˆë‹¤ (ì¬ì‹œì‘ ì‹œ ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€)"""
    global processed_titles
    if not os.path.exists(PENDING_PATH):
        return
    
    current_time = time.time()
    count = 0
    
    for f in os.listdir(PENDING_PATH):
        fp = os.path.join(PENDING_PATH, f)
        if not (os.path.isfile(fp) and f.endswith(".json")):
            continue
        
        # 3ì¼ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ì€ ìºì‹œì— ë„£ì§€ ì•ŠìŒ
        mtime = os.path.getmtime(fp)
        if current_time - mtime > CACHE_TTL:
            continue
            
        try:
            with open(fp, "r", encoding="utf-8") as file:
                news_data = json.load(file)
                title = news_data.get("title", "").strip()
                pub_dt_str = news_data.get("pub_dt", "")
                if not title:
                    continue
                
                try:
                    dt_obj = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S')
                    date_key = dt_obj.strftime('%Y%m%d')
                except:
                    date_key = "unknown"
                
                clean_key = f"{date_key}_{hashlib.md5(title.encode()).hexdigest()[:12]}"
                processed_titles[clean_key] = mtime
                count += 1
        except:
            continue
    
    print(f"ğŸ”„ ì¤‘ë³µ ìºì‹œ ë³µì› ì™„ë£Œ: {count}ê°œ í•­ëª© ë¡œë“œë¨ (3ì¼ ì´ë‚´)")



def save_file(entry, feed_name):
    """ê°œì„ ëœ íƒ€ì„ë¼ì¸ ë³´ì¡´ ì €ì¥ ë°©ì‹ (JSON)"""
    global processed_titles
    
    title = entry.title.strip()
# ğŸ¯ 1. ë°œí–‰ ì‹œê°„ì„ KST(í•œêµ­ í‘œì¤€ì‹œ)ë¡œ ì—„ê²©í•˜ê²Œ ë³€í™˜ [cite: 1, 4]
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        # UTC ê¸°ë°˜ êµ¬ì¡°ì²´ ì‹œê°„ì„ KST datetime ê°ì²´ë¡œ ë³€í™˜ [cite: 3, 4]
        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc).astimezone(KST)
    else:
        # ì‹œê°„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ KST ì‹œê° ì‚¬ìš© 
        dt_obj = get_now_kst()
        
    dt_str = dt_obj.strftime('%Y%m%d_%H%M%S')# íŒŒì¼ëª… ì •ë ¬ìš©
    date_key = dt_obj.strftime('%Y%m%d')     # ì¼ë³„ ì¤‘ë³µ ë¶„ë¦¬ìš©
    pub_dt_str = dt_obj.strftime('%Y-%m-%d %H:%M:%S') # ë°ì´í„° ì €ì¥ìš©
    
    # ğŸ¯ 2. ì¤‘ë³µ ì²´í¬ í‚¤ (ë‚ ì§œ + ì œëª© MD5 í•´ì‹œ - ì¶©ëŒ ë°©ì§€)
    clean_key = f"{date_key}_{hashlib.md5(title.encode()).hexdigest()[:12]}"
    
    if clean_key in processed_titles:
        return False
    
    # ğŸ¯ 3. íŒŒì¼ëª…ì— ì‹œê°„ ì •ë³´ ì£¼ì… (ì •ë ¬ ìµœì í™”)
    file_hash = hashlib.md5(title.encode()).hexdigest()[:6]
    filename = f"{dt_str}_{file_hash}.json" # JSON í™•ì¥ì ì‚¬ìš©
    filepath = os.path.join(PENDING_PATH, filename)
    
    # ğŸ¯ 4. ë°ì´í„° êµ¬ì¡°í™” (AI ë¶„ì„ìš© ì •ë³´ í™•ì¥)
    news_data = {
        "title": title,
        "pub_dt": pub_dt_str, # [ìˆ˜ì • ì™„ë£Œ]
        "source": feed_name,
        "summary": entry.get('summary', 'ë‚´ìš© ì—†ìŒ'),
        "link": entry.get('link', '')
    }
    
    try:
        os.makedirs(PENDING_PATH, exist_ok=True)
        with open(filepath, "w", encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        processed_titles[clean_key] = time.time()
        return True
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}") # ì—ëŸ¬ ë¡œê·¸ë¥¼ ë‚¨ê²¨ì•¼ ê²½ë¡œ ë¬¸ì œë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        return False
        
def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    max_files = 600 # ìµœëŒ€ íŒŒì¼ ê°œìˆ˜ ì œí•œ
    
    # 1. íŒŒì¼ ëª©ë¡ í™•ë³´ ë° ì •ë ¬ (ì˜¤ë˜ëœ ìˆœ)
    files = []
    for f in os.listdir(PENDING_PATH):
        fp = os.path.join(PENDING_PATH, f)
        if os.path.isfile(fp) and (f.endswith(".json") or f.endswith(".txt")):
            files.append((os.path.getmtime(fp), fp))
            
    files.sort(key=lambda x: x[0]) # ì˜¤ë¦„ì°¨ìˆœ: ì˜¤ë˜ëœ íŒŒì¼ -> ìµœì‹  íŒŒì¼
    
    # 2. ì‚­ì œ ìˆ˜í–‰
    total_cnt = len(files)
    for i, (mtime, fp) in enumerate(files):
        # ì‚­ì œ ì¡°ê±´: ê¸°ê°„ ë§Œë£Œ OR ê°œìˆ˜ ì´ˆê³¼ (ë‚¨ì€ íŒŒì¼ì´ 1500ê°œë³´ë‹¤ ë§ìœ¼ë©´ ì‚­ì œ)
        if (current_time - mtime > seconds_threshold) or ((total_cnt - i) > max_files):
            try:
                os.remove(fp)
                deleted_count += 1
            except: pass
        else:
            break # ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´í›„ íŒŒì¼ì€ ì•ˆì „
    
    # ë§Œë£Œëœ ìºì‹œ í•­ëª©ë§Œ ì„ íƒì  ì œê±° (3ì¼ TTL)
    expired_keys = [k for k, t in processed_titles.items() if current_time - t > CACHE_TTL]
    for k in expired_keys:
        del processed_titles[k]
    if deleted_count > 0 or expired_keys:
        print(f"ğŸ§¹ íŒŒì¼ {deleted_count}ê°œ ì •ë¦¬, ë§Œë£Œ ìºì‹œ {len(expired_keys)}ê°œ ì œê±° (ìºì‹œ ì”ì—¬: {len(processed_titles)}ê°œ)")


def generate_auto_report(config_data, r_type):
    """ìë™ ë³´ê³ ì„œ ìƒì„± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    # 0. ë°ì´í„° ìµœì‹ í™”: ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì‹œì¥ ë°ì´í„° ê°±ì‹  (ë§ˆì¼“ ì˜¤í”ˆ/í´ë¡œì¦ˆ íŒë³„)
    print(f"ğŸ”„ [Auto] ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì‹œì¥ ë°ì´í„° ê°±ì‹  ì ê²€...")
    try:
        if is_kr_market_open():
            get_krx_summary_raw(ignore_cache=True)
        
        if is_us_market_open():
            get_global_financials_raw(ignore_cache=True, fetch_type="all")
        else:
            get_global_financials_raw(ignore_cache=True, fetch_type="non_equities")
            
        get_fed_liquidity_raw()
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ê°±ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©): {e}")

    # 1. ë°ì´í„° ì¤€ë¹„ (common.py í™œìš©)
    input_content, label = prepare_report_data(r_type, config_data)
    
    if not input_content:
        print(f"âš ï¸ [Auto] ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False

    print(f"ğŸ¤– [Auto] {label} ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
    
    # 2. AI ìƒì„± (common.py í™œìš©)
    report_content = generate_invest_report(r_type, input_content, config_data)
    
    if report_content and "âŒ" not in report_content:
        # 3. ì €ì¥
        save_path = save_report_to_file(report_content, r_type)
        print(f"âœ¨ [Auto] {label} ìƒì„± ì™„ë£Œ! ì €ì¥ë¨: {save_path}")
        return True
    else:
        print(f"ğŸš¨ [Auto] ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {report_content}")
        return False

# --- [ 3. ë©”ì¸ ë£¨í”„ (ìˆ˜ë™ ì‘ì—…ì— ë°©í•´ë°›ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¤„ëŸ¬) ] ---

if __name__ == "__main__":
    # ğŸ’¡ ìë™í™”(Auto) ì „ìš© ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜ (ìˆ˜ë™ ì‹¤í–‰ ì‹œ ì´ ë³€ìˆ˜ë“¤ì„ ê±´ë“œë¦¬ì§€ ì•Šìœ¼ë©´ ìë™ ì‹¤í–‰ë¨)
    auto_daily_done_date = ""
    auto_weekly_done_week = ""
    auto_monthly_done_month = ""
    
    last_news_time = 0
    first_run = True
    _config_mtime = 0  # ì„¤ì • íŒŒì¼ ë³€ê²½ ê°ì§€ìš©
    _cached_config = None
    _cached_base_time = None  # ì˜ˆì•½ ì‹œê° ìºì‹œ
    _cached_weekly_time = None
    _cached_monthly_time = None

    def _load_config_if_changed():
        """ì„¤ì • íŒŒì¼ì´ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤ (ë””ìŠ¤í¬ I/O ìµœì†Œí™”)"""
        nonlocal _config_mtime, _cached_config, _cached_base_time, _cached_weekly_time, _cached_monthly_time
        try:
            mt = os.path.getmtime(CONFIG_PATH) if os.path.exists(CONFIG_PATH) else 0
        except:
            mt = 0
        if mt != _config_mtime or _cached_config is None:
            _config_mtime = mt
            _cached_config = load_data()
            # ì˜ˆì•½ ì‹œê°ë„ ì„¤ì • ë³€ê²½ ì‹œì—ë§Œ ì¬ê³„ì‚°
            base_time_str = str(_cached_config.get("report_gen_time", "08:00")).strip()
            base_dt = datetime.strptime(base_time_str, "%H:%M")
            _cached_base_time = base_time_str
            _cached_weekly_time = (base_dt + timedelta(minutes=10)).strftime("%H:%M")
            _cached_monthly_time = (base_dt + timedelta(minutes=20)).strftime("%H:%M")
        return _cached_config

    try:
        init_config = _load_config_if_changed()
        print(f"ğŸš€ [AI Analyst] ì‹œìŠ¤í…œ ê°€ë™ - ê¸°ì¤€ ì‹œê°: {init_config.get('report_gen_time', '08:00')} (KST)")
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

    init_processed_cache()

    while True:
        try:
            now_kst = get_now_kst()
            current_ts = time.time()
            current_config = _load_config_if_changed()
            
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            current_time_str = now_kst.strftime("%H:%M")
            
            # ì˜ˆì•½ ì‹œê° (ì„¤ì • ë³€ê²½ ì‹œì—ë§Œ ì¬ê³„ì‚°ë¨)
            base_time_str = _cached_base_time
            weekly_time_str = _cached_weekly_time
            monthly_time_str = _cached_monthly_time

            # --- [ ğŸ¤– ìë™ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ ] ---
            if auto_gen_enabled:
                
                # â‘  ì¼ê°„ ìë™ ë³´ê³ ì„œ
                if current_time_str == base_time_str:
                    today_str = now_kst.strftime("%Y-%m-%d")
                    # ğŸ’¡ ìˆ˜ë™ ë³´ê³ ì„œ íŒŒì¼ì´ ìˆì–´ë„, 'ìë™ ìŠ¤ì¼€ì¤„ëŸ¬'ê°€ ì˜¤ëŠ˜ ì²˜ìŒì´ë¼ë©´ ì‹¤í–‰í•©ë‹ˆë‹¤.
                    if auto_daily_done_date != today_str:
                        print(f"ğŸ¤– [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "daily"):
                            auto_daily_done_date = today_str # ìë™ ì‹¤í–‰ ì„±ê³µ ì‹œì—ë§Œ ë§ˆí‚¹

                # â‘¡ ì£¼ê°„ ìë™ ë³´ê³ ì„œ (ì¼ìš”ì¼)
                elif current_time_str == weekly_time_str and now_kst.weekday() == 6:
                    week_str = now_kst.strftime("%Y-%U")
                    if auto_weekly_done_week != week_str:
                        print(f"ğŸ“… [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì£¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "weekly"):
                            auto_weekly_done_week = week_str

                # â‘¢ ì›”ê°„ ìë™ ë³´ê³ ì„œ (1ì¼)
                elif current_time_str == monthly_time_str and now_kst.day == 1:
                    month_str = now_kst.strftime("%Y-%m")
                    if auto_monthly_done_month != month_str:
                        print(f"ğŸ›ï¸ [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì›”ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "monthly"):
                            auto_monthly_done_month = month_str
            # --- [ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¹ì…˜ ] ---
            update_interval_min = current_config.get("update_interval", 10)
            update_interval_sec = update_interval_min * 60
            
            # ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (ë¡œê·¸ìš©)
            time_since_last = current_ts - last_news_time
            next_in = max(0, update_interval_sec - time_since_last)

            if time_since_last >= update_interval_sec or first_run:
                print(f"ğŸ“¡ [{now_kst.strftime('%H:%M:%S')}] ë‰´ìŠ¤/ë³„ë„ì§€í‘œ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ (ì£¼ê¸°: {update_interval_min}ë¶„)")
                
                # ğŸ¯ [NEW] ì‹œì¥ ë°ì´í„°(KRX, Global, Fed) ê¸°ë™ ì‹œê°„ / íœ´ì¼ íŒë³„ ìë™ ìˆ˜ì§‘
                need_krx = first_run or is_kr_market_open()
                need_us = first_run or is_us_market_open()

                print(f"ğŸ“Š [{now_kst.strftime('%H:%M:%S')}] ì‹œì¥ ë°ì´í„° ê°±ì‹  ì ê²€ (ì²«ì‹¤í–‰: {first_run}, KRXìˆ˜ì§‘: {need_krx}, USìˆ˜ì§‘: {need_us})...")
                try:
                    if need_krx:
                        get_krx_summary_raw(ignore_cache=True)
                    
                    if need_us:
                        get_global_financials_raw(ignore_cache=True, fetch_type="all") # ì£¼ì‹ í¬í•¨ ì „ì²´
                    else:
                        get_global_financials_raw(ignore_cache=True, fetch_type="non_equities") # í™˜ìœ¨/ì›ìì¬ë§Œ
                    
                    get_fed_liquidity_raw()     # Fed (FRED)
                except Exception as e:
                    print(f"âš ï¸ ì‹œì¥ ë°ì´í„° ìë™ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

                feeds = current_config.get("feeds", [])
                g_exc_str = current_config.get('global_exclude', "")  # ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ê°€ì ¸ì˜´
                
                new_saved = 0
                for feed in feeds:
                    try:
                        parsed = feedparser.parse(feed['url'])
                        
                        feed_new = 0
                        for entry in parsed.entries[:50]:
                            if not check_news_filter(entry.title, g_exc_str):
                                continue
                            if save_file(entry, feed['name']):
                                feed_new += 1
                                new_saved += 1
                        if feed_new > 0:
                            print(f"   â””â”€ {feed['name']}: {feed_new}ê°œ ì‹ ê·œ ì €ì¥")
                    except Exception as e:
                        print(f"   â””â”€ âŒ {feed.get('name')} ì˜¤ë¥˜: {e}")
                
                print(f"âœ… [{now_kst.strftime('%H:%M:%S')}] ìˆ˜ì§‘ ì™„ë£Œ (ì´ {new_saved}ê°œ ì‹ ê·œ í™•ë³´)")
                
                # íŒŒì¼ ì •ë¦¬ (ê¸°ê°„ ë§Œë£Œ ë° ê°œìˆ˜ ì´ˆê³¼ ì‚­ì œ)
                cleanup_old_files(min(current_config.get("retention_days", 3), 3))
                
                last_news_time = current_ts
                first_run = False
            else:
                # ë§¤ ë¶„ë§ˆë‹¤ ì •ê¸° ìƒì¡´ ì‹ ê³  ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
                if now_kst.minute % 5 == 0: # 5ë¶„ë§ˆë‹¤ ì¶œë ¥
                    print(f"ğŸ’¤ [{now_kst.strftime('%H:%M:%S')}] ëŒ€ê¸° ì¤‘... (ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ê¹Œì§€ {int(next_in/60)}ë¶„ ë‚¨ìŒ)")

        except Exception as e: 
            print(f"ğŸš¨ [{datetime.now().strftime('%H:%M:%S')}] ë£¨í”„ ì¹˜ëª…ì  ì—ëŸ¬: {e}")
            
        time.sleep(60)
