import feedparser
import time
import os
import json
import hashlib

# --- ê²½ë¡œ ì„¤ì • ---
CONFIG_PATH = "/share/ai_analyst/rss_config.json"
SAVE_PATH = "/share/ai_analyst/pending"

def get_file_hash(text):
    """ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def save_file(entry, feed_name):
    os.makedirs(SAVE_PATH, exist_ok=True)
    # ì œëª© ê¸°ë°˜ í•´ì‹œë¡œ íŒŒì¼ëª… ìƒì„±
    title_hash = hashlib.md5(entry.title.encode('utf-8')).hexdigest()
    fname = f"{SAVE_PATH}/{title_hash}.txt"
    
    if os.path.exists(fname): return

    # ğŸ’¡ RSS ë‚ ì§œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„(2026-02-02)ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    pub_date = entry.get('published') 
    if not pub_date:
        pub_date = datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")

    try:
        with open(fname, "w", encoding="utf-8") as f:
            # âš ï¸ ì•„ë˜ ìˆœì„œë¥¼ ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš” (app.pyì˜ load_pending_filesì™€ ì§ê²°ë¨)
            f.write(f"ì œëª©: {entry.title}\n")
            f.write(f"ë§í¬: {entry.link}\n")
            f.write(f"ë‚ ì§œ: {pub_date}\n") # ğŸ’¡ 3ë²ˆì§¸ ì¤„ì— ë‚ ì§œ ê¸°ë¡
            f.write(f"ìš”ì•½: {entry.get('summary', 'ë‚´ìš© ì—†ìŒ')}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")



def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    # 1. ì œì™¸ í•„í„° (í•˜ë‚˜ë¼ë„ ê±¸ë¦¬ë©´ ë°”ë¡œ íƒˆë½)
    if any(x in text for x in exc_list if x):
        return False
    # 2. í¬í•¨ í•„í„° (ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì²´í¬)
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ë³´ê´€ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    if not os.path.exists(SAVE_PATH):
        return
        
    current_time = time.time()
    # 1ì¼ = 86400ì´ˆ
    seconds_threshold = retention_days * 86400
    
    deleted_count = 0
    for filename in os.listdir(SAVE_PATH):
        file_path = os.path.join(SAVE_PATH, filename)
        
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì²´í¬
        if os.path.isfile(file_path) and filename.endswith(".txt"):
            file_age = os.path.getmtime(file_path)
            if (current_time - file_age) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({filename}): {e}")
                    
    if deleted_count > 0:
        print(f"ğŸ§¹ ë³´ê´€ ê¸°ê°„ ë§Œë£Œë¡œ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. (ê¸°ì¤€: {retention_days}ì¼)")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (ì „ì—­ í•„í„°ë§ ë° ìë™ ì‚­ì œ ì‹œìŠ¤í…œ)...")
    
    while True:
        # 1. ì„¤ì • ë° ì „ì—­ í•„í„° ë¡œë“œ (ê¸°ë³¸ ë³€ìˆ˜ëª… config ìœ ì§€)
        config = {"feeds": [], "update_interval": 10, "global_include": "", "global_exclude": "", "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except Exception as e:
                print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        interval = config.get("update_interval", 10)
        retention_days = config.get("retention_days", 7)
        
        # [ìë™ ì‚­ì œ ë¡œì§ ì‹¤í–‰]
        cleanup_old_files(retention_days)
        
        # 2. ì „ì—­ í•„í„° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸í™”
        g_inc = [k.strip().lower() for k in config.get('global_include', "").split(",") if k.strip()]
        g_exc = [k.strip().lower() for k in config.get('global_exclude', "").split(",") if k.strip()]

        feeds = config.get("feeds", [])
        if not feeds:
            time.sleep(60); continue

        # 3. ê°œë³„ í”¼ë“œ ìˆœíšŒ ë° ìˆ˜ì§‘
        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                
                for entry in parsed.entries[:50]:
                    # ì œëª© ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
                    check_text = entry.title
                    
                    if not check_logic(check_text, g_inc, g_exc): continue
                    if not check_logic(check_text, l_inc, l_exc): continue
                    
                    save_file(entry, feed['name'])
            except: continue
        
        print(f"ğŸ’¤ {interval}ë¶„ í›„ ì—…ë°ì´íŠ¸ í™•ì¸ ë° íŒŒì¼ ì •ë¦¬ ì˜ˆì •...")
        time.sleep(interval * 60)

if __name__ == "__main__":
    start_scraping()
