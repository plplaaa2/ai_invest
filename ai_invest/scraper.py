import hashlib
from common import *
from collections import deque
from difflib import SequenceMatcher

# ğŸ¯ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ìºì‹œ
processed_titles = deque(maxlen=500)
SIMILARITY_THRESHOLD = 0.85 # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼

def is_similar(a, b):
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return SequenceMatcher(None, a, b).ratio()

def load_recent_titles():
    """ë””ìŠ¤í¬ì— ìˆëŠ” ìµœì‹  ë‰´ìŠ¤ ì œëª©ë“¤ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ ì¬ì‹œì‘ ì‹œì—ë„ ì¤‘ë³µ ë°©ì§€"""
    global processed_titles
    processed_titles.clear()
    
    if not os.path.exists(PENDING_PATH): return

    # ìµœì‹  íŒŒì¼ ìˆœìœ¼ë¡œ ì •ë ¬ (JSONë§Œ)
    files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
    
    count = 0
    for filename in files:
        if count >= processed_titles.maxlen: break
        try:
            with open(os.path.join(PENDING_PATH, filename), 'r', encoding='utf-8') as f:
                data = json.load(f)
                t = data.get('title', '')
                if t:
                    # ì €ì¥ ë¡œì§ê³¼ ë™ì¼í•œ ì •ê·œí™”
                    normalized = ''.join(filter(str.isalnum, t)).lower()
                    processed_titles.append(normalized)
                    count += 1
        except: continue
    
    if count > 0:
        print(f"ğŸ“‚ [Init] ê¸°ì¡´ ë‰´ìŠ¤ {count}ê±´ì„ ì¤‘ë³µ ë°©ì§€ ìºì‹œì— ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.")

def log_duplicate(title, feed_name):
    """ì¤‘ë³µëœ ë‰´ìŠ¤ ì œëª©ì„ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        log_path = os.path.join(PENDING_PATH, "duplicate_news.log")
        now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"[{now_str}] [{feed_name}] {title}\n")
    except: pass

def save_file(entry, feed_name, current_saved, current_total):
    """KST ì—­ìˆœ ì •ë ¬ íŒŒì¼ëª… ìƒì„± ë° ìƒì„¸ ë¡œê·¸ ê¸°ë¡ ë²„ì „"""
    global processed_titles
    try:
        title = entry.title.strip()

        # ğŸ¯ 1. ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
        # ê°•ë ¥í•œ ì •ê·œí™” (íŠ¹ìˆ˜ë¬¸ì/ê³µë°± ì œê±°, ì†Œë¬¸ìí™”)
        normalized_title = ''.join(filter(str.isalnum, title)).lower()

        for old_title in processed_titles:
            if is_similar(normalized_title, old_title) > SIMILARITY_THRESHOLD:
                # print(f"âš ï¸  ìœ ì‚¬ë„ ì¤‘ë³µ ë‰´ìŠ¤ ê±´ë„ˆë›°ê¸°: {title}") # ë””ë²„ê¹… í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
                log_duplicate(title, feed_name)
                return False

        # ğŸ¯ 2. ë°œí–‰ ì‹œê°„ íŒŒì‹± ë° KST ë³€í™˜
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc).astimezone(KST)
        else:
            dt_obj = get_now_kst()

        # ğŸ¯ 3. íŒŒì¼ëª…ìš© ì •ë ¬ ë¬¸ìì—´ (YYYYMMDD_HHMMSS)
        dt_str = dt_obj.strftime('%Y%m%d_%H%M%S')

        # ğŸ¯ 4. ê³ ìœ  íŒŒì¼ëª… ê²°ì • (ì‹œê°„ ì •ë³´ë¥¼ ë§¨ ì•ìœ¼ë¡œ)
        file_hash = hashlib.md5(title.encode()).hexdigest()[:6]
        filename = f"{dt_str}_{file_hash}.json"
        filepath = os.path.join(PENDING_PATH, filename)

        news_data = {
            "title": title,
            "pub_dt": dt_obj.strftime('%Y-%m-%d %H:%M:%S'), # KST ë¬¸ìì—´ ì €ì¥
            "source": feed_name,
            "summary": entry.get('summary', 'ë‚´ìš© ì—†ìŒ'),
            "link": entry.get('link', '')
        }

        with open(filepath, "w", encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)

        # ğŸ¯ 5. ì²˜ë¦¬ê°€ ëë‚œ ì œëª©ì„ ìºì‹œì— ì¶”ê°€ (ì •ê·œí™”ëœ ë²„ì „)
        processed_titles.append(normalized_title)
        return True
    except Exception as e:
        print(f"âŒ [Scraper] ì €ì¥ ì—ëŸ¬ ({feed_name}): {e}")
        return False
        
        
def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    log_cleaned = False
    
    for filename in os.listdir(PENDING_PATH):
        file_path = os.path.join(PENDING_PATH, filename)
        if not os.path.isfile(file_path): continue

        # ì˜¤ë˜ëœ íŒŒì¼ì¸ì§€ í™•ì¸
        is_old = (current_time - os.path.getmtime(file_path)) > seconds_threshold

        if is_old:
            # 1. ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼(.json) ì •ë¦¬ (ê¸°ì¡´ .txt ì˜¤ë¥˜ ìˆ˜ì •)
            if filename.endswith(".json"):
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: pass
            # 2. ì˜¤ë˜ëœ ì¤‘ë³µ ë¡œê·¸ íŒŒì¼(.log) ì •ë¦¬
            elif filename == "duplicate_news.log":
                try:
                    os.remove(file_path)
                    log_cleaned = True
                except: pass
    
    # ìºì‹œê°€ ë¹„ì›Œì§€ë©´ ì¤‘ë³µ ì €ì¥ì´ ë°œìƒí•˜ë¯€ë¡œ, íŒŒì¼ ì •ë¦¬ í›„ ìºì‹œë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.
    load_recent_titles()
    
    if deleted_count > 0:
        print(f"ğŸ§¹ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼(.json)ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    if log_cleaned:
        print(f"ğŸ§¹ ì˜¤ë˜ëœ ì¤‘ë³µ ë‰´ìŠ¤ ë¡œê·¸(duplicate_news.log)ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (íƒ€ì„ë¼ì¸ ë³´ì¡´ ë° ë™ì  ì¤‘ë³µ ì œê±°)...")
    
    # ì‹œì‘ ì‹œ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ
    load_recent_titles()
    
    while True:
        # 1. ì„¤ì • ë° í•„í„°ë§ í‚¤ì›Œë“œ ë¡œë“œ
        config = {"feeds": [], "update_interval": 10, "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except: pass
        
        interval = config.get("update_interval", 10)
        cleanup_old_files(config.get("retention_days", 7))
        
        g_inc = config.get('global_include', "")
        g_exc = config.get('global_exclude', "")

        # 2. í”¼ë“œ ìˆœíšŒ
        feeds = config.get("feeds", [])
        total_found, new_saved = 0, 0

        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                l_inc = feed.get('include', "")
                l_exc = feed.get('exclude', "")

                # ìƒìœ„ 50ê°œ ë‰´ìŠ¤ í™•ì¸
                feed_new = 0
                for entry in parsed.entries[:50]:
                    total_found += 1 # ë°œê²¬ ìˆ˜ ì¹´ìš´íŠ¸ ì—…
                    
                    if not is_filtered(entry.title, g_inc, g_exc, l_inc, l_exc): continue
                    
                    # ğŸ¯ [í•µì‹¬ ìˆ˜ì •] save_file í˜¸ì¶œ ì‹œ í˜„ì¬ ì¹´ìš´íŠ¸ ì •ë³´ë¥¼ í•¨ê»˜ ì „ë‹¬í•©ë‹ˆë‹¤.
                    # new_saved + 1 ì€ 'ì´ë²ˆì— ì €ì¥ë  ë‰´ìŠ¤ê°€ ëª‡ ë²ˆì§¸ì¸ì§€'ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
                      # âœ… save_file ì •ì˜ì™€ ì¸ì ê°œìˆ˜ ë§ì¶¤
                    if save_file(entry, feed['name'], new_saved + 1, total_found):
                        feed_new += 1
                        new_saved += 1
                        
                if feed_new > 0:
                    print(f"   â””â”€ {feed['name']}: {feed_new}ê°œ ì‹ ê·œ í™•ë³´")
                    
            except Exception as e:
                print(f"âŒ {feed.get('name')} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")                
                continue
        
        # 3. ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ í›„ ìš”ì•½ ë¡œê·¸
        now_str = datetime.now().strftime('%H:%M:%S')
        if total_found > 0:
            print(f"[{now_str}] ğŸ“Š ì‚¬ì´í´ ì¢…ë£Œ: ë°œê²¬ {total_found}ê°œ | ì‹ ê·œ ì €ì¥ {new_saved}ê°œ")
        
        time.sleep(interval * 60)

# --- [5. ë©”ì¸ ë£¨í”„] ---
if __name__ == "__main__":
    start_scraping()