import hashlib
from common import *

processed_titles = set()

def save_file(entry, feed_name):
    """ê°œì„ ëœ íƒ€ì„ë¼ì¸ ë³´ì¡´ ì €ì¥ ë°©ì‹ (JSON)"""
    global processed_titles
    
    title = entry.title.strip()
# ğŸ¯ 1. ë°œí–‰ ì‹œê°„ì„ KST(í•œêµ­ í‘œì¤€ì‹œ)ë¡œ ì—„ê²©í•˜ê²Œ ë³€í™˜ [cite: 1, 4]
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        # UTC ê¸°ë°˜ êµ¬ì¡°ì²´ ì‹œê°„ì„ KST datetime ê°ì²´ë¡œ ë³€í™˜ [cite: 3, 4]
        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc).astimezone(KST)
    else:
        # ì‹œê°„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ KST ì‹œê° ì‚¬ìš© 
        dt_obj = get_now_kst()
        
    dt_str = dt_obj.strftime('%Y%m%d_%H%M%S')# íŒŒì¼ëª… ì •ë ¬ìš©
    date_key = dt_obj.strftime('%Y%m%d')     # ì¼ë³„ ì¤‘ë³µ ë¶„ë¦¬ìš©
    pub_dt_str = dt_obj.strftime('%Y-%m-%d %H:%M:%S') # ë°ì´í„° ì €ì¥ìš©
    
    # ğŸ¯ 2. ì¤‘ë³µ ì²´í¬ í‚¤ ê°•í™” (ë‚ ì§œ + ì œëª© 15ì)
    # ì´ì œ ë‚ ì§œê°€ ë‹¤ë¥´ë©´ ê°™ì€ ì œëª©ì´ë¼ë„ ë³„ê°œ ë‰´ìŠ¤ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    clean_key = f"{date_key}_{title.replace(' ', '')[:15]}"
    
    if clean_key in processed_titles:
        return False
    
    # ğŸ¯ 3. íŒŒì¼ëª…ì— ì‹œê°„ ì •ë³´ ì£¼ì… (ì •ë ¬ ìµœì í™”)
    file_hash = hashlib.md5(title.encode()).hexdigest()[:6]
    filename = f"{dt_str}_{file_hash}.json" # JSON í™•ì¥ì ì‚¬ìš©
    filepath = os.path.join(PENDING_PATH, filename)
    
    # ğŸ¯ 4. ë°ì´í„° êµ¬ì¡°í™” (AI ë¶„ì„ìš© ì •ë³´ í™•ì¥)
    news_data = {
        "title": title,
        "pub_dt": pub_dt_str, # [ìˆ˜ì • ì™„ë£Œ]
        "source": feed_name,
        "summary": entry.get('summary', 'ë‚´ìš© ì—†ìŒ'),
        "link": entry.get('link', '')
    }
    
    try:
        with open(filepath, "w", encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        processed_titles.add(clean_key)
        return True
    except:
        return False
        
def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    if any(x in text for x in exc_list if x):
        return False
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    
    for filename in os.listdir(PENDING_PATH):
        file_path = os.path.join(PENDING_PATH, filename)
        if os.path.isfile(file_path) and (filename.endswith(".json") or filename.endswith(".txt")):
            if (current_time - os.path.getmtime(file_path)) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: pass
    
    # íŒŒì¼ ì‚­ì œ ì‹œ ë©”ëª¨ë¦¬ ìºì‹œë„ í•¨ê»˜ ë¹„ì›Œ ì‹œìŠ¤í…œì„ ê°€ë³ê²Œ ìœ ì§€
    processed_titles.clear()
    if deleted_count > 0:
        print(f"ğŸ§¹ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì •ë¦¬í•˜ê³  ì¤‘ë³µ í•„í„°ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (íƒ€ì„ë¼ì¸ ë³´ì¡´ ë° ë™ì  ì¤‘ë³µ ì œê±°)...")
    
    while True:
        # 1. ì„¤ì • ë° í•„í„°ë§ í‚¤ì›Œë“œ ë¡œë“œ
        config = {"feeds": [], "update_interval": 10, "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except: pass
        
        interval = config.get("update_interval", 10)
        cleanup_old_files(config.get("retention_days", 7))
        
        # ğŸ¯ ë©”ëª¨ë¦¬ ìºì‹œ(processed_titles)ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ ì£¼ê¸°ì ìœ¼ë¡œ ë¹„ì›Œì£¼ê±°ë‚˜ 
        # ìµœê·¼ Nê°œë§Œ ìœ ì§€í•˜ëŠ” ë¡œì§ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” ì‹¤í–‰ ì‹œ ìœ ì§€)
        
        g_inc = [k.strip().lower() for k in config.get('global_include', "").split(",") if k.strip()]
        g_exc = [k.strip().lower() for k in config.get('global_exclude', "").split(",") if k.strip()]

        # 2. í”¼ë“œ ìˆœíšŒ
        feeds = config.get("feeds", [])
        total_found, new_saved = 0, 0

        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                # í”¼ë“œë³„ ê°œë³„ í•„í„°
                l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                
                # ìƒìœ„ 50ê°œ ë‰´ìŠ¤ í™•ì¸
                for entry in parsed.entries[:50]:
                    total_found += 1
                    # ì „ì—­/ê°œë³„ í•„í„°ë§ ë¡œì§ (check_logic í•¨ìˆ˜ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                    if not check_logic(entry.title, g_inc, g_exc): continue
                    if not check_logic(entry.title, l_inc, l_exc): continue
                    
                    if save_file(entry, feed['name']):
                        new_saved += 1
            except Exception as e:
                print(f"âŒ {feed.get('name')} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
                continue
        
        # 3. ì‹¤ì‹œê°„ ë³´ê³  ë¡œê·¸
        now_str = datetime.now().strftime('%H:%M:%S')
        if total_found > 0:
            print(f"[{now_str}] ğŸ“Š ë°œê²¬ {total_found}ê°œ | ì‹ ê·œ {new_saved}ê°œ | í•„í„°/ì¤‘ë³µ ì œì™¸ {total_found - new_saved}ê°œ")
        
        # ğŸ’¤ ìˆ˜ì§‘ ì£¼ê¸°ëŠ” ìœ ë™ì ìœ¼ë¡œ (ê¸°ë³¸ 10ë¶„)
        time.sleep(interval * 60)

def generate_auto_report(config_data, r_type="daily"):
    """
    [í†µí•© ë³´ê³ ì„œ ì—”ì§„] 
    - ì¼ê°„(daily): 7ì¼ ì§€í‘œ + ë‹¹ì¼ ë‰´ìŠ¤ ë¶„ì„
    - ì£¼ê°„(weekly): 30ì¼ ì§€í‘œ + ì§€ë‚œ 7ì¼ì¹˜ ë¦¬í¬íŠ¸ ìš”ì•½
    - ì›”ê°„(monthly): 365ì¼ ì§€í‘œ + ì§€ë‚œ 30ì¼ì¹˜ ë¦¬í¬íŠ¸ ìš”ì•½
    """
    # ğŸ¯ 0. ê¸°ì´ˆ ë°ì´í„° ë° ì•ˆì „ì¥ì¹˜ í™•ì¸
    if not os.path.exists(CONFIG_PATH):
        print(f"â³ [ëŒ€ê¸°] ì„¤ì • íŒŒì¼({CONFIG_PATH})ì´ ì—†ìŠµë‹ˆë‹¤. UIì—ì„œ ì„¤ì •ì„ ì €ì¥í•´ì£¼ì„¸ìš”.")
        return False

    now_kst = get_now_kst()
    now_str = now_kst.strftime("%Y-%m-%d %H:%M")
    historical_context = load_historical_contexts()

    # ğŸ¯ 1. ë¦¬í¬íŠ¸ íƒ€ì…ë³„ ì§€í‘œ ì¡°íšŒ ê¸°ê°„ ì„¤ì • [ì‚¬ë ¹ê´€ë‹˜ ì§€ì¹¨ ë°˜ì˜]
    lookback_map = {"daily": 7, "weekly": 30, "monthly": 365}
    lookback_days = lookback_map.get(r_type, 30)
    
    
    # ğŸ¯ 2. ì…ë ¥ ë°ì´í„° êµ¬ì„± (ì¼ê°„ ë‰´ìŠ¤ vs ì£¼ê°„/ì›”ê°„ ê³¼ê±° ë¦¬í¬íŠ¸)
    if r_type == "daily":
        # --- [ê¸°ì¡´ ë‰´ìŠ¤ ì •ì œ ë¡œì§] ---
        news_count = config_data.get("report_news_count", 100)
        raw_news_list = []
        if os.path.exists(PENDING_PATH):
            files = sorted(os.listdir(PENDING_PATH), reverse=True)
            seen_keys = set()
            for f_name in files:
                with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                    title = file.readline().replace("ì œëª©:", "").strip()
                    # ì œëª© 18ì ê¸°ë°˜ ì¤‘ë³µ ì œê±° ë¡œì§
                    clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
                    if clean_key not in seen_keys:
                        seen_keys.add(clean_key)
                        raw_news_list.append(title)
                    if len(raw_news_list) >= news_count: break

        news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n"
        news_ctx += "\n".join([f"- {t}" for t in raw_news_list])
        input_content = f"{news_ctx}\n"
        report_label = "ì¼ê°„(Daily)"

    else:
        # --- [ì£¼ê°„/ì›”ê°„ ì „ìš© ê³¼ê±° ë¦¬í¬íŠ¸ ìš”ì•½ ë¡œì§] ---
        daily_dir = "/share/ai_analyst/reports/01_daily"
        files = sorted([f for f in os.listdir(daily_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        
        # ì£¼ê°„ì€ 7ê°œ, ì›”ê°„ì€ 30ê°œ íŒŒì¼ ì°¸ì¡°
        target_count = 7 if r_type == "weekly" else 30
        report_summary = f"### [ ì§€ë‚œ {target_count}ì¼ê°„ì˜ ë¶„ì„ ê¸°ë¡ ìš”ì•½ ]\n"
        
        for f_name in files[:target_count]:
            with open(os.path.join(daily_dir, f_name), 'r', encoding='utf-8') as f:
                # ê° ì¼ê°„ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ 500ì ë°œì·Œ
                report_summary += f"\n- {f_name}: {f.read()[:500]}...\n"
        
        input_content = f"{report_summary}\n"
        report_label = "ì£¼ê°„(Weekly)" if r_type == "weekly" else "ì›”ê°„(Monthly)"

    # ğŸ¯ 3. í•˜ì´ë¸Œë¦¬ë“œ AI ì„¤ì • (UI í”„ë¡¬í”„íŠ¸ ë§¤ì¹­)
    a_cfg = config_data.get("analyst_model", {})
    base_url = a_cfg.get("url", "").rstrip('/')
    model_name = a_cfg.get("name")
    base_prompt = a_cfg.get("prompt", "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤.")
    # r_typeë³„ ì „ìš© í”„ë¡¬í”„íŠ¸ ìš°ì„  ì‹œë„, ì—†ìœ¼ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    final_prompt = f"í˜„ì¬ ì„ë¬´: {report_label} íˆ¬ì ì „ëµ ë³´ê³ ì„œ ì‘ì„±\n\n{base_prompt}"
    
    oa_key = config.get("openai_api_key", "")
    gm_key = config.get("gemini_api_key", "")

    # ğŸ¯ 4. í˜ì´ë¡œë“œ êµ¬ì„± ë° í˜¸ì¶œ
    if "googleapis.com" in base_url or "gemini" in model_name.lower():
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={gm_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{
                "parts": [{"text": f"ì§€ì¹¨: {final_prompt}\n\nê³¼ê±°ë§¥ë½: {historical_context}\në°ì´í„°:\n{input_content}"}]
            }]
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if oa_key and "gpt" in model_name.lower():
            headers["Authorization"] = f"Bearer {oa_key}"
            
        payload = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": f"ì‹œê°: {now_str}\n{final_prompt}\n{historical_context}"},
                {"role": "user", "content": input_content}
            ],
            "temperature": a_cfg.get("temperature", 0.3)
        }

    # ğŸ¯ 5. ì‹¤í–‰ ë° ê³„ì¸µí˜• ì €ì¥ (Purge ìë™ ì—°ë™)
    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=300)
        resp.raise_for_status()
        result = resp.json()
        
        report_content = result['candidates'][0]['content']['parts'][0]['text'] if "candidates" in result else result['choices'][0]['message']['content']
        
        # ì‚¬ë ¹ê´€ë‹˜ì˜ save_report_to_fileì„ í†µí•´ í´ë” ë¶„ë¥˜ ë° í¼ì§€ ì‹¤í–‰
        save_report_to_file(report_content, r_type)
        print(f"[{now_str}] ğŸ›ï¸ {r_type.upper()} ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì§€í‘œê¸°ê°„: {lookback_days}ì¼)")
        return True
    except Exception as e:
        print(f"ğŸš¨ [{r_type}] ìƒì„± ì¤‘ë‹¨ ì›ì¸: {str(e)}")
        return False

## --- [5. ë©”ì¸ ë£¨í”„] ---
if __name__ == "__main__":
    last_prices = {} 
    last_collect_time = 0
    last_news_time = 0
    last_fred_time = 0 
    last_auto_report_date = ""
    last_weekly_report_date = "" 
    last_monthly_report_date = ""

    print(f"ğŸš€ [AI Analyst] ì‹œìŠ¤í…œ ê°€ë™ - ê¸°ì¤€ ì‹œê°: {data.get('report_gen_time', '08:00')} (KST)")

    while True:
        try:
            now_kst = get_now_kst()
            current_ts = time.time()
            current_config = load_data() 
            
            # ğŸ•’ ì‹¤í–‰ ì‹œê° ì„¤ì • ë° ê³„ì‚°
            base_time_str = str(current_config.get("report_gen_time", "08:00")).strip()
            base_time = datetime.strptime(base_time_str, "%H:%M")
            
            # 10ë¶„, 20ë¶„ ê°„ê²© ìˆœì°¨ ì‹¤í–‰ ì‹œê°
            weekly_time_str = (base_time + timedelta(minutes=10)).strftime("%H:%M")
            monthly_time_str = (base_time + timedelta(minutes=20)).strftime("%H:%M")
            
            current_time_str = now_kst.strftime("%H:%M")
            auto_gen_enabled = current_config.get("report_auto_gen", False)

            if auto_gen_enabled:
                # 1ï¸âƒ£ [T+0] ì¼ê°„ ë³´ê³ ì„œ (ë§¤ì¼)
                if current_time_str == base_time_str:
                    if last_auto_report_date != now_kst.strftime("%Y-%m-%d"):
                        print(f"ğŸ¤– [{now_kst.strftime('%H:%M:%S')}] (1/3) ì¼ê°„ ë³´ê³ ì„œ ìƒì„±...")
                        # r_typeì„ ëª…ì‹œí•˜ì—¬ commonì˜ save_report_to_fileê³¼ ì—°ë™
                        if generate_auto_report(current_config, r_type="daily"):
                            last_auto_report_date = now_kst.strftime("%Y-%m-%d")

                # 2ï¸âƒ£ [T+10ë¶„] ì£¼ê°„ ë³´ê³ ì„œ (ì¼ìš”ì¼ & 7ì¼ì¹˜ ë°ì´í„° í™•ì¸)
                elif current_time_str == weekly_time_str and now_kst.weekday() == 6:
                    daily_dir = "/share/ai_analyst/reports/01_daily"
                    daily_files = [f for f in os.listdir(daily_dir) if f.endswith(".txt") and f != "latest.txt"]
                    
                    if len(daily_files) >= 7:
                        current_week = now_kst.strftime("%Y-%U")
                        if last_weekly_report_date != current_week:
                            print(f"ğŸ“… [{now_kst.strftime('%H:%M:%S')}] (2/3) ì£¼ê°„ ê²°ì‚° ë¦¬í¬íŠ¸ ìƒì„±...")
                            if generate_auto_report(current_config, r_type="weekly"):
                                last_weekly_report_date = current_week
                    else:
                        print(f"âš ï¸ ì£¼ê°„ ë¦¬í¬íŠ¸ ìŠ¤í‚µ: ì¼ê°„ ë°ì´í„° ë¶€ì¡± ({len(daily_files)}/7)")

                # 3ï¸âƒ£ [T+20ë¶„] ì›”ê°„ ë³´ê³ ì„œ (ë§¤ì›” 1ì¼ & 20ì¼ì¹˜ ë°ì´í„° í™•ì¸)
                elif current_time_str == monthly_time_str and now_kst.day == 1:
                    daily_dir = "/share/ai_analyst/reports/01_daily"
                    daily_files = [f for f in os.listdir(daily_dir) if f.endswith(".txt") and f != "latest.txt"]
                    
                    if len(daily_files) >= 20:
                        current_month = now_kst.strftime("%Y-%m")
                        if last_monthly_report_date != current_month:
                            print(f"ğŸ›ï¸ [{now_kst.strftime('%H:%M:%S')}] (3/3) ì›”ê°„ ê²°ì‚° ë¦¬í¬íŠ¸ ìƒì„±...")
                            if generate_auto_report(current_config, r_type="monthly"):
                                last_monthly_report_date = current_month
                    else:
                        print(f"âš ï¸ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤í‚µ: ì¼ê°„ ë°ì´í„° ë¶€ì¡± ({len(daily_files)}/20)")

            # --- [T3: ë‰´ìŠ¤ ìˆ˜ì§‘] ---
            update_interval_sec = current_config.get("update_interval", 10) * 60
            if current_ts - last_news_time >= update_interval_sec:
                # (RSS ìˆ˜ì§‘ ë¡œì§ í˜¸ì¶œë¶€)
                last_news_time = current_ts
                
        except Exception as e: 
            print(f"âŒ ë£¨í”„ ì—ëŸ¬: {e}")
            
        time.sleep(60)













