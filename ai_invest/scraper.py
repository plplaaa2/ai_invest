import hashlib
from common import *

processed_titles = set()

def save_file(entry, feed_name):
    """ê°œì„ ëœ íƒ€ì„ë¼ì¸ ë³´ì¡´ ì €ì¥ ë°©ì‹ (JSON)"""
    global processed_titles
    
    title = entry.title.strip()
# ğŸ¯ 1. ë°œí–‰ ì‹œê°„ì„ KST(í•œêµ­ í‘œì¤€ì‹œ)ë¡œ ì—„ê²©í•˜ê²Œ ë³€í™˜ [cite: 1, 4]
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        # UTC ê¸°ë°˜ êµ¬ì¡°ì²´ ì‹œê°„ì„ KST datetime ê°ì²´ë¡œ ë³€í™˜ [cite: 3, 4]
        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc).astimezone(KST)
    else:
        # ì‹œê°„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ KST ì‹œê° ì‚¬ìš© 
        dt_obj = get_now_kst()
        
    dt_str = dt_obj.strftime('%Y%m%d_%H%M%S')# íŒŒì¼ëª… ì •ë ¬ìš©
    date_key = dt_obj.strftime('%Y%m%d')     # ì¼ë³„ ì¤‘ë³µ ë¶„ë¦¬ìš©
    pub_dt_str = dt_obj.strftime('%Y-%m-%d %H:%M:%S') # ë°ì´í„° ì €ì¥ìš©
    
    # ğŸ¯ 2. ì¤‘ë³µ ì²´í¬ í‚¤ ê°•í™” (ë‚ ì§œ + ì œëª© 15ì)
    # ì´ì œ ë‚ ì§œê°€ ë‹¤ë¥´ë©´ ê°™ì€ ì œëª©ì´ë¼ë„ ë³„ê°œ ë‰´ìŠ¤ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    clean_key = f"{date_key}_{title.replace(' ', '')[:15]}"
    
    if clean_key in processed_titles:
        return False
    
    # ğŸ¯ 3. íŒŒì¼ëª…ì— ì‹œê°„ ì •ë³´ ì£¼ì… (ì •ë ¬ ìµœì í™”)
    file_hash = hashlib.md5(title.encode()).hexdigest()[:6]
    filename = f"{dt_str}_{file_hash}.json" # JSON í™•ì¥ì ì‚¬ìš©
    filepath = os.path.join(PENDING_PATH, filename)
    
    # ğŸ¯ 4. ë°ì´í„° êµ¬ì¡°í™” (AI ë¶„ì„ìš© ì •ë³´ í™•ì¥)
    news_data = {
        "title": title,
        "pub_dt": pub_dt_str, # [ìˆ˜ì • ì™„ë£Œ]
        "source": feed_name,
        "summary": entry.get('summary', 'ë‚´ìš© ì—†ìŒ'),
        "link": entry.get('link', '')
    }
    
    try:
        os.makedirs(PENDING_PATH, exist_ok=True)
        with open(filepath, "w", encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        processed_titles.add(clean_key)
        return True
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}") # ì—ëŸ¬ ë¡œê·¸ë¥¼ ë‚¨ê²¨ì•¼ ê²½ë¡œ ë¬¸ì œë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        return False
        
def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    if any(x in text for x in exc_list if x):
        return False
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    
    for filename in os.listdir(PENDING_PATH):
        file_path = os.path.join(PENDING_PATH, filename)
        if os.path.isfile(file_path) and (filename.endswith(".json") or filename.endswith(".txt")):
            if (current_time - os.path.getmtime(file_path)) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: pass
    
    # íŒŒì¼ ì‚­ì œ ì‹œ ë©”ëª¨ë¦¬ ìºì‹œë„ í•¨ê»˜ ë¹„ì›Œ ì‹œìŠ¤í…œì„ ê°€ë³ê²Œ ìœ ì§€
    processed_titles.clear()
    if deleted_count > 0:
        print(f"ğŸ§¹ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì •ë¦¬í•˜ê³  ì¤‘ë³µ í•„í„°ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")


def _prepare_daily_report_data(config_data, now_kst):
    """ì¼ê°„ ë³´ê³ ì„œìš© ë°ì´í„° êµ¬ì„± (KRX ì‹œì¥ ì§€í‘œ + ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í†µí•©)"""
    print(f"ğŸ” [STEP 2-D] Daily ë°ì´í„° ìˆ˜ì§‘ (KRX ì§€í‘œ & ë‰´ìŠ¤ í•„í„°ë§) ì‹œì‘...")
    
    # ğŸ¯ 1. KRX ì‹œì¥ ì§€í‘œ ë°ì´í„° ìˆ˜ì§‘ (common.pyì˜ í•¨ìˆ˜ í™œìš©)
    market_summary = get_krx_market_indicators()    # ì§€ìˆ˜, ê±°ë˜ëŸ‰, ê±°ë˜ëŒ€ê¸ˆ, ìˆ˜ê¸‰
    top_purchases = get_krx_top_investors()      # ì™¸ì¸/ê¸°ê´€ ìˆœë§¤ìˆ˜ ìƒìœ„ 10ê°œ
    industry_indices = get_krx_sector_indices()    # ì£¼ìš” ì‚°ì—…ë³„ ì§€ìˆ˜ í˜„í™©
    
    # ğŸ¯ 2. ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¤‘ë³µ/ë‚ ì§œ í•„í„°ë§
    news_count = config_data.get("report_news_count", 100)
    raw_news_list = []
    seen_keys = set()
    target_date_limit = (now_kst - timedelta(days=3)).date()
    
    if os.path.exists(PENDING_PATH):
        files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
        parse_fail, filter_fail = 0, 0

        for f_name in files:
            try:
                with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                    news_data = json.load(file)
                    title = news_data.get("title", "").strip()
                    pub_dt_str = news_data.get("pub_dt", "")
                    
                    if not title: continue
                    
                    try:
                        f_dt = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S').date()
                    except:
                        f_dt = now_kst.date()

                    if f_dt < target_date_limit:
                        filter_fail += 1
                        continue

                    clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
                    if clean_key not in seen_keys:
                        seen_keys.add(clean_key)
                        raw_news_list.append(f"[{pub_dt_str[5:16]}] {title}")
                        
                    if len(raw_news_list) >= news_count:
                        break
            except:
                parse_fail += 1
                continue
        print(f"ğŸ“Š [ê²°ê³¼] ìˆ˜ì§‘ ì™„ë£Œ: ë‰´ìŠ¤ {len(raw_news_list)}ê°œ | ì œì™¸ {filter_fail} | ì‹¤íŒ¨ {parse_fail}")
    
    # ğŸ¯ 3. ìµœì¢… ë°ì´í„° í†µí•© (ì§€í‘œ ìš°ì„  ë°°ì¹˜)
    news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n" + "\n".join([f"- {t}" for t in raw_news_list])
    
    # ì‹¤ì œ ìˆ˜ì¹˜ ë°ì´í„°ì™€ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ AIì—ê²Œ ì „ë‹¬
    combined_content = (
        f"{market_summary}\n"
        f"{top_purchases}\n"
        f"{industry_indices}\n\n"
        f"{news_ctx}"
    )
    
    return combined_content, "ì¼ê°„(Daily)"


def _prepare_periodical_report_data(config_data, r_type):
    """ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œìš© ë°ì´í„° êµ¬ì„± (ê³¼ê±° ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ìš”ì•½ í™œìš©)"""
    lookback = 7 if r_type == "weekly" else 30
    label = "ì£¼ê°„(Weekly)" if r_type == "weekly" else "ì›”ê°„(Monthly)"
    print(f"ğŸ—“ï¸ [STEP 2-{r_type[0].upper()}] {label} ëª¨ë“œ: ê³¼ê±° ë¦¬í¬íŠ¸ ìš”ì•½ êµ¬ì„± ì¤‘...")

    # ê³¼ê±° ì¼ê°„ ë¦¬í¬íŠ¸ íŒŒì¼ ì½ê¸° (í…ìŠ¤íŠ¸ì—ì„œ ì§€í‘œ íë¦„ì„ íŒŒì•…í•˜ê¸° ìœ„í•¨)
    daily_dir = os.path.join(REPORT_DIR, "01_daily")
    report_summary = f"### [ ì§€ë‚œ {lookback}ì¼ê°„ì˜ ë¶„ì„ ê¸°ë¡ ìš”ì•½ ]\n"
    
    if os.path.exists(daily_dir):
        files = sorted([f for f in os.listdir(daily_dir) if f.endswith(".txt") and f != "latest.txt"], reverse=True)
        for f_name in files[:lookback]:
            try:
                with open(os.path.join(daily_dir, f_name), 'r', encoding='utf-8') as f:
                    # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ (AIê°€ í…ìŠ¤íŠ¸ ë‚´ ìˆ˜ì¹˜ë¥¼ ì½ì„ ìˆ˜ ìˆë„ë¡ í•¨)
                    report_summary += f"\n- {f_name}: {f.read()[:400]}...\n"
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({f_name}): {e}")
    
    return report_summary, label

def _execute_report_ai_engine(config_data, r_type, report_label, input_content):
    """
    [ì§€í‘œ ì¶”ì¶œ íŠ¹í™”í˜•] ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ê²½ì œì§€í‘œë¥¼ ì‹ë³„í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
    """
    now_kst = get_now_kst()
    now_str = now_kst.strftime("%Y-%m-%d %H:%M")
    historical_context = load_historical_contexts()
    
    a_cfg = config_data.get("analyst_model", {})
    base_url = a_cfg.get("url", "").rstrip('/')
    model_name = a_cfg.get("name")
    
    print(f"ğŸ¤– [STEP 3] ì§€í‘œ ì¶”ì¶œí˜• AI ëª¨ë¸ í˜¸ì¶œ: {model_name} ({report_label})")

    # ğŸ¯ STEP 1: ë¦¬í¬íŠ¸ íƒ€ì…ë³„ ë¶„ì„ ì‹¬ë„ ë° ë³€ìˆ˜ ì •ì˜ (ì—ëŸ¬ í•´ê²°)
    if r_type == "daily":
        base_prompt = config_data.get("council_prompt", "ë‹¹ì‹ ì€ ì „ëµ ìì‚° ë°°ë¶„ê°€ì…ë‹ˆë‹¤.")
        specific_guideline = (
            "**[KRX ë°ì´í„° ë¶„ì„ ë°©ë²•ë¡ ]**\n"
            "1.  **ì‹œì¥ ë°©í–¥ì„± í™•ì¸**: `KRX ì‹œì¥ ì§€í‘œ ìš”ì•½`ì—ì„œ ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì§€ìˆ˜ ë“±ë½ì„ ë³´ê³  ì‹œì¥ì˜ ì „ë°˜ì ì¸ ë°©í–¥(ìƒìŠ¹/í•˜ë½/ë³´í•©)ì„ ë¨¼ì € ì •ì˜í•©ë‹ˆë‹¤.\n"
            "2.  **ì£¼ë„ ì£¼ì²´ ì‹ë³„**: `íˆ¬ìì ìˆ˜ê¸‰` ë°ì´í„°ì—ì„œ ì™¸êµ­ì¸ê³¼ ê¸°ê´€ ì¤‘ ëˆ„ê°€ ì‹œì¥ì„ ì£¼ë„í–ˆëŠ”ì§€(ìˆœë§¤ìˆ˜ ê·œëª¨)ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n"
            "3.  **ì£¼ë„ ì„¹í„° íŠ¹ì •**: `ìˆ˜ê¸‰ ìƒìœ„ ì¢…ëª©` ë¦¬ìŠ¤íŠ¸ì—ì„œ ì£¼ë„ ì£¼ì²´(2ë²ˆ)ê°€ ì§‘ì¤‘ì ìœ¼ë¡œ ë§¤ìˆ˜í•œ ì¢…ëª©ë“¤ì„ í™•ì¸í•˜ì—¬ 'ì˜¤ëŠ˜ì˜ ì£¼ë„ ì„¹í„°'ë¥¼ íŠ¹ì •í•©ë‹ˆë‹¤.\n"
            "4.  **ì„¹í„° ê°•ë„ êµì°¨ê²€ì¦**: `ì£¼ìš” ì‚°ì—…ë³„ ì§€ìˆ˜ í˜„í™©`ì—ì„œ 3ë²ˆì—ì„œ íŠ¹ì •í•œ ì„¹í„°ì˜ ì§€ìˆ˜ê°€ ì‹¤ì œë¡œ ìƒìŠ¹í–ˆëŠ”ì§€ êµì°¨ ê²€ì¦í•©ë‹ˆë‹¤.\n"
            "5.  **ë‰´ìŠ¤ ì—°ê³„ í•´ì„**: ìœ„ 1~4ë²ˆ ê³¼ì •ìœ¼ë¡œ ë„ì¶œëœ 'ë°ì´í„° ê¸°ë°˜ ê²°ë¡ 'ì— ëŒ€í•œ ì´ìœ ë‚˜ ë°°ê²½ì„ `ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤`ì—ì„œ ì°¾ì•„ë‚´ì–´ ì„¤ëª…ì„ ë³´ê°•í•©ë‹ˆë‹¤. ë‰´ìŠ¤ëŠ” ë°ì´í„° ë¶„ì„ì„ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°ë¡œë§Œ í™œìš©í•˜ì„¸ìš”."
        )
        structure_type = "ì¼ê°„ ì‹œí™© ë° ë°ì´í„° ê¸°ë°˜ ì „ëµ ë¶„ì„"
    else:
        # ì£¼ê°„(Weekly) ë° ì›”ê°„(Monthly) ì „ìš©
        base_prompt = f"ë‹¹ì‹ ì€ 'ê±°ì‹œê²½ì œ ì‹œê³„ì—´ ì „ëµê°€'ì…ë‹ˆë‹¤. ì§€ë‚œ {r_type}ê°„ì˜ ê¸°ë¡ì—ì„œ ì§€í‘œì˜ ê¶¤ì ì„ ë¶„ì„í•˜ì„¸ìš”."
        specific_guideline = (
            f"1. ì§€í‘œ ì¶”ì„¸ ë¶„ì„: ì§€ë‚œ {r_type}ê°„ ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ë°˜ë³µ ì–¸ê¸‰ëœ ì£¼ìš” ì§€í‘œì˜ ë³€í™” ê¶¤ì ì„ ì¬êµ¬ì„±í•˜ë¼.\n"
            "2. ì ì¤‘ë¥  ê²€í† : ê³¼ê±° ë¦¬í¬íŠ¸(historical_context)ì— ì–¸ê¸‰ëœ ì „ë§ ìˆ˜ì¹˜ì™€ í˜„ì¬ ì‹¤ì œ ìˆ˜ì¹˜ë¥¼ ëŒ€ì¡°í•˜ë¼.\n"
            "3. ì „ëµ ì œì–¸: ìˆ˜ì§‘ëœ ì§€í‘œ íë¦„ì„ ë°”íƒ•ìœ¼ë¡œ ìì‚° ë°°ë¶„ ë¹„ì¤‘ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¡°ì ˆí•˜ë¼."
        )
        structure_type = f"{r_type.capitalize()} ì „ëµ ìì‚° ë°°ë¶„ ë¦¬í¬íŠ¸" # âœ… ë³€ìˆ˜ ì •ì˜ í™•ì¸

    # ğŸ¯ STEP 2: ë¶„ì„ ì§€ì¹¨ ë° êµ¬ì¡° í†µí•©
    analysis_guideline = (
        f"### [ {report_label} ì§€í‘œ ë¶„ì„ ì§€ì¹¨ ]\n"
        f"{specific_guideline}\n"
        "4. ì‹œì¥ ìƒíƒœ: ì£¼ë§ì¸ ê²½ìš° ê°€ì¥ ìµœê·¼ ê±°ë˜ì¼(ê¸ˆìš”ì¼) ë°ì´í„°ë¥¼ í˜„ì¬ê°€ë¡œ ê°„ì£¼í•œë‹¤.\n"
        "5. ì „ëµì  ìˆ˜ì •: ìˆ˜ì¹˜ ë³€í™”ì— ë”°ë¼ íˆ¬ì í–‰ë™ ì§€ì¹¨ì„ ìœ ì—°í•˜ê²Œ ì—…ë°ì´íŠ¸í•˜ë¼."
    )

    structure_instruction = (
        f"### [ ë³´ê³ ì„œ ì‘ì„± í˜•ì‹: {structure_type} ]\n" # âœ… ì´ì œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        "1. ì‹œí™© ì¢…í•© ë¸Œë¦¬í•‘ / 2. ë‰´ìŠ¤ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ì§€í‘œ ë¦¬ìŠ¤íŠ¸ / 3. ì§€í‘œë³„ ì¶”ì„¸ ë¶„ì„ / "
        "4. ê±°ì‹œê²½ì œ íŒë‹¨ / 5. ì‚°ì—…/í…Œë§ˆ ì˜í–¥ / 6. ë¦¬ìŠ¤í¬ ê´€ë¦¬ / 7. ìì‚° ë°°ë¶„ ì „ëµ(%) / "
        "8. ì°¨ê¸° ë¶„ì„ìš© ì§€í‘œ ë©”ëª¨"
    )

    # ğŸ¯ STEP 3: ìµœì¢… í”„ë¡¬í”„íŠ¸ í†µí•©
    system_prompt = (
        f"í˜„ì¬ ì„ë¬´: {report_label} íˆ¬ì ì „ëµ ë³´ê³ ì„œ ì‘ì„±\n"
        f"ê¸°ì¤€ ì‹œê°: {now_str}\n\n"
        f"ë‹¹ì‹ ì€ {base_prompt}ì´ë©°, ì§€ê¸ˆë¶€í„° ì œì‹œëœ ë¶„ì„ ë°©ë²•ë¡ ì„ ì² ì €íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"{analysis_guideline}\n\n"
        f"--- [ ê³¼ê±° ë¶„ì„ ê¸°ë¡ (ì°¸ê³ ìš©) ] ---\n{historical_context}\n\n"
        f"--- [ ìµœì¢… ì§€ì‹œ ] ---\n"
        f"ì´ì œ ì œê³µë  ë°ì´í„°(KRX ì›ì²œ ë°ì´í„°, ìµœì‹  ë‰´ìŠ¤)ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìœ„ ë°©ë²•ë¡ ê³¼ ê³¼ê±° ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì•„ë˜ í˜•ì‹ì— ë§ì¶° íˆ¬ì ì „ëµ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"{structure_instruction}"
    )

    # ğŸ¯ STEP 4: API ì¸ì¦ ì •ë³´ ë¡œë“œ
    oa_key = config.get("openai_api_key", "")
    gm_key = config.get("gemini_api_key", "")

    # ğŸ¯ STEP 5: ëª¨ë¸ ìœ í˜•ë³„ í˜ì´ë¡œë“œ êµ¬ì„± ë° í˜¸ì¶œ
    if "googleapis.com" in base_url or "gemini" in model_name.lower():
        url = f"{base_url}/v1beta/models/{model_name}:generateContent?key={gm_key}"
        headers = {"Content-Type": "application/json"}
           # GeminiëŠ” System Promptë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì³ì„œ ì „ë‹¬
        payload = {"contents": [{"parts": [{"text": f"{system_prompt}\n\n--- [ ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ] ---\n{input_content}"}]}]}
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        if oa_key: headers["Authorization"] = f"Bearer {oa_key}"
        payload = {
            "model": model_name,
            # OpenAI ê·œê²©ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ë¦¬í•˜ì—¬ ì „ë‹¬
            "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": input_content}],
            "temperature": 0.2 # ì§€í‘œ ìˆ˜ì¹˜ íŒŒì‹±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ìœ ì§€
        }

    # ğŸ¯ STEP 6: ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
    try:
        start_time = time.time()
        resp = requests.post(url, json=payload, headers=headers, timeout=300)
        resp.raise_for_status()
        result = resp.json()
        
        if "candidates" in result:
            report_content = result['candidates'][0]['content']['parts'][0]['text']
        else:
            report_content = result['choices'][0]['message']['content']
        
        save_path = save_report_to_file(report_content, r_type)
        print(f"âœ¨ [STEP 4] {report_label} ìƒì„± ì„±ê³µ! (ì†Œìš”ì‹œê°„: {time.time()-start_time:.1f}ì´ˆ)")
        return True
    except Exception as e:
        print(f"ğŸš¨ [ì—ëŸ¬] AI ì—”ì§„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def generate_auto_report(config_data, r_type):
    """ìë™ ë³´ê³ ì„œ ìƒì„± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    now_kst = get_now_kst()
    
    if r_type == "daily":
        input_content, label = _prepare_daily_report_data(config_data, now_kst)
    else:
        input_content, label = _prepare_periodical_report_data(config_data, r_type)
        
    if not input_content:
        print(f"âš ï¸ [Auto] ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False

    return _execute_report_ai_engine(config_data, r_type, label, input_content)

# --- [ 3. ë©”ì¸ ë£¨í”„ (ìˆ˜ë™ ì‘ì—…ì— ë°©í•´ë°›ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¤„ëŸ¬) ] ---

if __name__ == "__main__":
    # ğŸ’¡ ìë™í™”(Auto) ì „ìš© ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜ (ìˆ˜ë™ ì‹¤í–‰ ì‹œ ì´ ë³€ìˆ˜ë“¤ì„ ê±´ë“œë¦¬ì§€ ì•Šìœ¼ë©´ ìë™ ì‹¤í–‰ë¨)
    auto_daily_done_date = ""
    auto_weekly_done_week = ""
    auto_monthly_done_month = ""
    
    last_news_time = 0

    try:
        init_config = load_data()
        print(f"ğŸš€ [AI Analyst] ì‹œìŠ¤í…œ ê°€ë™ - ê¸°ì¤€ ì‹œê°: {init_config.get('report_gen_time', '08:00')} (KST)")
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

    while True:
        try:
            now_kst = get_now_kst()
            current_ts = time.time() # ğŸš¨ NameError í•´ê²°
            current_config = load_data()
            
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            base_time_str = str(current_config.get("report_gen_time", "08:00")).strip()
            current_time_str = now_kst.strftime("%H:%M")
            
            # ì˜ˆì•½ ì‹œê° ê³„ì‚° (10ë¶„/20ë¶„ ê°„ê²©)
            base_dt = datetime.strptime(base_time_str, "%H:%M")
            weekly_time_str = (base_dt + timedelta(minutes=10)).strftime("%H:%M")
            monthly_time_str = (base_dt + timedelta(minutes=20)).strftime("%H:%M")

            # --- [ ğŸ¤– ìë™ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ ] ---
            if auto_gen_enabled:
                
                # â‘  ì¼ê°„ ìë™ ë³´ê³ ì„œ
                if current_time_str == base_time_str:
                    today_str = now_kst.strftime("%Y-%m-%d")
                    # ğŸ’¡ ìˆ˜ë™ ë³´ê³ ì„œ íŒŒì¼ì´ ìˆì–´ë„, 'ìë™ ìŠ¤ì¼€ì¤„ëŸ¬'ê°€ ì˜¤ëŠ˜ ì²˜ìŒì´ë¼ë©´ ì‹¤í–‰í•©ë‹ˆë‹¤.
                    if auto_daily_done_date != today_str:
                        print(f"ğŸ¤– [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "daily"):
                            auto_daily_done_date = today_str # ìë™ ì‹¤í–‰ ì„±ê³µ ì‹œì—ë§Œ ë§ˆí‚¹

                # â‘¡ ì£¼ê°„ ìë™ ë³´ê³ ì„œ (ì¼ìš”ì¼)
                elif current_time_str == weekly_time_str and now_kst.weekday() == 6:
                    week_str = now_kst.strftime("%Y-%U")
                    if auto_weekly_done_week != week_str:
                        print(f"ğŸ“… [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì£¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "weekly"):
                            auto_weekly_done_week = week_str

                # â‘¢ ì›”ê°„ ìë™ ë³´ê³ ì„œ (1ì¼)
                elif current_time_str == monthly_time_str and now_kst.day == 1:
                    month_str = now_kst.strftime("%Y-%m")
                    if auto_monthly_done_month != month_str:
                        print(f"ğŸ›ï¸ [{now_kst.strftime('%H:%M:%S')}] >>> ìŠ¤ì¼€ì¤„ëŸ¬: ìë™ ì›”ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œë„")
                        if generate_auto_report(current_config, "monthly"):
                            auto_monthly_done_month = month_str
            # --- [ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¹ì…˜ ] ---
            update_interval_min = current_config.get("update_interval", 10)
            update_interval_sec = update_interval_min * 60
            
            # ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (ë¡œê·¸ìš©)
            time_since_last = current_ts - last_news_time
            next_in = max(0, update_interval_sec - time_since_last)

            if time_since_last >= update_interval_sec:
                print(f"ğŸ“¡ [{now_kst.strftime('%H:%M:%S')}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ (ì£¼ê¸°: {update_interval_min}ë¶„)")
                
                feeds = current_config.get("feeds", [])
                g_inc = [k.strip().lower() for k in current_config.get('global_include', "").split(",") if k.strip()]
                g_exc = [k.strip().lower() for k in current_config.get('global_exclude', "").split(",") if k.strip()]
                
                new_saved = 0
                for feed in feeds:
                    try:
                        parsed = feedparser.parse(feed['url'])
                        l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                        l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                        
                        feed_new = 0
                        for entry in parsed.entries[:50]:
                            if not check_logic(entry.title, g_inc, g_exc): continue
                            if not check_logic(entry.title, l_inc, l_exc): continue
                            if save_file(entry, feed['name']):
                                feed_new += 1
                                new_saved += 1
                        if feed_new > 0:
                            print(f"   â””â”€ {feed['name']}: {feed_new}ê°œ ì‹ ê·œ ì €ì¥")
                    except Exception as e:
                        print(f"   â””â”€ âŒ {feed.get('name')} ì˜¤ë¥˜: {e}")
                
                print(f"âœ… [{now_kst.strftime('%H:%M:%S')}] ìˆ˜ì§‘ ì™„ë£Œ (ì´ {new_saved}ê°œ ì‹ ê·œ í™•ë³´)")
                last_news_time = current_ts
            else:
                # ë§¤ ë¶„ë§ˆë‹¤ ì •ê¸° ìƒì¡´ ì‹ ê³  ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
                if now_kst.minute % 5 == 0: # 5ë¶„ë§ˆë‹¤ ì¶œë ¥
                    print(f"ğŸ’¤ [{now_kst.strftime('%H:%M:%S')}] ëŒ€ê¸° ì¤‘... (ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ê¹Œì§€ {int(next_in/60)}ë¶„ ë‚¨ìŒ)")

        except Exception as e: 
            print(f"ğŸš¨ [{datetime.now().strftime('%H:%M:%S')}] ë£¨í”„ ì¹˜ëª…ì  ì—ëŸ¬: {e}")
            
        time.sleep(60)
