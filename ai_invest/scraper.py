import feedparser
import time
import os
import json
import hashlib

# --- ê²½ë¡œ ì„¤ì • ---
CONFIG_PATH = "/share/ai_analyst/rss_config.json"
SAVE_PATH = "/share/ai_analyst/pending"

def get_file_hash(text):
    """ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def save_file(entry, feed_name):
    os.makedirs(SAVE_PATH, exist_ok=True)
    # ì œëª© ê¸°ë°˜ í•´ì‹œë¡œ íŒŒì¼ëª… ìƒì„±
    title_hash = hashlib.md5(entry.title.encode('utf-8')).hexdigest()
    fname = f"{SAVE_PATH}/{title_hash}.txt"
    
    if os.path.exists(fname): return

    # ğŸ’¡ RSS ë‚ ì§œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„(2026-02-02)ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    pub_date = entry.get('published') 
    if not pub_date:
        pub_date = datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")

    try:
        with open(fname, "w", encoding="utf-8") as f:
            # âš ï¸ ì•„ë˜ ìˆœì„œë¥¼ ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš” (app.pyì˜ load_pending_filesì™€ ì§ê²°ë¨)
            f.write(f"ì œëª©: {entry.title}\n")
            f.write(f"ë§í¬: {entry.link}\n")
            f.write(f"ë‚ ì§œ: {pub_date}\n") # ğŸ’¡ 3ë²ˆì§¸ ì¤„ì— ë‚ ì§œ ê¸°ë¡
            f.write(f"ìš”ì•½: {entry.get('summary', 'ë‚´ìš© ì—†ìŒ')}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")



def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    # 1. ì œì™¸ í•„í„° (í•˜ë‚˜ë¼ë„ ê±¸ë¦¬ë©´ ë°”ë¡œ íƒˆë½)
    if any(x in text for x in exc_list if x):
        return False
    # 2. í¬í•¨ í•„í„° (ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì²´í¬)
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ë³´ê´€ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    if not os.path.exists(SAVE_PATH):
        return
        
    current_time = time.time()
    # 1ì¼ = 86400ì´ˆ
    seconds_threshold = retention_days * 86400
    
    deleted_count = 0
    for filename in os.listdir(SAVE_PATH):
        file_path = os.path.join(SAVE_PATH, filename)
        
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì²´í¬
        if os.path.isfile(file_path) and filename.endswith(".txt"):
            file_age = os.path.getmtime(file_path)
            if (current_time - file_age) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({filename}): {e}")
                    
    if deleted_count > 0:
        print(f"ğŸ§¹ ë³´ê´€ ê¸°ê°„ ë§Œë£Œë¡œ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. (ê¸°ì¤€: {retention_days}ì¼)")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (ì „ì—­ í•„í„°ë§ ë° ìë™ ì‚­ì œ ì‹œìŠ¤í…œ)...")
    
    while True:
        # 1. ì„¤ì • ë° ì „ì—­ í•„í„° ë¡œë“œ (ê¸°ë³¸ ë³€ìˆ˜ëª… config ìœ ì§€)
        config = {"feeds": [], "update_interval": 10, "global_include": "", "global_exclude": "", "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except Exception as e:
                print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        interval = config.get("update_interval", 10)
        retention_days = config.get("retention_days", 7)
        
        # [ìë™ ì‚­ì œ ë¡œì§ ì‹¤í–‰]
        cleanup_old_files(retention_days)
        
        # 2. ì „ì—­ í•„í„° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸í™”
        g_inc = [k.strip().lower() for k in config.get('global_include', "").split(",") if k.strip()]
        g_exc = [k.strip().lower() for k in config.get('global_exclude', "").split(",") if k.strip()]

        feeds = config.get("feeds", [])
        if not feeds:
            time.sleep(60); continue

        # 3. ê°œë³„ í”¼ë“œ ìˆœíšŒ ë° ìˆ˜ì§‘
        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                
                for entry in parsed.entries[:50]:
                    # ì œëª© ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
                    check_text = entry.title
                    
                    if not check_logic(check_text, g_inc, g_exc): continue
                    if not check_logic(check_text, l_inc, l_exc): continue
                    
                    save_file(entry, feed['name'])
            except: continue
        
        print(f"ğŸ’¤ {interval}ë¶„ í›„ ì—…ë°ì´íŠ¸ í™•ì¸ ë° íŒŒì¼ ì •ë¦¬ ì˜ˆì •...")
        time.sleep(interval * 60)

# --- [5. ë©”ì¸ ë£¨í”„] ---
if __name__ == "__main__":
    # ğŸ’¡ ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„°ë¥¼ ì €ì¥í•  ë©”ëª¨ë¦¬ ê³µê°„ (ì´ˆê¸°í™”)
    last_prices = {} 
    last_collect_time, last_news_time, last_auto_report_date = 0, 0, ""
# ğŸ¯ [í•µì‹¬] ìˆ˜ì§‘ê¸°ë¥¼ ì¼°ì„ ë•Œ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ë¯¸ë¦¬ ë„£ì–´ ì •ì‹œ ê°€ë™ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    last_auto_report_date = datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸš€ [AI Analyst V3] í†µí•© ìˆ˜ì§‘ê¸° ê°€ë™ ì‹œì‘")

# ğŸ¯ [ì‹ ê·œ] ì„¤ì • íŒŒì¼ ìë™ ìƒì„± ë¡œì§
    if not os.path.exists(CONFIG_PATH):
        print(f"ğŸ› ï¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤: {CONFIG_PATH}")
        default_config = {
            "report_auto_gen": True,         # ê¸°ë³¸ì ìœ¼ë¡œ ìë™ ìƒì„± ì¼¬
            "report_gen_time": "08:00",      # ê¸°ë³¸ ì•„ì¹¨ 8ì‹œ
            "report_news_count": 100,        # ë‰´ìŠ¤ 100ê°œ
            "update_interval": 10,           # 10ë¶„ ì£¼ê¸°
            "feeds": [],                     # ë¹„ì–´ìˆëŠ” í”¼ë“œ ë¦¬ìŠ¤íŠ¸
            "analyst_model": {               # 5070 Tiì— ìµœì í™”ëœ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
                "name": "openai/gpt-oss-20b",
                "url": "http://192.168.1.105:11434/v1", 
                "prompt": "ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ì§€í‘œì™€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ˜ìµ ì „ëµì„ ì œì‹œí•˜ì„¸ìš”."
            }
        }
        with open(CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(default_config, f, indent=4, ensure_ascii=False)
        print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ.")

    while True:
        try:
            now, current_ts = datetime.now(), time.time()
            # ìµœì‹  ì„¤ì •ê°’ ë¡œë“œ (ë³´ê³ ì„œ ìƒì„± ì‹œê°„, ìë™ ìƒì„± ì—¬ë¶€ ë“±)
            with open(CONFIG_PATH, "r", encoding="utf-8") as f: 
                current_config = json.load(f)

            # ---------------------------------------------------------
            # ğŸ¯ [T1: ìë™ ë³´ê³ ì„œ ë¡œì§ ë³µêµ¬]
            # ---------------------------------------------------------
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            target_time_str = current_config.get("report_gen_time", "08:00")
            today_date_str = now.strftime("%Y-%m-%d")
            current_time_str = now.strftime("%H:%M")

            # 1. ìë™ ìƒì„± í™œì„±í™” ì—¬ë¶€ í™•ì¸
            # 2. í˜„ì¬ ì‹œê°„ì´ ì„¤ì •ëœ ì‹œê°„(HH:MM)ì„ ì§€ë‚¬ëŠ”ì§€ í™•ì¸
            # 3. ì˜¤ëŠ˜ ì´ë¯¸ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ìƒì„± ë°©ì§€)
            current_time_str = now.strftime("%H:%M")
            
            # 1. ìë™ ìƒì„±ì´ ì¼œì ¸ ìˆê³ 
            # 2. ì§€ê¸ˆ ì‹œê°ì´ ì„¤ì •í•œ ì‹œê°ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ë©° (ë˜ëŠ” 1ë¶„ ì´ë‚´ ë£¨í”„ ì‹œì )
            # 3. ì˜¤ëŠ˜ ë³´ê³ ì„œë¥¼ ë°œí–‰í•œ ì ì´ ì—†ì„ ë•Œë§Œ ì‹¤í–‰
            if auto_gen_enabled and current_time_str == target_time_str and last_auto_report_date != today_date_str:
                print(f"ğŸ¤– [{now.strftime('%H:%M:%S')}] ì •ì‹œ ë³´ê³ ì„œ ìƒì„± ì‹œê° ë„ë˜: ë¶„ì„ ì‹œì‘...")
                
                success = generate_auto_report(current_config)
                
                if success:
                    print(f"âœ… [{now.strftime('%H:%M:%S')}] ì •ê¸° ë³´ê³ ì„œ ë°•ì œ ì™„ë£Œ.")
                    last_auto_report_date = today_date_str
                else:
                    print(f"âš ï¸ [{now.strftime('%H:%M:%S')}] ìƒì„± ì‹¤íŒ¨ (1ë¶„ ë’¤ ì¬ì‹œë„)")
            # ---------------------------------------------------------
# [T3: ë‰´ìŠ¤ ìˆ˜ì§‘] (ìˆ˜ì§‘ ì£¼ê¸° ì„¤ì •ê°’ ë°˜ì˜)
            update_interval_sec = current_config.get("update_interval", 10) * 60
            if current_ts - last_news_time >= update_interval_sec:
                # print(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™...")
                # fetch_all_rss_feeds() í•¨ìˆ˜ ë“± ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§ í˜¸ì¶œ
                last_news_time = current_ts
                
        except Exception as e: 
            print(f"âŒ ë£¨í”„ ì—ëŸ¬: {e}")
        time.sleep(60)
