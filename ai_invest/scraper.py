import hashlib
from common import *

processed_titles = set()

def save_file(entry, feed_name):
    """ê°œì„ ëœ íƒ€ì„ë¼ì¸ ë³´ì¡´ ì €ì¥ ë°©ì‹ (JSON)"""
    global processed_titles
    
    title = entry.title.strip()
# ğŸ¯ 1. ë°œí–‰ ì‹œê°„ì„ KST(í•œêµ­ í‘œì¤€ì‹œ)ë¡œ ì—„ê²©í•˜ê²Œ ë³€í™˜ [cite: 1, 4]
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        # UTC ê¸°ë°˜ êµ¬ì¡°ì²´ ì‹œê°„ì„ KST datetime ê°ì²´ë¡œ ë³€í™˜ [cite: 3, 4]
        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc).astimezone(KST)
    else:
        # ì‹œê°„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ KST ì‹œê° ì‚¬ìš© 
        dt_obj = get_now_kst()
        
    dt_str = dt_obj.strftime('%Y%m%d_%H%M%S')# íŒŒì¼ëª… ì •ë ¬ìš©
    date_key = dt_obj.strftime('%Y%m%d')     # ì¼ë³„ ì¤‘ë³µ ë¶„ë¦¬ìš©
    pub_dt_str = dt_obj.strftime('%Y-%m-%d %H:%M:%S') # ë°ì´í„° ì €ì¥ìš©
    
    # ğŸ¯ 2. ì¤‘ë³µ ì²´í¬ í‚¤ ê°•í™” (ë‚ ì§œ + ì œëª© 15ì)
    # ì´ì œ ë‚ ì§œê°€ ë‹¤ë¥´ë©´ ê°™ì€ ì œëª©ì´ë¼ë„ ë³„ê°œ ë‰´ìŠ¤ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    clean_key = f"{date_key}_{title.replace(' ', '')[:15]}"
    
    if clean_key in processed_titles:
        return False
    
    # ğŸ¯ 3. íŒŒì¼ëª…ì— ì‹œê°„ ì •ë³´ ì£¼ì… (ì •ë ¬ ìµœì í™”)
    file_hash = hashlib.md5(title.encode()).hexdigest()[:6]
    filename = f"{dt_str}_{file_hash}.json" # JSON í™•ì¥ì ì‚¬ìš©
    filepath = os.path.join(PENDING_PATH, filename)
    
    # ğŸ¯ 4. ë°ì´í„° êµ¬ì¡°í™” (AI ë¶„ì„ìš© ì •ë³´ í™•ì¥)
    news_data = {
        "title": title,
        "pub_dt": pub_dt_str, # [ìˆ˜ì • ì™„ë£Œ]
        "source": feed_name,
        "summary": entry.get('summary', 'ë‚´ìš© ì—†ìŒ'),
        "link": entry.get('link', '')
    }
    
    try:
        os.makedirs(PENDING_PATH, exist_ok=True)
        with open(filepath, "w", encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        processed_titles.add(clean_key)
        return True
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}") # ì—ëŸ¬ ë¡œê·¸ë¥¼ ë‚¨ê²¨ì•¼ ê²½ë¡œ ë¬¸ì œë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        return False
        
def check_logic(text, inc_list, exc_list):
    """í•„í„°ë§ ë¡œì§: ì œì™¸ì–´ í¬í•¨ ì‹œ íƒˆë½, í¬í•¨ì–´ ì„¤ì • ì‹œ í¬í•¨ë˜ì–´ì•¼ í†µê³¼"""
    text = text.lower()
    if any(x in text for x in exc_list if x):
        return False
    if inc_list:
        if not any(i in text for i in inc_list if i):
            return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ ë° ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ"""
    global processed_titles
    if not os.path.exists(PENDING_PATH): return
    
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    
    for filename in os.listdir(PENDING_PATH):
        file_path = os.path.join(PENDING_PATH, filename)
        if os.path.isfile(file_path) and (filename.endswith(".json") or filename.endswith(".txt")):
            if (current_time - os.path.getmtime(file_path)) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: pass
    
    # íŒŒì¼ ì‚­ì œ ì‹œ ë©”ëª¨ë¦¬ ìºì‹œë„ í•¨ê»˜ ë¹„ì›Œ ì‹œìŠ¤í…œì„ ê°€ë³ê²Œ ìœ ì§€
    processed_titles.clear()
    if deleted_count > 0:
        print(f"ğŸ§¹ {deleted_count}ê°œì˜ ë‰´ìŠ¤ íŒŒì¼ì„ ì •ë¦¬í•˜ê³  ì¤‘ë³µ í•„í„°ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

def start_scraping():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (íƒ€ì„ë¼ì¸ ë³´ì¡´ ë° ë™ì  ì¤‘ë³µ ì œê±°)...")
    
    while True:
        # 1. ì„¤ì • ë° í•„í„°ë§ í‚¤ì›Œë“œ ë¡œë“œ
        config = {"feeds": [], "update_interval": 10, "retention_days": 7}
        if os.path.exists(CONFIG_PATH):
            try:
                with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                    config.update(json.load(f))
            except: pass
        
        interval = config.get("update_interval", 10)
        cleanup_old_files(config.get("retention_days", 7))
        
        # ğŸ¯ ë©”ëª¨ë¦¬ ìºì‹œ(processed_titles)ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ ì£¼ê¸°ì ìœ¼ë¡œ ë¹„ì›Œì£¼ê±°ë‚˜ 
        # ìµœê·¼ Nê°œë§Œ ìœ ì§€í•˜ëŠ” ë¡œì§ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” ì‹¤í–‰ ì‹œ ìœ ì§€)
        
        g_inc = [k.strip().lower() for k in config.get('global_include', "").split(",") if k.strip()]
        g_exc = [k.strip().lower() for k in config.get('global_exclude', "").split(",") if k.strip()]

        # 2. í”¼ë“œ ìˆœíšŒ
        feeds = config.get("feeds", [])
        total_found, new_saved = 0, 0

        for feed in feeds:
            try:
                parsed = feedparser.parse(feed['url'])
                # í”¼ë“œë³„ ê°œë³„ í•„í„°
                l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                
                # ìƒìœ„ 50ê°œ ë‰´ìŠ¤ í™•ì¸
                for entry in parsed.entries[:50]:
                    total_found += 1
                    # ì „ì—­/ê°œë³„ í•„í„°ë§ ë¡œì§ (check_logic í•¨ìˆ˜ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                    if not check_logic(entry.title, g_inc, g_exc): continue
                    if not check_logic(entry.title, l_inc, l_exc): continue
                    
                    if save_file(entry, feed['name']):
                        new_saved += 1
            except Exception as e:
                print(f"âŒ {feed.get('name')} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
                continue
        
        # 3. ì‹¤ì‹œê°„ ë³´ê³  ë¡œê·¸
        now_str = datetime.now().strftime('%H:%M:%S')
        if total_found > 0:
            print(f"[{now_str}] ğŸ“Š ë°œê²¬ {total_found}ê°œ | ì‹ ê·œ {new_saved}ê°œ | í•„í„°/ì¤‘ë³µ ì œì™¸ {total_found - new_saved}ê°œ")
        
        # ğŸ’¤ ìˆ˜ì§‘ ì£¼ê¸°ëŠ” ìœ ë™ì ìœ¼ë¡œ (ê¸°ë³¸ 10ë¶„)
        time.sleep(interval * 60)

def generate_auto_report(config_data, r_type="daily"):
    """
    [í†µí•© ë³´ê³ ì„œ ì—”ì§„] - ë‹¨ê³„ë³„ ë””ë²„ê·¸ ë¡œê·¸ ë° JSON íŒŒì‹± ê°•í™”
    """
    now_kst = get_now_kst()
    now_str = now_kst.strftime("%Y-%m-%d %H:%M:%S")
    
    print(f"\n[ {now_str} ] ğŸ›ï¸ {r_type.upper()} ë³´ê³ ì„œ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")

    if not os.path.exists(CONFIG_PATH):
        print(f"âŒ [ì—ëŸ¬] ì„¤ì • íŒŒì¼ ë¯¸ì¡´ì¬: {CONFIG_PATH}")
        return False

    historical_context = load_historical_contexts()
    print(f"ğŸ“š [STEP 1] ê³¼ê±° ë§¥ë½ ë¡œë“œ ì™„ë£Œ (ê¸¸ì´: {len(historical_context)}ì)")

    lookback_map = {"daily": 7, "weekly": 30, "monthly": 365}
    lookback_days = lookback_map.get(r_type, 30)
    
    if r_type == "daily":
        news_count = config_data.get("report_news_count", 100)
        raw_news_list = []
        
        if os.path.exists(PENDING_PATH):
            files = sorted([f for f in os.listdir(PENDING_PATH) if f.endswith(".json")], reverse=True)
            print(f"ğŸ” [STEP 2] {PENDING_PATH}ì—ì„œ {len(files)}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
            
            seen_keys = set()
            target_date_limit = (now_kst - timedelta(days=3)).date()
            
            parse_fail, filter_fail = 0, 0

            for f_name in files:
                try:
                    with open(os.path.join(PENDING_PATH, f_name), "r", encoding="utf-8") as file:
                        news_data = json.load(file)
                        title = news_data.get("title", "").strip()
                        pub_dt_str = news_data.get("pub_dt", "")
                        
                        if not title: continue

                        # ë‚ ì§œ ì²´í¬
                        try:
                            f_dt = datetime.strptime(pub_dt_str, '%Y-%m-%d %H:%M:%S').date()
                        except:
                            f_dt = now_kst.date()

                        if f_dt < target_date_limit:
                            filter_fail += 1
                            continue

                        clean_key = title.replace("[íŠ¹ì§•ì£¼]", "").replace("[ì†ë³´]", "").replace(" ", "")[:18]
                        if clean_key not in seen_keys:
                            seen_keys.add(clean_key)
                            raw_news_list.append(f"[{pub_dt_str[5:16]}] {title}")
                            
                        if len(raw_news_list) >= news_count: 
                            print(f"ğŸ“ [ì •ë³´] ëª©í‘œ ë‰´ìŠ¤ ê°œìˆ˜({news_count}ê°œ) ë„ë‹¬ë¡œ ì½ê¸° ì¤‘ë‹¨")
                            break
                except Exception as e:
                    parse_fail += 1
                    continue

            print(f"ğŸ“Š [ê²°ê³¼] ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: ìµœì¢… {len(raw_news_list)}ê°œ | ì œì™¸(ë‚ ì§œ/ì¤‘ë³µ): {filter_fail}ê°œ | íŒŒì‹±ì‹¤íŒ¨: {parse_fail}ê°œ")
        else:
            print(f"âš ï¸ [ê²½ê³ ] PENDING_PATH ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {PENDING_PATH}")

        news_ctx = f"### [ ê¸ˆì¼ ì£¼ìš” ë‰´ìŠ¤ {len(raw_news_list)}ì„  ]\n"
        news_ctx += "\n".join([f"- {t}" for t in raw_news_list])
        input_content = f"{news_ctx}\n"
        report_label = "ì¼ê°„(Daily)"

    else:
        # ì£¼ê°„/ì›”ê°„ ë¡œì§ ë¡œê·¸ (ìƒëµ)
        print(f"ğŸ—“ï¸ [STEP 2] {r_type.upper()} ëª¨ë“œ: ê³¼ê±° ë¦¬í¬íŠ¸ ìš”ì•½ ë°ì´í„° êµ¬ì„± ì¤‘...")
        # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼í•œ ìš”ì•½ ë¡œì§ ìˆ˜í–‰) ...
        report_label = r_type.capitalize()
        input_content = "ì£¼ê°„/ì›”ê°„ ìš”ì•½ ë°ì´í„°(ì¤‘ëµ)"

    # ğŸ¯ 3. AI í˜¸ì¶œ ë¡œê·¸
    a_cfg = config_data.get("analyst_model", {})
    model_name = a_cfg.get("name")
    print(f"ğŸ¤– [STEP 3] AI ëª¨ë¸ í˜¸ì¶œ ì‹œë„: {model_name} (URL: {a_cfg.get('url')})")
    
    # (í˜ì´ë¡œë“œ êµ¬ì„± ë¡œì§ ë™ì¼ - ì¤‘ëµ)
    
    # ğŸ¯ 4. ì‹¤í–‰ ë° ì €ì¥ ë¡œê·¸
    try:
        start_time = time.time()
        resp = requests.post(url, json=payload, headers=headers, timeout=300)
        resp.raise_for_status()
        duration = time.time() - start_time
        
        result = resp.json()
        # (ê²°ê³¼ ì¶”ì¶œ ë¡œì§ ë™ì¼)
        
        print(f"âœ¨ [STEP 4] AI ì‘ë‹µ ìˆ˜ì‹  ì„±ê³µ! (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
        
        save_path = save_report_to_file(report_content, r_type)
        print(f"ğŸ’¾ [STEP 5] ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {save_path}")
        return True
        
    except Exception as e:
        print(f"ğŸš¨ [ì—ëŸ¬] AI í˜¸ì¶œ ë˜ëŠ” ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

if __name__ == "__main__":
    last_news_time = 0
    last_auto_report_date = ""
    last_weekly_report_date = "" 
    last_monthly_report_date = ""

    print(f"ğŸš€ [AI Analyst] ì‹œìŠ¤í…œ ê°€ë™ - ê¸°ì¤€ ì‹œê°: {data.get('report_gen_time', '08:00')} (KST)")
    print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {BASE_PATH} | ë‰´ìŠ¤ ëŒ€ê¸°ì—´: {PENDING_PATH}")

    while True:
        try:
            now_kst = get_now_kst()
            current_ts = time.time()
            current_config = load_data() 
            
            # ğŸ•’ 1. ì‹œê° ì„¤ì • ë¡œê·¸
            base_time_str = str(current_config.get("report_gen_time", "08:00")).strip()
            current_time_str = now_kst.strftime("%H:%M")
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            
            # 2. ì‹¤í–‰ ì‹œê° ê³„ì‚° (ì£¼ê°„/ì›”ê°„ 10~20ë¶„ ê°„ê²©)
            base_dt = datetime.strptime(base_time_str, "%H:%M")
            weekly_time_str = (base_dt + timedelta(minutes=10)).strftime("%H:%M")
            monthly_time_str = (base_dt + timedelta(minutes=20)).strftime("%H:%M")

            # --- [ ë³´ê³ ì„œ ìƒì„± ì„¹ì…˜ ] ---
            if auto_gen_enabled:
                # ì¼ê°„ ë³´ê³ ì„œ
                if current_time_str == base_time_str and last_auto_report_date != now_kst.strftime("%Y-%m-%d"):
                    print(f"ğŸ¤– [{now_kst.strftime('%H:%M:%S')}] >>> (1/3) ì¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œí€€ìŠ¤ ì§„ì…")
                    if generate_auto_report(current_config, r_type="daily"):
                        last_auto_report_date = now_kst.strftime("%Y-%m-%d")
                
                # ì£¼ê°„ ë³´ê³ ì„œ (ì¼ìš”ì¼)
                elif current_time_str == weekly_time_str and now_kst.weekday() == 6:
                    print(f"ğŸ“… [{now_kst.strftime('%H:%M:%S')}] >>> (2/3) ì£¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œí€€ìŠ¤ ì§„ì…")
                    if generate_auto_report(current_config, r_type="weekly"):
                        last_weekly_report_date = now_kst.strftime("%Y-%U")

                # ì›”ê°„ ë³´ê³ ì„œ (1ì¼)
                elif current_time_str == monthly_time_str and now_kst.day == 1:
                    print(f"ğŸ›ï¸ [{now_kst.strftime('%H:%M:%S')}] >>> (3/3) ì›”ê°„ ë³´ê³ ì„œ ìƒì„± ì‹œí€€ìŠ¤ ì§„ì…")
                    if generate_auto_report(current_config, r_type="monthly"):
                        last_monthly_report_date = now_kst.strftime("%Y-%m")

            # --- [ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„¹ì…˜ ] ---
            update_interval_min = current_config.get("update_interval", 10)
            update_interval_sec = update_interval_min * 60
            
            # ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ ë‚¨ì€ ì‹œê°„ ê³„ì‚° (ë¡œê·¸ìš©)
            time_since_last = current_ts - last_news_time
            next_in = max(0, update_interval_sec - time_since_last)

            if time_since_last >= update_interval_sec:
                print(f"ğŸ“¡ [{now_kst.strftime('%H:%M:%S')}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ (ì£¼ê¸°: {update_interval_min}ë¶„)")
                
                feeds = current_config.get("feeds", [])
                g_inc = [k.strip().lower() for k in current_config.get('global_include', "").split(",") if k.strip()]
                g_exc = [k.strip().lower() for k in current_config.get('global_exclude', "").split(",") if k.strip()]
                
                new_saved = 0
                for feed in feeds:
                    try:
                        parsed = feedparser.parse(feed['url'])
                        l_inc = [k.strip().lower() for k in feed.get('include', "").split(",") if k.strip()]
                        l_exc = [k.strip().lower() for k in feed.get('exclude', "").split(",") if k.strip()]
                        
                        feed_new = 0
                        for entry in parsed.entries[:50]:
                            if not check_logic(entry.title, g_inc, g_exc): continue
                            if not check_logic(entry.title, l_inc, l_exc): continue
                            if save_file(entry, feed['name']):
                                feed_new += 1
                                new_saved += 1
                        if feed_new > 0:
                            print(f"   â””â”€ {feed['name']}: {feed_new}ê°œ ì‹ ê·œ ì €ì¥")
                    except Exception as e:
                        print(f"   â””â”€ âŒ {feed.get('name')} ì˜¤ë¥˜: {e}")
                
                print(f"âœ… [{now_kst.strftime('%H:%M:%S')}] ìˆ˜ì§‘ ì™„ë£Œ (ì´ {new_saved}ê°œ ì‹ ê·œ í™•ë³´)")
                last_news_time = current_ts
            else:
                # ë§¤ ë¶„ë§ˆë‹¤ ì •ê¸° ìƒì¡´ ì‹ ê³  ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
                if now_kst.minute % 5 == 0: # 5ë¶„ë§ˆë‹¤ ì¶œë ¥
                    print(f"ğŸ’¤ [{now_kst.strftime('%H:%M:%S')}] ëŒ€ê¸° ì¤‘... (ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ê¹Œì§€ {int(next_in/60)}ë¶„ ë‚¨ìŒ)")

        except Exception as e: 
            print(f"ğŸš¨ [{datetime.now().strftime('%H:%M:%S')}] ë£¨í”„ ì¹˜ëª…ì  ì—ëŸ¬: {e}")
            
        time.sleep(60)














