import feedparser
import time
import os
import json
import hashlib
import requests
from datetime import datetime, timedelta, date

# --- ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€) ---
CONFIG_PATH = "/share/ai_analyst/rss_config.json"
SAVE_PATH = "/share/ai_analyst/pending"
REPORTS_BASE_DIR = "/share/ai_analyst/reports"

def get_file_hash(text):
    """ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def save_file(entry, feed_name):
    os.makedirs(SAVE_PATH, exist_ok=True)
    title_hash = hashlib.md5(entry.title.encode('utf-8')).hexdigest()
    fname = f"{SAVE_PATH}/{title_hash}.txt"
    
    if os.path.exists(fname): return

    pub_date = entry.get('published') 
    if not pub_date:
        pub_date = datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")

    try:
        with open(fname, "w", encoding="utf-8") as f:
            # âš ï¸ app.pyì˜ load_pending_filesì™€ ì§ê²°ëœ ì €ì¥ ìˆœì„œ ì¤€ìˆ˜
            f.write(f"ì œëª©: {entry.title}\n")
            f.write(f"ë§í¬: {entry.link}\n")
            f.write(f"ë‚ ì§œ: {pub_date}\n")
            f.write(f"ìš”ì•½: {entry.get('summary', 'ë‚´ìš© ì—†ìŒ')}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def check_logic(text, inc_list, exc_list):
    text = text.lower()
    if any(x in text for x in exc_list if x): return False
    if inc_list:
        if not any(i in text for i in inc_list if i): return False
    return True

def cleanup_old_files(retention_days):
    """ì„¤ì •ëœ ë³´ê´€ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼ ì‚­ì œ"""
    if not os.path.exists(SAVE_PATH): return
    current_time = time.time()
    seconds_threshold = retention_days * 86400
    deleted_count = 0
    for filename in os.listdir(SAVE_PATH):
        file_path = os.path.join(SAVE_PATH, filename)
        if os.path.isfile(file_path) and filename.endswith(".txt"):
            if (current_time - os.path.getmtime(file_path)) > seconds_threshold:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except: continue
    if deleted_count > 0:
        print(f"ğŸ§¹ ë‰´ìŠ¤ íŒŒì¼ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ (ê¸°ì¤€: {retention_days}ì¼)")

def load_historical_contexts():
    """ê³¼ê±° ë¦¬í¬íŠ¸ ë§¥ë½ ë¡œë“œ (RAG ê¸°ëŠ¥)"""
    dir_map = {
        'YEARLY_STRATEGY': '04_yearly/latest.txt',
        'MONTHLY_THEME': '03_monthly/latest.txt',
        'WEEKLY_MOMENTUM': '02_weekly/latest.txt',
        'DAILY_LOG': '01_daily/latest.txt'
    }
    context_text = "### [ ì—­ì‚¬ì  ë§¥ë½ ì°¸ì¡° ë°ì´í„° ]\n"
    for label, rel_path in dir_map.items():
        full_path = os.path.join(REPORTS_BASE_DIR, rel_path)
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                if len(content.strip()) > 10:
                    context_text += f"\n<{label}>\n{content[:1000]}\n"
                else:
                    context_text += f"\n<{label}>: ë°ì´í„° ë¹„ì–´ ìˆìŒ.\n"
        else:
            context_text += f"\n<{label}>: ì´ì „ ê¸°ë¡ ì—†ìŒ. í˜„ì¬ ë‰´ìŠ¤ ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.\n"
    return context_text

def save_report_to_file(content, section_name):
    """AI ë³´ê³ ì„œ ê³„ì¸µí˜• ì €ì¥ ë° ì •ì œ"""
    subdir = {'daily': '01_daily', 'weekly': '02_weekly', 'monthly': '03_monthly'}.get(section_name.lower(), "05_etc")
    report_dir = os.path.join(REPORTS_BASE_DIR, subdir)
    os.makedirs(report_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filepath = os.path.join(report_dir, f"{timestamp}_{section_name}.txt")
    
    with open(filepath, "w", encoding="utf-8") as f: f.write(content)
    with open(os.path.join(report_dir, "latest.txt"), "w", encoding="utf-8") as f: f.write(content)

    # ìë™ ì •ì œ
    purge_rules = {'01_daily': 7, '02_weekly': 30, '03_monthly': 365}
    if subdir in purge_rules:
        threshold = time.time() - (purge_rules[subdir] * 86400)
        for f in os.listdir(report_dir):
            if f == "latest.txt": continue
            f_p = os.path.join(report_dir, f)
            if os.path.isfile(f_p) and os.path.getmtime(f_p) < threshold:
                os.remove(f_p)
    return filepath

def generate_auto_report(config_data):
    """DB ì—†ì´ ë‰´ìŠ¤ ë° ê³¼ê±° ë§¥ë½ë§Œìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±"""
    if not os.path.exists(CONFIG_PATH): return False
    
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    historical_context = load_historical_contexts() 

    # ğŸ¯ [ë‰´ìŠ¤ ë¡œë“œ] DB ì§€í‘œ ë¡œì§ ì‚­ì œë¨
    news_count = config_data.get("report_news_count", 100)
    news_ctx = f"### [ ê¸ˆì¼ ìµœì‹  ë‰´ìŠ¤ {news_count}ì„  ]\n"
    if os.path.exists(SAVE_PATH):
        pending_files = sorted(os.listdir(SAVE_PATH), reverse=True)[:news_count]
        for f_name in pending_files:
            try:
                with open(os.path.join(SAVE_PATH, f_name), "r", encoding="utf-8") as file:
                    news_ctx += f"- {file.readline().strip()}\n"
            except: continue

    a_cfg = config_data.get("analyst_model", {})
    payload = {
        "model": a_cfg.get("name", "gpt-4o"),
        "messages": [
            {
                "role": "system", 
                "content": f"í˜„ì¬ì‹œê°: {now_str}\n{a_cfg.get('prompt', 'ì „ë¬¸ ì „ëµê°€ë¡œì„œ ë¶„ì„í•˜ë¼.')}\n\n{historical_context}"
            },
            {
                "role": "user", 
                "content": f"ì•„ë˜ ìµœì‹  ë‰´ìŠ¤ì˜ íë¦„ì„ ê³¼ê±° ë§¥ë½ê³¼ ê²°í•©í•˜ì—¬ ì˜¤ëŠ˜ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.\n\n{news_ctx}"
            }
        ], 
        "temperature": 0.3
    }

    try:
        url = f"{a_cfg.get('url').rstrip('/')}/chat/completions"
        headers = {"Authorization": f"Bearer {a_cfg.get('key')}"} if a_cfg.get('key') else {}
        resp = requests.post(url, json=payload, headers=headers, timeout=300)
        report_content = resp.json()['choices'][0]['message']['content']
        save_report_to_file(report_content, "daily") 
        print(f"[{now_str}] ğŸ›ï¸ ë‰´ìŠ¤ ê¸°ë°˜ ìë™ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"ğŸš¨ [ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨] ì›ì¸: {str(e)}")
        return False

# --- [ ë©”ì¸ ë£¨í”„ ] ---
if __name__ == "__main__":
    last_news_time, last_auto_report_date = 0, datetime.now().strftime("%Y-%m-%d")
    
    if not os.path.exists(CONFIG_PATH):
        print(f"ğŸ› ï¸ ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤: {CONFIG_PATH}")
        save_data({"report_auto_gen": True, "report_gen_time": "08:00", "report_news_count": 100, "update_interval": 10, "feeds": []})

    while True:
        try:
            now, current_ts = datetime.now(), time.time()
            if os.path.exists(CONFIG_PATH):
                with open(CONFIG_PATH, "r", encoding="utf-8") as f: current_config = json.load(f)
            else: continue

            # ğŸ¯ ìë™ ë³´ê³ ì„œ ë¡œì§ (ì§€í‘œ ë¡œë“œ êµ¬ë¬¸ ì‚­ì œë¨)
            auto_gen_enabled = current_config.get("report_auto_gen", False)
            target_time_str = current_config.get("report_gen_time", "08:00")
            today_date_str = now.strftime("%Y-%m-%d")
            
            if auto_gen_enabled and now.strftime("%H:%M") == target_time_str and last_auto_report_date != today_date_str:
                if generate_auto_report(current_config): last_auto_report_date = today_date_str

            # ğŸ¯ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—”ì§„
            interval_sec = current_config.get("update_interval", 10) * 60
            if current_ts - last_news_time >= interval_sec:
                cleanup_old_files(current_config.get("retention_days", 7))
                # (ì—¬ê¸°ì— ì‹¤ì œ RSS ìˆ˜ì§‘ ë£¨í”„ í˜¸ì¶œ ì¶”ê°€ ê°€ëŠ¥)
                last_news_time = current_ts
                
        except Exception as e: print(f"âŒ ë£¨í”„ ì—ëŸ¬: {e}")
        time.sleep(60)
